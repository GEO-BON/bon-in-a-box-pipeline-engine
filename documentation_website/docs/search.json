[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "BON in a Box serves as a hub for Biodiversity Observation Networks (BONs), consolidating diverse tools and resources to enhance data analysis and collaboration, as well as a project catalogue. For more information about BON in a Box in general, please refer to BON in a Box’s website\n\nWhat is BON in a Box’s pipeline engine?\nBON in a Box now provides an open-source, fully transparent platform that allows users to contribute data and analysis pipelines to calculate indicators to assess progress towards biodiversity targets. BON in Box can integrate diverse data and expert knowledge, from Earth observation to genetic diversity metrics to calculate monitoring framework indicators.\nWe present here the BON in a Box pipeline engine as the ultimate ecosystem for harmonizing biodiversity observations, fostering efficiency, and amplifying the power of data-driven insights.\nIt is a community-contributed and open-source data to biodiversity indicator or data to Essential Biodiversity Variable (EBV) workflow repository curated by GEO BON. Its engine allows seamless connection of steps in different languages (R, Julia, Python) through a modular pipeline approach that analyzes both from user-provided and publicly available data sources. These pipelines combine local and global data sources in the EBV and biodiversity indicator calculations.\n\n\nWhy do we need this platform?\nScientists around the world are developing tools to create workflows to calculate essential biodiversity variables and indicators to report on biodiversity change. However, there are almost no platforms to share these workflows and tools, leading to duplication of effort. BON in a Box allows networks to share and enhance each other’s work.\nAdditionally, organizations often lack the technical capabilities to calculate these indicators to monitor biodiversity, which impairs decision-making. BON in a Box can bridge the gap between those knowing how to calculate, and those needing the indicators for monitoring and conservation.\n\n\nWhat is a pipeline?\nA pipeline is a collection of steps needed to transform input data into valuable output for the stakeholder in an automated way. It can be an EBV, an indicator, or anything useful for monitoring and decision-making in the context of biodiversity conservation and natural resource management.\nThe smaller steps composing a pipeline are scripts. These scripts also convert input data into valuable outputs, just like pipelines, but at a finer scale: cleaning, augmenting, standardizing, or performing statistical analyses. Importantly, these scripts can be reused or repurposed across varied contexts. For instance, data cleaning scripts could be applied to multiple pipelines that use similar types of data, enhancing efficiency and consistency in data processing workflows. Since the outputs are in-between every step, the pipeline can abstract the programming language, allowing a pipeline to mix R, python and Julia scripts without any interoperability issues.\nIf a pipeline builds upon the results of another pipeline, this “inner pipeline” can be included as a step in the “outer pipeline”; we call this a subpipeline. An SDM pipeline could serve to generate range maps inside a Species Habitat Index pipeline, or embedded in a Species Richness pipeline. We can thus define a step as “a script or a subpipeline performing an operation in the context of a pipeline”.\n\n\n\nSimplified pipeline with steps (blue), intermediate results (grey), and final results (green)\n\n\n\n\nWhy should you contribute?\nContributing to BON in a Box can increase the impact of your research by allowing your analysis workflows to be used in broader areas and contexts for decision-making. Contributors will become part of a global community generating analyses of the status and trends of biodiversity to inform management decisions."
  },
  {
    "objectID": "working_with_stac.html",
    "href": "working_with_stac.html",
    "title": "Working with STAC",
    "section": "",
    "text": "STAC stands for SpatioTemporal Asset Catalog. It is a common language to describe geospatial information, so it can be more easily worked with, indexed, and discovered. Spatial information is stored in a geoJSON format, which can be easily adapted to different domains. The goal of STAC is to enable a global index of all imagery derived data products with an easily implementable standard for organizations to expose their data in a persistent and reliable way. STAC objects are hosted in geoJSON format as HTML pages."
  },
  {
    "objectID": "working_with_stac.html#what-is-a-stac-catalog",
    "href": "working_with_stac.html#what-is-a-stac-catalog",
    "title": "Working with STAC",
    "section": "",
    "text": "STAC stands for SpatioTemporal Asset Catalog. It is a common language to describe geospatial information, so it can be more easily worked with, indexed, and discovered. Spatial information is stored in a geoJSON format, which can be easily adapted to different domains. The goal of STAC is to enable a global index of all imagery derived data products with an easily implementable standard for organizations to expose their data in a persistent and reliable way. STAC objects are hosted in geoJSON format as HTML pages."
  },
  {
    "objectID": "working_with_stac.html#geo-bon-stac-catalog",
    "href": "working_with_stac.html#geo-bon-stac-catalog",
    "title": "Working with STAC",
    "section": "GEO BON STAC Catalog",
    "text": "GEO BON STAC Catalog\nPipelines can be created using layers pulled from any STAC catalog, such as the Planetary Computer. GEO BON hosts a STAC catalog that contains many commonly used data layers, which can be explored in JSON format or in a more user-friendly viewer.\nThere is a step to pull and filter data from a STAC catalog in BON in a Box called ‘Load from STAC’, where users can specify the layers, bounding box, projection system, and spatial resolution they are interested in. It outputs a geoJSON file that becomes an input into a subsequent pipeline step."
  },
  {
    "objectID": "working_with_stac.html#working-with-the-stac-catalog-in-r",
    "href": "working_with_stac.html#working-with-the-stac-catalog-in-r",
    "title": "Working with STAC",
    "section": "Working with the STAC Catalog in R",
    "text": "Working with the STAC Catalog in R\nData in STAC catalogs can be loaded directly into R and visualized using the rstac package.\nTo start, install and load the following packages.\n\nlibrary(gdalcubes)\nlibrary(rstac)\nlibrary(knitr)\nlibrary(stars)\n\nLoading required package: abind\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\n\nNext, connect to the STAC catalog using the stac function in the rstac package.\n\nstac_obj &lt;- stac(\"https://stac.geobon.org/\")\n\nIn the STAC catalog, layers are organized into collections. To make a list of the collections:\n\ncollections &lt;- stac_obj |&gt; collections() |&gt; get_request()\n\nAnd to view the collections and their descriptions in a table.\n\ndf&lt;-data.frame(id=character(),title=character(),description=character())\nfor (c in collections[['collections']]){\n  df&lt;-rbind(df,data.frame(id=c$id,title=c$title,description=c$description))\n}\nkable(head(df))\n\n\n\n\n\n\n\n\n\nid\ntitle\ndescription\n\n\n\n\nchelsa-clim\nCHELSA Climatologies\nCHELSA Climatologies\n\n\ngfw-lossyear\nGlobal Forest Watch - Loss year\nGlobal Forest Watch - Loss year\n\n\nsoilgrids\nSoil Grids datasets\nSoilGrids aggregated datasets at 1km resolution\n\n\nghmts\nGlobal Human Modification of Terrestrial Systems\nThe Global Human Modification of Terrestrial Systems data set provides a cumulative measure of the human modification of terrestrial lands across the globe at a 1-km resolution. It is a continuous 0-1 metric that reflects the proportion of a landscape modified, based on modeling the physical extents of 13 anthropogenic stressors and their estimated impacts using spatially-explicit global data sets with a median year of 2016.\n\n\ngfw-treecover2000\nGlobal Forest Watch - Tree cover 2000\nGlobal Forest Watch - Tree cover 2000\n\n\ngfw-gain\nGlobal Forest Watch - Gain\nGlobal Forest Watch - Gain\n\n\n\n\n\nTo search for a specific collection. Here we will search for the land cover collection from EarthEnv.\n\nit_obj &lt;- stac_obj |&gt;\n  stac_search(collections = \"earthenv_landcover\") |&gt;\n  post_request() |&gt; items_fetch()\nit_obj\n\n###Items\n- features (12 item(s)):\n  - class_9\n  - class_8\n  - class_7\n  - class_6\n  - class_5\n  - class_4\n  - class_3\n  - class_2\n  - class_12\n  - class_11\n  - class_10\n  - class_1\n- assets: data\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nView the available layers in this collection.\n\nit_obj &lt;- stac_obj |&gt;\n  collections(\"earthenv_landcover\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nit_obj\n\n###Items\n- features (12 item(s)):\n  - class_9\n  - class_8\n  - class_7\n  - class_6\n  - class_5\n  - class_4\n  - class_3\n  - class_2\n  - class_12\n  - class_11\n  - class_10\n  - class_1\n- assets: data\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nView the properties of the first item (layer).\n\nit_obj[['features']][[1]]$properties\n\n$class\n[1] \"9\"\n\n$datetime\n[1] \"2000-01-01T00:00:00Z\"\n\n$`proj:epsg`\n[1] 4326\n\n$description\n[1] \"Urban/Built-up\"\n\n$full_filename\n[1] \"consensus_full_class_9.tif\"\n\n\nWe can summarize each layer in a table\n\ndf&lt;-data.frame(id=character(),datetime=character(), description=character())\nfor (f in it_obj[['features']]){\n  df&lt;-rbind(df,data.frame(id=f$id,datetime=f$properties$datetime,description=f$properties$description))\n}\nkable(head(df))\n\n\n\n\nid\ndatetime\ndescription\n\n\n\n\nclass_9\n2000-01-01T00:00:00Z\nUrban/Built-up\n\n\nclass_8\n2000-01-01T00:00:00Z\nRegularly Flooded Vegetation\n\n\nclass_7\n2000-01-01T00:00:00Z\nCultivated and Managed Vegetation\n\n\nclass_6\n2000-01-01T00:00:00Z\nHerbaceous Vegetation\n\n\nclass_5\n2000-01-01T00:00:00Z\nShrubs\n\n\nclass_4\n2000-01-01T00:00:00Z\nMixed/Other Trees\n\n\n\n\n\nNow, you can load one of the items with the stars package, which allows access to files remotely in a fast and efficient way by only loading the areas that you need.\nHere we will select the layer representing the percentage of “Evergreen/Deciduous Needleleaf Trees”, which is the 12th layer in the collection.\n\nlc1&lt;-read_stars(paste0('/vsicurl/',it_obj[['features']][[12]]$assets$data$href), proxy = TRUE)\nplot(lc1)\n\ndownsample set to 23\n\n\n\n\n\n\n\n\n\nYou can crop the raster by a bounding box and visualize it.\n\nbbox&lt;-st_bbox(c(xmin = -76, xmax = -70, ymax = 54, ymin = 50), crs = st_crs(4326))\nlc2 &lt;- lc1 |&gt; st_crop(bbox)\n\npal &lt;- colorRampPalette(c(\"black\",\"darkblue\",\"red\",\"yellow\",\"white\"))\nplot(lc2,breaks=seq(0,100,10),col=pal(10))\n\n\n\n\n\n\n\n\nYou can now save this object to your computer in Cloud Optimized GeoTiff (COG) format. COG works on the cloud and allows the user to access just the part of the file that is needed because all relevant information is stored together in each tile.\n# Replace \" ~ \" for the directory of your choice on your computer.\nwrite_stars(lc2,'~/lc2.tif',driver='COG',options=c('COMPRESS=DEFLATE'))\nNote that for a layer with categorical variables, saving is more complex.\nlc1 |&gt; st_crop(bbox) |&gt; write_stars('~/lc1.tif', driver='COG', RasterIO=c('resampling'='mode'),\noptions=c('COMPRESS=DEFLATE', 'OVERVIEW_RESAMPLING=MODE', 'LEVEL=6',\n'OVERVIEW_COUNT=8', 'RESAMPLING=MODE', 'WARP_RESAMPLING=MODE', 'OVERVIEWS=IGNORE_EXISTING'))"
  },
  {
    "objectID": "working_with_stac.html#working-with-gdal-cubes",
    "href": "working_with_stac.html#working-with-gdal-cubes",
    "title": "Working with STAC",
    "section": "Working with GDAL Cubes",
    "text": "Working with GDAL Cubes\nThe R package gdalcubes allows for the processing of four dimensional regular raster data cubes from irregular image collections, hiding complexities in the data such as different map projections and spatial overlaps of images or different spatial resolutions of spectral bands. This can be useful for working with many layers or time series of satellite imagery.\nThe following step is necessary for GDAL Cubes to work with STAC catalog data.\n\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\n\nYou can filter based on item properties and create a collection.\n\nst &lt;- stac_image_collection(it_obj$features, asset_names=c('data'),\n             property_filter = function(f){f[['class']] %in% c('1','2','3','4')},srs='EPSG:4326')\n\nNext, you can build a GDAL cube to process or visualize the data. This cube can be in a different CRS and resolution from those of the original elements/files. However, the time dimension must capture the temporal framework of the item. dt is expressed as a time period, P1M is a period of one month, P1Y is a period of one year. Resampling methods should be tailored to the data type. For categorical data, use “mode” or “nearest”. For continuous data, use “bilinear”. Aggregation is relevant only when multiple rasters overlap.\nHere is an example that sums the four forest categories using aggregation=“sum”, with a change of the reference system to use Quebec Lambert (EPSG:32198) and a resolution of 1 km.\n\nbbox&lt;-st_bbox(c(xmin = -483695, xmax = -84643, ymin = 112704 , ymax = 684311), crs = st_crs(32198))\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2000-01-01\", t1 = \"2000-01-01\", left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin),\ndx=1000, dy=1000, dt=\"P1D\", aggregation = \"sum\", resampling = \"mean\")\n\nCreate a raster cube\n\nlc_cube &lt;- raster_cube(st, v)\n\nSave the resulting file to your computer\n# Replace \" ~ \" for the directory of your choice on your computer.\nlc_cube |&gt; write_tif('~/',prefix='lc2',creation_options=list('COMPRESS'='DEFLATE'))\nPlot the raster cube\n\nlc_cube |&gt; plot(zlim=c(0,100),col=pal(10))\n\n\n\n\n\n\n\n\nUse the accessibility from cities dataset, keeping the same CRS and extent\n\nit_obj &lt;- stac_obj |&gt;\n  collections(\"accessibility_to_cities\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2015-01-01\", t1 = \"2015-01-01\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin),\n                                                dx=1000, dy=1000, dt=\"P1D\", aggregation\n\n = \"mean\", resampling = \"bilinear\")\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\nst &lt;- stac_image_collection(it_obj$features)\nlc_cube &lt;- raster_cube(st, v)\nlc_cube |&gt; plot(col=heat.colors)\n\n\n\n\n\n\n\n\nUse the CHELSA dataset on climatologies and create an average map for the months of June, July, and August 2010 to 2019\nit_obj &lt;- s_obj |&gt;\n  stac_search(collections = \"chelsa-monthly\", datetime=\"2010-06-01T00:00:00Z/2019-08-01T00:00:00Z\") |&gt;\n  get_request() |&gt; items_fetch()\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2010-06-01\", t1 = \"2019-08-31\", left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin), dx=1000, dy=1000, dt=\"P10Y\", aggregation = \"mean\",resampling = \"bilinear\")\n\nfor (i in 1:length(it_obj$features)){\n  names(it_obj$features[[i]]$assets)='data'\n  it_obj$features[[i]]$assets$data$roles='data'\n}\n\nanames=unlist(lapply(it_obj$features,function(f){f['id']}))\n\nst &lt;- stac_image_collection(it_obj$features, asset_names = 'data', property_filter = function(f){f[['variable']] == 'tas' & (f[['month']] %in% c(6,7,8)) })\n\nc_cube &lt;- raster_cube(st, v)\n\nc_cube |&gt; plot(col=heat.colors)\n\nc_cube |&gt; write_tif('~/',prefix='chelsa-monthly',creation_options=list('COMPRESS'='DEFLATE'))"
  },
  {
    "objectID": "how_to_install.html",
    "href": "how_to_install.html",
    "title": "Install",
    "section": "",
    "text": "The user interface allows the user to run and edit pipelines. You can access BON in a Box through the browser, but it accesses your local pipeline repository. The code for the scripts, however, is not edited through the UI but rather with a standard code editor such as VS Code or RStudio."
  },
  {
    "objectID": "how_to_install.html#video-tutorial",
    "href": "how_to_install.html#video-tutorial",
    "title": "Install",
    "section": "Video tutorial",
    "text": "Video tutorial"
  },
  {
    "objectID": "how_to_install.html#deploying-bon-in-a-box-locally",
    "href": "how_to_install.html#deploying-bon-in-a-box-locally",
    "title": "Install",
    "section": "Deploying BON in a Box locally",
    "text": "Deploying BON in a Box locally\nBON in a Box can be installed and ran on a local computer. While personal computers have less computing power than servers, it is more convenient for pipeline development, or when working with small examples. Users adding scripts and creating pipelines on their local computer can run them directly through BON in a Box in the user interface.\nPrerequisites:\n\nA GitHub account, and git installed on your computer\nAt least 6GB of free space (RAM requirements depend on the script that is being run)\nA Linux shell\n\nThis can be Terminal on a Mac or Git Bash on a PC\n\n\n\nStep 1: Install Docker\n\nInstall Docker here\n\nNote: It is not necessary to make an account\nTo test whether docker is added to the path of your terminal, run docker run --rm hello-world\nFor Mac users: Make sure to check the chip of your machine and download the correct version.\n\nIf there is an error message when running docker run --rm hello-world, see https://stackoverflow.com/a/71923962/3519951\nIf you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951.\nIf you have a Silicon chip, check “Use Rosetta for x86_64/amd64 emulation on Apple Silicon” in the Docker Desktop preferences. See issue #175.\n\nFor Linux users: Make sure to add your user to the docker group\n\n\n\n\nStep 2: Generate SSH Key\n\nFirst, make sure that you do not already have an SSH key registered to your computer by running ls -al ~/.ssh in your terminal.\n\nIf you already have a key it will be called one of the following:\n\nid_rsa.pub\nid_ecdsa.pub\nid_ed25519.pub\n\n\nIf you do not already have a key, generate one by running ssh-keygen -t ed25519 -C \"your_email@example.com\" replacing the email with the email address associated with your GitHub account.\n\nThis will prompt you to make a passphrase. If you choose to have a passphrase, although more secure, it will prompt you to enter it many times while creating a pipeline so consider skipping this step for easier use. You can keep pressing enter to continue without creating a passphrase.\n\nTo check if you successfully generated the SSH key, you can run ls -al ~/.ssh in the terminal. Your SSH key should be there now.\nNow, open the file with your SSH key by running cat ~/.ssh/&lt;name of SSH key&gt; e.g. cat ~/.ssh/id_ed25519.pub and copy your SSH key.\nNow, add your SSH key to your GitHub account by going to GitHub settings &gt; SSH and GPG keys and “New SSH key”. You can name this whatever you want and paste your SSH key into the “Key” field.\n\n\n\nStep 3: Clone repository\n\nClone repository (Windows users: make sure the cloned repo is under the user folder (`C:\\Users\\yourname\\…`), and not under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop.\n\n\n\nStep 4: Set environment file\n\nProvide the environment variables: Open the newly cloned repository on your computer\n\nFind the file called runner-sample.env\nDuplicate the file and rename the copy to runner.env.\nFill the properties depending on what you intend to run. Include any API keys that you need to access data (e.g. GBIF or IUCN)\nAdjust any server option as you see fit (optional).\n\nUsing a Linux terminal (terminal on Mac, Git Bash or WSL on Windows), navigate to the folder of the cloned repository.\nIf you are not in the same folder as the cloned repository, navigate to that folder so it can access the files. This can be done with `cd &lt;folder name&gt;`\n\nOn Windows, the drives will be noted as /c/ instead of C:\\. The path should look like\n` /c/Users/yourUsername/folder/bon-in-a-box-pipelines`\n\nIn the Linux terminal, type ./server-up.sh\n\nNote for windows users: this has to be a Linux terminal (e.g. git bash). Launching the server locally will not work using Powershell or Windows CMD terminal.\nIf using Docker Desktop, make sure you have docker open and running on your computer.\nThe first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes.\nNetwork problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure.\nWindows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization). Access to the BIOS may be required to enable virtualization.\n\nThe startup script might prompt you for an update. Accepting it is recommended.\n\n\n\nStep 5: Opening BON in a Box in the browser\n\nIn a regular browser, type http://localhost/ to open BON in a Box\nHead to the ““Run a Pipeline” and select the one that interests you. See Running a script or pipeline section.\n\n\n\nStep 6: Stopping BON in a Box\n\nRun ./server-down.sh in the terminal to stop the server when done\nOn Windows, to completely stop the processes, you might have to run wsl --shutdown\n\n\n\nUpdating BON in a Box\nEach time that the server is started, the startup script will check for updates and propose them. Type y for yes or press enter to update.\nAccepting the updates are generally recommended, unless:\n\nYou are currently using an unstable, rate-limited, or billable Internet connection. Defer the update by typing n (no) until a better internet connection is available. You can also use ./server-up.sh --offline option to completely skip update checks. Use with caution, since this may create errors if elements of the repository depend on new server features.\nThe runner containers need to be discarded, and you are using a pipeline that has installed dependencies. This is the case for a minority of script whose dependencies cannot be found on anaconda.org. Since we know that installing dependencies live can take a while, when the runners need an update, you will see a warning like this:\n\nIn this case, defer the update until you can afford the time to start a run that will reinstall the dependencies."
  },
  {
    "objectID": "how_to_install.html#deploying-the-servers-remotely",
    "href": "how_to_install.html#deploying-the-servers-remotely",
    "title": "Install",
    "section": "Deploying BON in a Box on a server",
    "text": "Deploying BON in a Box on a server\nInstallation steps on a server are the same as installing on a Linux computer, but some additional configurations need to be changed:\n\nIn runner.env, choose the appropriate HTTP_ADDRESS and HTTP_PORT for your server configuration. In a typical installation, use 0.0.0.0 as an HTTP address and default port (80).\nIn runner.env, select the “partial” cache cleaning mode. This will allow for calculated results to be long-lived on the server, as long as they are not run again with a new version of the script. The results in the output folder are always accessible by URL.\nIn the case where a server is used for demonstration purposes only, set environment variable BLOCK_RUNS=true\nThe outputs are precious, while the server machine can be re-generated anytime. Consider mounting an external backed-up drive to the output folder of your BON in a Box installation.\nBy default, BON in a Box listens for http. To enable https, we hide it behind a reverse proxy (documentation here) and activate https with certbot (documentation here)."
  },
  {
    "objectID": "peer_review.html",
    "href": "peer_review.html",
    "title": "Peer review",
    "section": "",
    "text": "Each pipeline will be reviewed by 2-5 people for scientific rigor, usability, and generalizability. The contributor will suggest potential reviewers, and they will be contacted by the BON in a Box team. Additional reviewers will be chosen by the BON in a Box team. It is encouraged to have reviewers from both the academic and policy sphere (e.g. a government scientist). The contributor will specify whether the pipeline is a pipeline for reporting, a pipeline for BONs, or a sampling prioritization pipeline.\nThe pipelines will be reviewed based on the following criteria:\nThe reviewers will test the pipeline with a number of data sets, species, and scales based on their expertise, and provide feedback. The reviewer will decide if the pipeline should be accepted with minor revisions, needs to be revised and resubmitted, or is not suitable for BON in a Box."
  },
  {
    "objectID": "peer_review.html#pipeline-publication",
    "href": "peer_review.html#pipeline-publication",
    "title": "Peer review",
    "section": "Pipeline publication",
    "text": "Pipeline publication\nWhen pipelines have been peer reviewed and are accepted, they will be published on Zenodo and given a DOI, which will be cited whenever the pipeline is used."
  },
  {
    "objectID": "peer_review.html#instructions-for-reviewers",
    "href": "peer_review.html#instructions-for-reviewers",
    "title": "Peer review",
    "section": "Instructions for reviewers",
    "text": "Instructions for reviewers\nReviewers will need to install BON in a Box to test the pipeline. See the installation instructions here.\nDownload the pipeline review form to fill out while testing and reviewing the pipeline.\n\nRun the pipeline with the default parameters and make sure that it works, that the pipeline, input, and output descriptions are clear, and that the outputs are useful. Users should be able to understand how to parameterize and run the pipeline using these descriptions.\nReview the code in the GitHub pull request to make sure there are no errors and it is doing what it is supposed to (note: not all reviewers will be reviewing code, only reviewers that are familiar with the coding methods). If there are specific code issues, comment those directly in the pull request.\nRead the markdown tutorial and evaluate it for clarity.\nTest the pipeline several times with different parameters. If you have your own data on hand, test the pipeline with that data. It is preferential to test the data in areas or with parameters that you are familiar with so you can evaluate the accuracy of the results. If the pipeline crashes or there are incorrect results, make note of this in the pipeline review form.\nMake a decision about whether the pipeline is ready to publish.\n\n\nDownload pipeline review form here"
  },
  {
    "objectID": "how_to_contribute.html",
    "href": "how_to_contribute.html",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "",
    "text": "Analysis pipelines in BON in a Box are workflows that have been adapted to its pipelines framework. Here we will walk you through a few steps to adapt your code to run in BON in a Box."
  },
  {
    "objectID": "how_to_contribute.html#github-contribution-workflow",
    "href": "how_to_contribute.html#github-contribution-workflow",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "GitHub contribution workflow",
    "text": "GitHub contribution workflow\nContributors will follow the GitHub flow when contributing pipelines to BON in a Box. Contributors can follow these steps:\n\nMake sure you have a personal/institutional GitHub repository\nFork the bon-in-a-box-pipelines repository to your personal GitHub repository\n\nDocumentation on working with forks can be found here\nMake sure you keep the forked repository up to date by adding the bon-in-a-box-pipelines repository as an upstream repository and regularly fetching changes\n\nCreate a branch from the main branch for your pipeline, with a descriptive branch name (e.g. ProtConn_pipeline)\nCreate your pipeline on this branch\nWhen it is complete, create a pull request (instructions here) to the main branch on the bon-in-a-box-pipelines GitHub and fill out the peer review template, suggesting at least two potential peer reviewers\nWhen peer review is complete, make any suggested changes to the pipeline and resubmit\nWhen the pipeline has been accepted, it will be merged to the main branch and published on Zenodo with a DOI\nThe BON in a Box team will contact you for any bug fixes or feature requests, and new releases will follow the versioning structure below\nDuring the development process of the pipeline, regularly merge the main branch to your branch to avoid potential conflicts when merging your branch."
  },
  {
    "objectID": "how_to_contribute.html#how-to-write-a-script-video-tutorial",
    "href": "how_to_contribute.html#how-to-write-a-script-video-tutorial",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "How to write a script video tutorial",
    "text": "How to write a script video tutorial"
  },
  {
    "objectID": "how_to_contribute.html#how-to-create-a-pipeline-video-tutorial",
    "href": "how_to_contribute.html#how-to-create-a-pipeline-video-tutorial",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "How to create a pipeline video tutorial",
    "text": "How to create a pipeline video tutorial\n\nAnalysis workflows can be adapted to BON in a Box pipelines using a few simple steps. Once BON in a Box is installed on your computer, pipelines can be edited and run locally by creating files in your local BON-in-a-box folder cloned from the GitHub repository. Workflows can be written in a variety of languages, including R, Julia, and Python, and not all pipeline steps need to be in the same language."
  },
  {
    "objectID": "how_to_contribute.html#before-you-start",
    "href": "how_to_contribute.html#before-you-start",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Before you start",
    "text": "Before you start\nMake sure that you are working in the right environment when running code in BON in a Box. This can be in your code editor of choice, such as Visual Studio Code opened to the BON in a Box pipelines folder cloned to your computer from GitHub, or an R Studio project set up with the BON in a Box GitHub repository. This will make it easier to pull and push to the GitHub repository later."
  },
  {
    "objectID": "how_to_contribute.html#step-1-plan-your-pipeline",
    "href": "how_to_contribute.html#step-1-plan-your-pipeline",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 1: Plan your pipeline",
    "text": "Step 1: Plan your pipeline\nThe first step is to sketch out what your pipeline will look like. This includes breaking up your workflow into logical steps that perform one task and could potentially be reused in other pipelines. Think of the inputs and outputs that you want each script to have and how you want to connect these into a full pipeline.\nHere is an example:\n\nCheck if any of the steps you need are already available in BON in a Box to avoid having to rewrite them."
  },
  {
    "objectID": "how_to_contribute.html#step-2-break-up-your-scripts",
    "href": "how_to_contribute.html#step-2-break-up-your-scripts",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 2: Break up your scripts",
    "text": "Step 2: Break up your scripts\nYou can start writing the individual scripts in your pipeline. If you already have an analysis workflow, break this up into individual scripts representing the steps of the pipeline. Plan what you want as the inputs and outputs for each script."
  },
  {
    "objectID": "how_to_contribute.html#step-3-create-a-yaml-file",
    "href": "how_to_contribute.html#step-3-create-a-yaml-file",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 3: Create a YAML file",
    "text": "Step 3: Create a YAML file\nEach script should be accompanied by a YAML file (.yml extension) that specifies inputs and outputs of that step of the pipeline.\nMarkdown is supported for pipeline, script, input, and output descriptions. See this CommonMark quick reference for allowed markups. These can include special characters, links, and images.\n\nName and description\nYAML files should begin with a name, description, and the authors of the script. The name should be the name of the script described by the YAML file (script_name.R below). Save the file with the same name as the script that it describes but with a .yml extension (e.g. if the script is called my_script.R the YAML should be called my_script.yml).\nExample:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\n\n\nInputs\nNext, the YAML file should specify the inputs of the script. These inputs will either be outputs from the previous script, which can be connected by lines in the pipeline editor, or specified by the user in the input form.\nAll input and output ids should follow the snake case naming conventions. Names should be all lowercase with underscores instead of spaces. For example, an input labelled “Species name” would have the snake case id of “species_name”. This id will be the name of the input parameter in your code.\nHere is an example where the inputs are a coordinate reference system and a distance, specified by the user:\ninputs:\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: text\n    example: \"EPSG:4326\"\n  distance:\n    label: Distance\n    description: Some distance measurement (in meters) for a parameter\n    type: int\n    example: 1000\nSpecifying an example will auto-fill that input with the example unless changed by the user. If you do not want an example that auto-fills the input, write “example: null”. This will leave the input blank unless the user adds one. Not specifying any example will cause an error. It is mandatory that a script can be run successfully by leaving all the examples and simply pressing Run.\nThere are several different types of user inputs that can be specified in the YAML file:\n\n\n\n\n\n\n\n\n“Type” attribute in the yaml\nUI rendering\nDescription\n\n\n\n\nboolean\nPlain text\ntrue/false\n\n\nfloat or float[] (float array)\nPlain text\npositive or negative numbers with a decimal point, accepts int and a valid input.\n\n\nint or int[] (int array)\nPlain text\ninteger\n\n\noptions1\nDropdown menu\nSelect one from predefined options\n\n\noptions[]1\nMulti-select dropdown menu\nSelect several from predefined options\n\n\ntext or text[] (text array)\nPlain text\ntext input (words)\n\n\n(any unknown type)\nPlain text\n\n\n\n\n\nNote that the square brackets specify an array, where the user can input multiple values separated by a comma. This is useful if you want the analysis to run with several different parameters (e.g. for multiple species or with multiple raster bands).\n\n1options type requires an additional options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n  - first option\n  - second option\n  - third option\n  example: third option\nThe user inputs can also be file types if you want to upload a file or connect the script to a previous one that outputs a specific file type. There are several file types that you can specify:\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nDescription\n\n\n\n\nCSV\ntext/csv\nComma separated values file format (.csv)\n\n\nGeoJSON\napplication/geo+json\nGeoJSON file format (.geojson)\n\n\nGeoPackage\napplication/geopackage+sqlite3\nGeoPackage file format (.gpkg)\n\n\nGeoTIFF2\nimage/tiff;application=geotiff\nGeoTIFF file format (.tif)\n\n\nShapefile\napplication/dbf\nShapefile format (.shp)\n\n\nText\ntext/plain\nPlain text output (no need to save as a format just output in the script)\n\n\nTSV\ntext/tab-separated-values\nTab separated values format (.tsv)\n\n\n\nFor example:\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n    example: null\n  species_occurences:\n    label: Occurence points of species\n    description: Occurence points from GBIF or uploaded by the user\n    type: text/csv\n    example: null\n\n\nOutputs\nAfter specifying the inputs, you can specify the files that you want to output for that script. These can either be final pipeline outputs, outputs for that step to connect to another step, or both.\nThere are also several types of outputs that can be created\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON2\napplication/geo+json\nMap\n\n\nGeoPackage\napplication/geopackage+sqlite3\nLink\n\n\nGeoTIFF3\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nHTML\ntext/html\nOpen in new tab\n\n\nJPG\nimage/jpg\ntag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\n2 GeoJSON coordinates must be in WGS 84. See specification section 4 for details.\n3 When used as an output, image/tiff;application=geotiff type allows an additional range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows something\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\nHere is an example output:\noutputs:\n  result:\n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv\n  result_plot:\n    label: Result plot\n    description: Plot of results\n    type: image/png\n\n\nSpecifying R and Python package dependencies with conda\n“Conda provides package, dependency, and environment management for any language.” If a script is dependent on a specific package version, conda will load that package version to prevent breaking changes when packages are updated. BON in a Box uses a central conda environment that includes commonly used libraries, and supports dedicated environments for the scripts that need it.\nThis means that instead of installing packages during script execution scripts, they are installed in the R environment in conda. Note that you still need to load the packages in your scripts using the library() command for R or import for Python.\n\nCheck if the libraries you need are in conda\nTo check if the packages you need are already in the default conda environment, go to runners/conda/r-environment.yml or runners/conda/python-environment.yml in the BON in a Box project folder. The list of packages already installed can be found under “dependencies”.\nIf all the packages that you need are in the R environment file and you do not need specific package versions, you do not need to specify a conda sub environment and you can skip the next step.\n\n\nSpecify a conda sub-environment in your YML file\nIf you include a conda section, make sure you list all your dependencies. If you specify a conda environment in your YML file, it is creating a new separate conda environment, not a supplement to the base environment. This means that all of the packages that you need for the script need to be specified here, even if they are in the r-environment.yml file.\nSearch for your package names on anaconda. Packages that are in anaconda will be recognized by conda and can be included in the conda sub-environment in the YML file. If your package is not there, see below.\n\nRPythonJulia\n\n\nNote that every conda sub environment and script in R must load rjson because it is necessary for running the pipelines in BON in a Box\nExample of conda sub-environment for an R script:\nconda: # Optional: if you script requires specific versions of packages.\n  channels:\n    - conda-forge # make sure to put conda forge first\n    - r\n  dependencies:\n    - r-rjson # read inputs and save outputs for R scripts\n    - r-terra\n    - r-ggplot2\n    - r-rredlist=1.0.0 # package=version\n\n\nExample of conda sub-environment for a python script. You do not need to put anything after conda-forge or before the package names when specifying a conda environment for python.\nconda:\n  channels:\n    - conda-forge\n  dependencies:\n    - openeo\n    - pandas\n\n\nConda dependency management is not available in Julia. There is a central project at /julia_depot/.\n\n\n\n\n\nLoading packages that are not available in conda\nIf you need a package that is not available in conda, such as one downloaded from GitHub, install the package directly in the script. Do not add these packages to the conda portion of the yml file because conda will not be able to recognize them.\nFor more information, refer to the conda documentation.\n\n\n\nExamples\n\nFull example of a YAML file:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe\n    email: john.doe@example.com\n  - name: Jane Doe\n    email: jane.doe@example.com\n    identifier: https://orcid.org/0000-0000-0000-0000\n    role: [Software, Visualization, Conceptualization]\nreviewer:\n  - name: John Smith\n    email: john.smith@example.com\n  - name: Jane Smith\n    email: jane.smith@example.com\n    identifier: https://orcid.org/0000-0000-0000-0000\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of\n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\noutputs:\n  result:\n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv\n  result_plot:\n    label: Result plot\n    description: Plot of results\n    type: image/png\nconda: # Optional, only needed if you need packages not in the central environment\n  channels:\n    - conda-forge\n    - r\n  dependencies:\n    - r-rjson=0.2.22 #package=version\n    - r-rredlist\n    - r-ggplot2\n    - r-sf\n    - r-terra\nYAML is a space sensitive format, so make sure all the tab sets are correct in the file.\nIf inputs are outputs from a previous step in the pipeline, make sure to give the inputs and outputs the same name and they will be automatically linked in the pipeline.\n\n\nEmpty commented template to fill in:\nscript: # script file with extension, such as \"myScript.R\".\nname: # short name, such as My Script\ndescription: # Targeted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\n    role: # Optional, array of roles undertaken by the author in their contribution. We recommend to use CRediT roles (https://credit.niso.org/).\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, GitHub repo, etc.\ntimeout: # Optional, in minutes. By defaults scripts time out after 24h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name that will show up in UI\n    description: # Targeted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\n\nconda: # Optional, only needed if your script has specific package dependencies\n  channels: # programs that you need\n    - # list here\n  dependencies: # any packages that are dependent on a specfic version\n    - # list here\nSee example"
  },
  {
    "objectID": "how_to_contribute.html#step-4-integrating-yaml-inputs-and-outputs-into-your-script",
    "href": "how_to_contribute.html#step-4-integrating-yaml-inputs-and-outputs-into-your-script",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 4: Integrating YAML inputs and outputs into your script",
    "text": "Step 4: Integrating YAML inputs and outputs into your script\nNow that you have created your YAML file, the inputs and outputs need to be integrated into your scripts.\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported versions of R, Julia, Python and shell are displayed in the server info tab of the user interface.\nScript life cycle:\n\nScript launched with output folder as a parameter (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (ex. number, string, etc.)\n\nSee empty R script for a minimal script life cycle example.\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in a generated input.json file in this unique run folder.\nThis file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6622\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads the input.json file that is in the run folder. This JSON file is automatically created from the BON in a Box pipeline.\n\nRPythonJulia\n\n\nA convenience function biab_inputs is loaded in R by default to load the file into a list. Here is an example of how to read the input file in the R script:\ninput &lt;- biab_inputs()\nNext, for any functions that are using these inputs, they need to be specified.\nInputs are specified by input$input_name.\nFor example, to create a function to take the square root of a number that was input by the user in the UI:\nresult &lt;- sqrt(input$number)\nThis means if the user inputs the number 144, this will be run in the function and output 12.\nHere is another example to read in an output from a previous step of the pipeline, which was output in csv format.\ndat &lt;- read.csv(input$csv_file)\nThe input that is specified in the script needs to match the name that is in the YML file. For example, to load the landcover rasters that I used as an example above, I would write:\nlandcover &lt;- rast(input$landcover_raster)\nAnd to project it using the CRS that the user specified:\nlandcover_projected &lt;- project(landcover, input$crs)\n\n\nA convenience function biab_inputs is loaded in Python by default to load the file into a list. Here is an example of how to read the input file in the script and use it’s value:\ndata = biab_inputs()\nmy_int = data['some_int_input']\n# do something with my_int\n\n\nThe following code allows to read inputs from the json file into a variable in a julia script:\nusing JSON\n\n# Read the inputs from input.json\ninput_data = open(\"input.json\", \"r\") do f\n    JSON.parse(f)\nend\n\nnumber = input_data[\"number\"]\n# do something with number\n\n\n\n\n\nWriting outputs\nThe outputs can be saved in a variety of file formats or primitive values (int, text, etc.). The format must be accurately specified in the YAML file. If your script has many outputs, it is recommended to call biab_output(...) throughout your script, and not only at the end: in the event that your script fails, the results that had already been already output can be valuable to understand what has happened.\n\nRPythonJulia\n\n\nA variable named outputFolder is automatically loaded and contains the file path to output folder.\nIn R, a convenience function biab_output allows to save the outputs. Here is an example of an output with a csv file and a plot in png format.\nresult_path &lt;- file.path(outputFolder, \"result.csv\")\nwrite.csv(result, result_path, row.names = F )\nbiab_output(\"result\", result_path)\n\nresult_plot_path &lt;- file.path(outputFolder, \"result_plot.png\")\nggsave(result_path, result)\nbiab_output(\"result_plot\", result_plot_path)\n\n\nA variable named output_folder is automatically loaded and contains the file path to output folder.\nA convenience function biab_output allows to save the outputs as soon as they are created in the course of your script.\nintOutput = 12345\nbiab_output(\"my_int_output\", intOutput)\n\n\nA variable named output_folder is loaded before your script is executed. It contains a string representation of the output folder.\nA convenience function biab_output allow to save the outputs as soon as they are created in the course of your script.\n# Write the output file to disk\nmy_output_path=joinpath(output_folder, \"someOutput.csv\")\nopen(my_output_path, \"w\") do f\n    write(f, #= ... =#)\nend\n\n# Add it to the BON in a Box outputs for this script\nbiab_output(\"my_output\", my_output_path)\n\n\n\n\n\nModifying your script\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted:\n\nWhen modifying an existing script, simply save the file and re-run the script from the UI for the new version to be executed.\nWhen adding or renaming scripts, refresh the browser page.\nIf your scripts depends on additional files or external dependencies that have been updated, the update will not be detected automatically. In that case, re-save the main script file - the one reference in the .yml file - to trigger a re-execution for the next runs (we use the saved date of the script file to trigger re-runs).\n\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted:\n\nIn the pipeline editor, click save and run it again from the “pipeline run” page.\nWhen adding or renaming pipelines, refresh the browser page.\n\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh in your terminal.\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests. Accepted pipelines are tagged as in development, in review, reviewed or deprecated with an optional message describing why it was given that status. Scripts that are not intended for real-world use cases are tagged as examples.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed with special formatting in the UI.\nAny error message will also halt the script and the rest of the pipeline. It is recommended to output specific error messages to avoid crashes when errors can be foreseen. This can be done using the biab_error_stop helper function, or by writing an “error” attribute to the output.json file.\nFor example, if an analysis should stop if the result was less than 0:\n\nRPythonJulia\n\n\n# R helper function for errors\n# stop if result is less than 0\nif (result &lt; 0){\n    biab_error_stop(\"Analysis was halted because result is less than 0.\")\n    # Script stops here\n}\n\n# Other reporting functions\nbiab_warning(\"This warning will appear in the UI, but does not stop the pipeline.\")\nbiab_info(\"This message will appear in the UI.\")\n\n\nif result &lt; 0 :\n  biab_error_stop(\"Analysis was halted because result is less than 0.\")\n  # Script stops here\n\n# Other reporting functions\nbiab_warning(\"This warning will appear in the UI, but does not stop the pipeline.\")\nbiab_info(\"This message will appear in the UI.\")\n\n\n# Sanitize the inputs\nif result &lt; 0\n    biab_error_stop(\"Analysis was halted because result is less than 0.\")\n    # Script stops here\nend\n\n# Other reporting functions\nbiab_warning(\"This warning will appear in the UI, but does not stop the pipeline.\")\nbiab_info(\"This message will appear in the UI.\")\n\n\n\nAnd then the customized message will appear in the UI.\nA crash will have the same behavior as an error output, stopping the pipeline with the error displayed in the UI. A script that completed without creating the output.json file is assumed to have crashed, and a general error message will be displayed. In both cases, the logs can be inspected for more details."
  },
  {
    "objectID": "how_to_contribute.html#step-5-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor",
    "href": "how_to_contribute.html#step-5-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 5: Connect your scripts with the BON in a Box pipeline editor",
    "text": "Step 5: Connect your scripts with the BON in a Box pipeline editor\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from GitHub, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nA pipeline is a collection of steps that are connected to automate an analysis workflow. Each script becomes a pipeline step.\nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows the user to create pipelines by connecting individual scripts through inputs and outputs.\nThe left pane shows the available steps, which can be dragged onto the pipeline editor canvas.\n\n\nOn the right side, a collapsible pane allows the user to edit the labels and descriptions of the pipeline inputs and outputs.\n\nBelow that pane, there is a pane for writing the metadata of the pipeline.\n\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many steps, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as shown below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and the user will have to fill the value.\n\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons."
  },
  {
    "objectID": "how_to_contribute.html#step-6-run-pipeline-and-troubleshoot",
    "href": "how_to_contribute.html#step-6-run-pipeline-and-troubleshoot",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 6: Run pipeline and troubleshoot",
    "text": "Step 6: Run pipeline and troubleshoot\nThe last step is to load your saved pipeline in BON and a Box and run it in the “run pipelines” tab.\nYour pipeline should generate a form, which you can fill and then click run.\n\nYou can look at the log of each of your scripts to read errors and debug.\n\nOnce you change a script’s code and save to your local computer, the changes will immediately be implemented when you run the script through BON in a Box. However, if you change the YAML file, you will need to reload the page in the pipeline editor to see those changes. If this does not work, you may need to delete the script from the pipeline and re-drag it to the pipeline editor. Inputs and outputs can also be directly edited in the pipeline editor."
  },
  {
    "objectID": "pipeline_standards.html",
    "href": "pipeline_standards.html",
    "title": "Quality requirements for BON in a Box analysis pipelines",
    "section": "",
    "text": "BON in a Box is community-driven and open tool for biodiversity modeling and indicator calculation developed by GEO BON\nBON in a Box connects individual analysis into pipelines that are contributed by experts\nContributed pipelines should be generalizable, user-friendly, scientifically robust, and well documented\nThis document outlines a set of standards for analysis pipelines to meet these requirements"
  },
  {
    "objectID": "pipeline_standards.html#key-takeaways",
    "href": "pipeline_standards.html#key-takeaways",
    "title": "Quality requirements for BON in a Box analysis pipelines",
    "section": "",
    "text": "BON in a Box is community-driven and open tool for biodiversity modeling and indicator calculation developed by GEO BON\nBON in a Box connects individual analysis into pipelines that are contributed by experts\nContributed pipelines should be generalizable, user-friendly, scientifically robust, and well documented\nThis document outlines a set of standards for analysis pipelines to meet these requirements"
  },
  {
    "objectID": "pipeline_standards.html#glossary",
    "href": "pipeline_standards.html#glossary",
    "title": "Quality requirements for BON in a Box analysis pipelines",
    "section": "Glossary",
    "text": "Glossary\n\n\n\nScript\nA sequence of code written in a programming language that accomplishes a single task, such as data cleaning, analysis, visualization, or modeling.\n\n\nPipeline\nA sequence of scripts connected to automate an entire analysis workflow, from the input parameters to the analysis result.\n\n\nBiodiversity Observation Network (BON)\nA network of observation sites or stations and/or a network of experts and groups who collect and analyze biodiversity data for different needs. A BON coordinates monitoring efforts to support conservation policy and/or management action from national biodiversity strategies and action plans. A BON can be regional (e.g. Europe, Asia-Pacific), national (e.g. Japan), or thematic (e.g. Marine, Freshwater).\n\n\nEssential Biodiversity Variable (EBV)\n“A biological variable that critically contributes to the characterization of Earth’s biodiversity; they are a minimum set of common and complementary sets of observable variables across the dimensions of biodiversity that can be used to create indicators of system-level biodiversity trends” (after Brummitt et al. 2017. EBVs provide scalable, comparable metrics that can be aggregated into time-series or spatial maps, enabling the detection of patterns and drivers of biodiversity change.\n\n\nIndicator\nA derived metric informed by biodiversity datasets (e.g. EBVs) that summarizes biodiversity information into a single value that can help track changes in biodiversity status or the pressures affecting it, thereby providing measurable data that informs policy decisions and conservation actions. In the context of the Kunming-Montreal Global Biodiversity Framework, an indicator is used to assess progress toward the framework’s goals and targets.\n\n\nKunming-Montreal Global Biodiversity Framework (GBF)\nThe latest agreement of the United Nations Convention on Biological Diversity (CBD) adopted by 196 Parties at its 15th Conference of the Parties in December 2022. The GBF sets an ambitious pathway to reach a global vision of living in harmony with nature and halting biodiversity loss by 2050 and supports the achievement of the Sustainable Development Goals (CBD, 2022a). |\n\n\nGBF Monitoring Framework\nThe Monitoring framework is designed to track progress toward the goals and targets of the GBF through a set of indicators. It emphasizes the need for consistent data collection, reporting, and evaluation at national and global levels to ensure accountability and transparency. It calls upon the development of national and regional monitoring systems, including the technologies, tools, networks and communities needed to sustain monitoring.\n\n\nFAIR principles\nFour foundational principles of data - findability, accessibility, interoperability, and reusability - which aim to increase the openness, equity, and integrity of the scientific process. See https://www.go-fair.org/fair-principles/"
  },
  {
    "objectID": "pipeline_standards.html#guidelines-and-requirements",
    "href": "pipeline_standards.html#guidelines-and-requirements",
    "title": "Quality requirements for BON in a Box analysis pipelines",
    "section": "Guidelines and Requirements",
    "text": "Guidelines and Requirements\n\nAnalysis types\nBON in a Box is a platform to share expertise and knowledge and promote collaboration by allowing scientists to share analysis workflows in an organized and open way. BON in a Box has three main groups of pipelines:\n\nPipelines for reporting: Pipelines to calculate Essential Biodiversity Variables and indicators that are in the Convention on Biological Diversity’s Global Biodiversity Framework. These pipelines have undergone a more rigorous peer review process and must adhere to the EBV guidelines and/or the official methodology in the GBF’s Monitoring Framework outlined in the UNEP-WCMC documentation, if it exists. Pipelines will be created or reviewed by members of the EBV working group or the custodial institution to assure that they follow standards for reporting. These pipelines will be accompanied by a tag that specifies them as reporting ready.\nGeneral biodiversity monitoring: Pipelines that are relevant to biodiversity monitoring at any scale but don’t have direct relevance to national reporting through the CBD. These pipelines can be more directed at specific biomes, habitats, or ecosystems but still useful to share between national or thematic biodiversity observation networks. \nPipelines for sampling prioritization: Pipelines to help build national BONs by prioritizing sampling areas based on different criteria such as environmental distance, presence of threatened species, or accessibility. These sampling pipelines will help national and regional BONs expand their sampling programs in a systematic way. \n\nAdditionally, all analyses must be:\n\nGeneralizable: BON in a Box is a platform for sharing analyses between BONs, so contributed pipelines must be generalizable between different areas, taxa, spatial scales, etc. Analyses should not be specific to one region, species, or context. \nScalable: Pipelines should be able to be calculated at different scales.\nScientifically robust: Pipelines should follow a method that has been published in the literature or verified in some way. Sometimes, pipeline and method development will happen concurrently but pipelines will keep the “experimental pipeline” tag until the science has been verified by independent experts. Ideally, models will be ground-truthed with independent data sources.\nFAIR: Findable, accessible, interoperable, and reproducible*\nTransparent: All pipelines must be open access and all code must be publicly available; intermediate steps should display outputs relevant for auditing purposes; any calculations run outside of BON in a Box and imported using APIs must be run on open access platforms so all results can be audited and reproduced. Analyses should, whenever possible, have some estimates of uncertainty, predictive power and goodness of fit measures. \n\n\n\nData standards\nBON in a Box is a data processing platform, not a data repository. However, to ensure that pipelines follow FAIR principles, the data that is analyzed must meet a set of standards.\nBON in a Box is designed to run analyses with both publicly available data and data input by the user. This allows the user to customize the analysis with data that is more accurate to their region, taking ownership of the process without having to share their own data. \n\nNaming standards:\n\nInput and output file names should be machine readable, with no spaces, symbols, capital letters, or special characters. Spaces should be replaced with an underscore (_).\nFile names should be descriptive (e.g. “population_polygons.gpkg”).\n\n\n\nStandards for publicly available input data:\n\nWhen using publicly available data, BON in a Box should use APIs when they are available so pipelines can be run with the most up to date data available.\n\nWhen processing GBIF data in a pipeline, the GBIF download API should be used to ensure reproducibility and citability, as it provides a download DOI for each run, and this doi script output must be included in the pipeline outputs. The “GBIF observations with download API” script should be used for basic GBIF data retrieval. \n\nRaster data should be in cloud optimized GeoTIFF (e.g. STAC catalog) formats whenever possible. \n\nThe GEO BON STAC catalog and other STAC catalogs such as the planetary computer can be accessed through the load from STAC script. For more information about how to use STAC catalogs, see these tutorials.\n\n\n\n\nStandards for user-defined input data:\n\nPoint, line, and polygon data should be in the form of a geopackage.\n\nThe geometry column must be called “geom”.\nData should be in the coordinate reference system (CRS) appropriate for the region of interest (to find a CRS, search here).\n\nRaster data should be a GeoTiff file\n\nRaster data should be projected into a coordinate reference system appropriate for the region of interest.\nMulti-band rasters are allowed when the band names can be easily interpreted (e.g. year, species, model). \n\nTabular data can be uploaded as CSV (comma separated values) or TSV (tab separated values). Column names should be in snake_case and should avoid special characters and capital letters. When geographical coordinates are included, use the CRS EPSG:4326 (latitude, longitude WGS 84) and column names “latitude” and “longitude”, whenever possible. \n\nSpecific file formats and column formatting guidelines will depend on the pipeline and should be specified in the pipeline documentation.\n\n\n\n\nStandards for output data\nEach BON in a Box script creates outputs that can be downloaded. They should be in standard file formats and follow these guidelines:\n\nOutputs should be in interoperable data formats (e.g. not a language specific format such as RData).\nPoint, line, and polygon data should be output as geopackages. Include just one layer per geopackage whenever possible. \nRaster data should be output as a GeoTIFF, compressed (ex. Deflate or LZW).\nTabular data can be output as a comma separated values (CSV) or tab separated value (TSV) format.\n\n\n\n\nCode standards\nBON in a Box is designed to be a fully open and transparent platform where all code is open and reproducible. This allows the user to audit the process and edit the code if necessary to customize the process. Therefore, code must be well formatted, readable, and well documented. \nCode should adhere to the following requirements:\n\nFile names should be machine readable, with no spaces, symbols, or special characters. Spaces should be delimited with an underscore (_).\nCode must be divided into modular scripts that perform one task and can be reused in other pipelines.\nCode should be well formatted with indents\n\nFor R code, follow the tidyverse style guide (with 120 character lines)\nFor Python code, follow the PEP-8 style guide\nFor Julia code, follow the custom formatting file, which can be applied to scripts using the JuliaFormatter package\n\nScripts should be well annotated with descriptions of each step commented within the code\nScripts should check and validate that user inputs are properly entered and are in the expected format. Scripts should output informative messages and stop the execution with the biab_error_stop() function when invalid inputs are detected. \n\nFor example, if someone misspells a country name and the country polygon is not found, write if(is.null(country_polygon)) { biab_error_stop(“Country polygon not found. Check the spelling of the country name”)\n\nEach main step of the script prints a log (console output) message so the user can know at what stage the analysis is. The contributor is encouraged to add additional information messages that will be added to the log:\n\nMessages can be added with the biab_info() function. For example biab_info(“Study area downloaded”)\nWarning messages can be added with the biab_warning() function.\n\nThe script should provide informative error messages whenever mistakes are encountered using the biab_error_stop() function.\n\n\n\nYAML description files\nEach script will be accompanied by a YAML description file (.yml format) that has a description of the script, specifies the inputs and outputs, defines the Conda dependencies, and provides references (see our documentation for guidelines on how to format your YAML file).\n\nYAML file standards\n\nYAML files must have the same name as the script that they accompany with a .yml file extension\nYAML files should have a thorough description section that includes the purpose of the script\nYAML files should have the name, email, and ORCID (if available) of all script authors\nNames of inputs and output IDs should be in snake case, all lowercase with underscores for spaces\nInput and output labels should be short, human readable and descriptive\nInput and output descriptions should describe the format of the input and guide the user on parameterization using photos or links in the YAML description when necessary (see CommonMark reference for formatting)\nUsers should be able to successfully run the pipeline with the example inputs\nFor file outputs, type should be written as a MIME type\nFor primitive outputs, such as text and numbers, use a primitive type (int, float, text). Refer to the inputs section of the user documentation.\nThe example will autofill in the document. If the example is a file, make sure the file is in the appropriate folder (note the ‘userdata’ folder is local only and will not be pushed to GitHub) and the file path starts at that folder (e.g. /scripts/data/protected_area_file_example.gpkg). Note BON in a Box cannot access the whole computer so the root must be in the BON in a Box pipelines folder. Make sure the whole script runs with the chosen examples.\n\n\nConda dependencies\nConda is a package, dependency, and environment management system that prevents scripts from encountering breaking changes when packages are updated. Conda environments should list all of the necessary packages and package versions should be specified when necessary. Conda sub-environments should only be specified when the required packages or package versions are not in the main environment.\n\n\n\n\nDocumentation\n\nPipeline metadata\nEach pipeline contains metadata, which can be edited in the right sliding pane of the pipeline editor canvas. Pipeline metadata must contain:\n\nThe name of the pipeline in human readable form\nThe description of the pipeline, including a short description of the methods, any data it is using, the limitations of the pipeline, and if it is an indicator in the GBF, what kind of indicator it is and which targets/goals it is related to\nThe license of the pipeline (this is chosen by the user, see section on licensing below)\nThe author(s) of the pipeline and their emails and ORCIDs\nAny relevant references or links\n\n\n\nPipeline markdown tutorial\nEach pipeline should be accompanied by a tutorial in markdown format. This document should have the same name as the pipeline JSON file but with a .md extension and be contained in the same folder as the pipeline it describes. This tutorial should contain a more thorough description of the methods (around 2-3 paragraphs). The tutorial should provide guidance on how to parameterize the pipeline, and walk through a full example analysis. This document should also more thoroughly communicate the limitations of the pipeline in terms of the species, regions, and scales at which it is relevant. You can create the tutorial using this template.\n\n\n\nVersioning\nCreating pipelines to calculate Essential Biodiversity Variables and indicators is an iterative process, and pipelines will deprecate over time as methods are updated. Therefore, we will have a structured versioning system as pipelines are released. \nVersioning will follow a semantic versioning style in the form of major.minor.patch.\n\nNon peer-reviewed or stable pipelines should have versions starting with 0.*\nThe first peer-reviewed stable version of the pipeline will be released as 1.0.0.\nMinor bug fixes will be released continuously (e.g. 1.0.1).\nAny major changes to methods or packages will be a scheduled major release (e.g. 2.0.0). The old pipeline will be labeled as deprecated but will still be available for backwards compatibility.\n\n\n\nLicensing\nThe BON in a Box software is under the GPLv3 license, but each pipeline is licensed individually, with the specific license chosen by the contributor. Our recommended license is MIT. You can explore licenses here."
  },
  {
    "objectID": "how_to_use.html",
    "href": "how_to_use.html",
    "title": "User Guide",
    "section": "",
    "text": "The instructions for how to install BON in a Box on your computer can be found here."
  },
  {
    "objectID": "how_to_use.html#installing-bon-in-a-box",
    "href": "how_to_use.html#installing-bon-in-a-box",
    "title": "User Guide",
    "section": "",
    "text": "The instructions for how to install BON in a Box on your computer can be found here."
  },
  {
    "objectID": "how_to_use.html#connecting-to-the-server-and-running-bon-in-a-box",
    "href": "how_to_use.html#connecting-to-the-server-and-running-bon-in-a-box",
    "title": "User Guide",
    "section": "Connecting to the server and running BON in a Box",
    "text": "Connecting to the server and running BON in a Box\n\nConnect to the server by running ./server-up.sh in a Linux terminal (e.g. Git Bash or WSL on Windows or Terminal on Mac). Make sure you are in the bon-in-a-box-pipeline directory on your computer.\nOpen a web browser and navigate to http://localhost/."
  },
  {
    "objectID": "how_to_use.html#home",
    "href": "how_to_use.html#home",
    "title": "User Guide",
    "section": "Home",
    "text": "Home\nUnder the Home tab, you will find a description of the BON in a Box modeling tool, as well as its featured pipelines. And your most recent pipeline runs."
  },
  {
    "objectID": "how_to_use.html#run-a-script",
    "href": "how_to_use.html#run-a-script",
    "title": "User Guide",
    "section": "Run a script",
    "text": "Run a script\nThe Run a script tab lets you run an individual script and works very similarly to the Run a pipeline tab. Scripts are individual parts of pipelines designed to perform one well-defined task at a time, like extracting data from a file, cleaning files, or applying a specific formulas.\nThe Run a script functionality is normally used for testing individual components of a pipeline rather than producing usable outputs. Additionally, unlike pipelines, scripts often require inputs that are produced by other scripts, and so they may not run successfully with the default parameters. However, running a script can be useful for development, bug fixing, or trying to understand the purpose and outputs of a given step.\n\n\n\n\n\n\n\nSimply fill out the desired inputs and click on Run script. To terminate the execution of the pipeline at any point, use the stop button. While the pipeline is running, you may see its progress under Results.\nIndividual scripts sometimes rely on files that are output by other scripts, and therefore may not run with default parameters. You may need to input files from the “userdata” folder or files downloaded from the outputs of other scripts to make the scripts run.\nAfter the script runs, you’ll see a detailed Results box, which includes drop down arrows to view your inputs and script outputs. To simply view your output, click on the blue name or arrow. A gray box containing the script logs can be found under the outputs, which can help with debugging problems that occurred.\n\n\n\n\n\n\n\nIf you see a red exclamation mark, the script failed to completely execute and an error message will be displayed. If you encounter an error due to the script’s internal structure (i.e. not a problem with your input parameters), please contact us on Discourse or email us at boninabox@geobon.org.\nTo download outputs, simply click the link next the output name.\n\n\n\n\n\n\n\nTo run the script again, click Run script in the input form."
  },
  {
    "objectID": "how_to_use.html#run-a-pipeline",
    "href": "how_to_use.html#run-a-pipeline",
    "title": "User Guide",
    "section": "Run a pipeline",
    "text": "Run a pipeline\nThe Run a pipeline tab can be used to run an entire pipeline to calculate an indicator or EBV. Pipelines run a full sequence of connected scripts and are designed to carry out a full analysis from start to finish. Each pipeline integrates multiple steps like pulling data, cleaning data, performing analyses, and plotting and formatting outputs into one automated workflow.\n\n\n\n\n\n\n\n\nSelect from the drop down menu the pipeline you would like to run.\n\nOnce the pipeline is selected, there will be a description of the pipeline, with its authors and references.\nEach pipeline will have a tag indicating whether it is still being developed, in review, or peer reviewed.\n\nFill out the form with the input parameters the pipeline should use.\n\nThe pipeline will run with the default parameters, so you can run the pipeline right away to see what the outputs should look like.\nSome inputs can be typed in, while others are options.\nEach input field includes a description and its purpose on the right of the input box. Here you will also see the type of the input.\nPlease ensure each input matches the required type that is specified in the input description. The input will either be a primitive type (i.e. text, int, float, boolean, etc.) which the user will type in or a MIME type representing a file (e.g. text/csv, application/geo+json, etc.) where the user will provide the path to that file. The square brackets [] after a type represent an array of that type, meaning you can input several parameters separated by a comma (i.e. text[], application/geo+json[], etc.). For more details on input types, please refer to “Step 3: Create a YAML File” on this page.\nThe form might ask you for a file. In order to provide a file that you have locally on your computer, upload or copy it to the userdata folder. You can then refer to it with a url as such: /userdata/myFile.shp, or /userdata/myFolder/myFile.shp if there are subfolders. Most pipelines can also accept remote files (https://). That will be specified in the input documentation.\nThe Input yaml tab can be used to see how your inputs are being passed in to the pipeline. This is useful for sharing pipeline inputs for recreating results, as it can be copied and pasted to fill the input form. By scrolling down, you can also see how the inputs were defined by the author(s).\n\nClick on Run pipeline. To terminate the execution of the script at any point, use the Stop button. If the pipeline or any of its steps has previously been run with the exact same inputs and the code has not changed, the existing results will be loaded.\n\nAfter the script runs, you’ll see a detailed Results box, which includes drop down arrows to view your inputs and script outputs. To simply view your output, click on the blue name or arrow. A gray box containing the logs of each individual script can be found under the outputs, where you can monitor the progress of the script. The logs also help with any debugging problems that occurred.\nYou can view the results using the drop down menu for each, or use See in results viewer for a more detailed and user-friendly (less technical) view of the outputs. Outputs can be downloaded by using the blue link next to the output name.\nIn the Detailed results you will find all the individual scripts used throughout the pipeline, as well as their inputs and outputs."
  },
  {
    "objectID": "how_to_use.html#pipeline-editor",
    "href": "how_to_use.html#pipeline-editor",
    "title": "User Guide",
    "section": "Pipeline editor",
    "text": "Pipeline editor\nUnder the Pipeline editor tab, users can create and edit pipelines. For more information on how to do so, please refer to Step 6 “Connect your scripts with the BON in a Box pipeline editor” of the how to contribute page."
  },
  {
    "objectID": "how_to_use.html#history",
    "href": "how_to_use.html#history",
    "title": "User Guide",
    "section": "History",
    "text": "History\nThe History tab shows previous script and pipeline runs.\n\n\n\n\n\n\n\nUse the drop down arrow to see the inputs of a run and See run in UI to view the details of a run, including the inputs, outputs, and logs. The latter can be used to try the run again with some or all of the same inputs. For pipelines, you may see outputs in the results viewer using See in viewer.\nIf you run the same pipeline with the same input parameters multiple times, it will only show once in the history view.\n\n\n\n\n\n\n\nFor any further questions, please contact us on Discourse or email us at boninabox@geobon.org."
  },
  {
    "objectID": "what_to_contribute.html",
    "href": "what_to_contribute.html",
    "title": "what_to_contribute",
    "section": "",
    "text": "Does your analysis pipeline fit in BON in a Box?\nWe are looking for pipelines that calculate biodiversity metrics that are relevant to policy and biodiversity reporting, such as Essential Biodiversity Variables (EBVs) and Indicators. These pipelines should be\n\nopen source, written in the coding languages R, Julia, or Python\nrelevant to many contexts (different species, countries, and regions)\n\n\n\nSteps for contributing\nWe strongly encourage those who have created or are developing code to calculate Essential Biodiversity Variables or indicators to share it through BON in a Box. The purpose of the modeling tool is to have state of the art processing pipelines shared across the community, for the benefit of all. We recommend following these steps: \n\nIdentify a workflow with a good potential for reuse by other organisations, and that can be mostly automated.\nDraw a high-level view of the pipeline that identifies each step, the inputs and outputs, how the scripts are put together, and the format of the final results.\nMeet the BON in a Box team. We will help design the pipeline, ensuring that each step is modular, identifying scripts that are already in the modeling tool to avoid rewriting code, and finding places where the existing code would need to be modified. For example, using cloud-optimized approaches such as STAC and Parquet files instead of downloading large files during the pipeline’s execution.\nSecure the resources. This could be the time of a graduate student or work study student. If applying for funding, include BON in a Box in your proposal as a tool.\nCreate the pipeline following the steps below. We recommend starting with the simplest version and then adding complexity or features when needed.\nWhen you feel that the pipeline is complete and you have verified that it meets the [quality requirement standards](pipeline_standards.qmd), you can submit the pipeline for peer review. While you are waiting, why not contribute by reviewing someone else’s contribution?\nOnce the reviews are complete, implement any necessary changes.\nIf accepted, the pipeline will be published in BON in a Box and will receive a DOI for identification. Now, the pipeline can be used by others.\n\nIf you wish to contribute a pipeline, please email us at boninabox@geobon.org."
  },
  {
    "objectID": "how_to_contribute.html#general-github-contribution-workflow",
    "href": "how_to_contribute.html#general-github-contribution-workflow",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "General GitHub contribution workflow",
    "text": "General GitHub contribution workflow\nContributors will follow the GitHub flow when contributing pipelines to BON in a Box. Contributors can follow these steps:\n\nMake sure you have a personal/institutional GitHub repository.\nIf BON in a Box developers have granted you access to the repository, create a branch for your pipeline. Otherwise, fork the bon-in-a-box-pipelines repository to your personal GitHub repository and then clone it to your computer.\n\nDocumentation on working with forks can be found here and cloning here.\nMake sure you keep the forked repository up to date by adding the bon-in-a-box-pipelines repository as an upstream repository and regularly fetching changes.\n\nCreate a branch from the main branch for your pipeline, with a descriptive branch name (e.g. ProtConn_pipeline).\nCreate your pipeline on this branch. Make sure it adheres to the pipeline standards.\nWhen it is complete, create a pull request (instructions here) to the main branch on the bon-in-a-box-pipelines GitHub and fill out the peer review template that will autofill in the pull request, suggesting at least two potential peer reviewers.\nWhen peer review is complete, make any suggested changes to the pipeline and resubmit.\nWhen the pipeline has been accepted, it will be merged to the main branch and published on Zenodo with a DOI.\nThe BON in a Box team will contact you for any bug fixes or feature requests, and new releases will follow the versioning structure specified at the bottom of this page.\nDuring the development process of the pipeline, regularly merge the main branch to your branch to avoid potential conflicts when merging your branch."
  },
  {
    "objectID": "how_to_contribute.html#step-5-testing-your-script",
    "href": "how_to_contribute.html#step-5-testing-your-script",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 5: Testing your script",
    "text": "Step 5: Testing your script\nTo start testing:\n\nConnect to the server by running ./server-up.sh in a Linux terminal (e.g. Git Bash or WSL on Windows or Terminal on Mac). Make sure you are in the bon-in-a-box-pipeline directory on your computer.\nIn a web browser, type http://localhost/.\n\nYou can test your script in the “run script” tab of the tool. This will generate a form where you can enter sample inputs or use your default parameters to run the script for debugging."
  },
  {
    "objectID": "how_to_contribute.html#step-6-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor",
    "href": "how_to_contribute.html#step-6-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 6: Connect your scripts with the BON in a Box pipeline editor",
    "text": "Step 6: Connect your scripts with the BON in a Box pipeline editor\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from GitHub, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nA pipeline is a collection of steps that are connected to automate an analysis workflow. Each script becomes a pipeline step.\nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows the user to create pipelines by connecting individual scripts through inputs and outputs.\nThe left pane shows the available steps, which can be dragged onto the pipeline editor canvas.\n\n\nOn the right side, a collapsible pane allows the user to edit the labels and descriptions of the pipeline inputs and outputs.\n\nBelow that pane, there is a pane for writing the metadata of the pipeline.\n\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many steps, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as shown below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and the user will have to fill the value.\n\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons."
  },
  {
    "objectID": "how_to_contribute.html#step-7-run-pipeline-and-troubleshoot",
    "href": "how_to_contribute.html#step-7-run-pipeline-and-troubleshoot",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 7: Run pipeline and troubleshoot",
    "text": "Step 7: Run pipeline and troubleshoot\nThe last step is to load your saved pipeline in BON and a Box and run it in the “run pipelines” tab.\nYour pipeline should generate a form, which you can fill and then click run.\n\nYou can look at the log of each of your scripts to read errors and debug.\n\nOnce you change a script’s code and save to your local computer, the changes will immediately be implemented when you run the script through BON in a Box. However, if you change the YAML file, you will need to reload the page in the pipeline editor to see those changes. If this does not work, you may need to delete the script from the pipeline and re-drag it to the pipeline editor. Inputs and outputs can also be directly edited in the pipeline editor."
  },
  {
    "objectID": "protconn_pipeline.html",
    "href": "protconn_pipeline.html",
    "title": "Protected Connected Index",
    "section": "",
    "text": "The Protected Connected Index (ProtConn) is a component indicator in the Global Biodiversity Framework (GBF). ProtConn measures the percent of a given country or region that is conserved and managed through well-connected protected areas. This is an important indicator for assessing progress towards Goal A and Target 3 of the Kunming-Montreal Global Biodiversity Framework, which aim to have 30% of land area protected by a network of well-connected protected areas by 2030.\n\n\nProtConn can be used to assess current progress towards Goal A and Target 3 of the the GBF. The pipeline can also be used to compare the connectedness of different proposed protected areas, assisting with planning and design.\n\n\n\n\nOn larger datasets, the pipeline is slow and uses a lot of memory\nCurrently, the pipeline does not take into account landscape resistance (ie. whether areas between protected areas are easily traversed by species)\n\n\n\n\n\nTo use this pipeline, you’ll need a Protected Planet API key to access data on the World Database of Protected Areas. If you would like to run the pipeline with custom protected area data, ensure your data is in GeoPackage format and use the ProtConn Analysis with custom PAs pipeline.\n\n\n\n\n\nBON in a Box contains a pipeline to calculate ProtConn for a given country or region of interest. The pipeline has the following user inputs:\n\nISO3 country code: the user can input the ISO3 country code of the country of interest and the pipeline will pull the polygon and protected areas for this country.\nState/Province: the user can specify a state/province within the country of interest and the pipeline will pull the polygon and protected areas for this region. This is input as the full name of the state.\n\nPolygon of study area: there is also an option to add a custom study area file, which will override the polygon from the specified country or region of interest.\n\nPolygon of protected areas: this input should only be used if the user wants to use custom protected area data, for example if they want to calculate ProtConn for proposed protected areas or protected areas that are not yet in WDPA. If you use the ProtConn Analysis with WDPA pipeline, this input is optional and any file added will be combined with WDPA data of the country of interest. If you use the ProtConn Analysis with custom PAs pipeline, this input is mandatory and the pipeline will analyze only user-input protected area polygons.\nCoordinate Reference System: the coordinate reference system of choice. Search for a CRS of interest here. This needs to be a projected coordinate reference system for ProtConn calculations.\nDate Column Name: the user must indicate the name of the column in the custom protected area data file that specifies when the protected area was created (leave blank if only using WDPA data).\nDistance analysis threshold: the user can specify one or more dispersal distances depending on which species they are interested in. Common dispersal distances are 1,000 meters (1km), 10,000 m (10km) and 100,000 m (100 km) The dispersal distance is the median of the negative exponential dispersal kernel, meaning that at that distance there is a dispersal probability of 0.5. Note that larger dispersal distances will be more computationally intensive. \nType of distance matrix: the user can specify whether the distances between protected areas should be measured using the centroid (geometric center) of the protected area or the closest edge.\n \nYear for cutoff: the user can specify a year for the analysis. The analysis will only calculate values for protected areas that were established before this cutoff year.\nStart year: the start year of the time series of ProtConn values. The time series will calculate ProtConn for the protected areas established on or before the chosen year, and will count up to the cutoff year by the specified interval.\nYear interval: the year interval for the time series of ProtConn values. (eg. an input of 10 will calculate ProtConn values every 10 years).\nPA legal status types to include: the user can choose legal status types of WDPA data to include in the analysis. The protected areas can have a legal status of Designated, Inscribed, or Established.\nDesignated means that it is officially established under national or international law/policy.\nInscribed means that it is inscribed in an international list (e.g. World Heritage). This can overlap with designated.\nEstablished means that it is protected and managed, but possibly lacks formal legal designation.\nInclude UNESCO Biosphere reserves: the user can specify whether they want to include UNESCO Man and the Biosphere reserves in the analysis or not. These serve as learning sites for sustainable development and combine biodiversity conservation with the sustainable use of natural resources and sustainable development. They may not be legally protected and may not be fully conserved, as they are often used for development or human settlement. Excluding these will limit the dataset to meeting stricter conservation standards. Note that this is only relevant if using WDPA data.\nBuffer protected area points: the user can specify whether they want to buffer protected area points. For any protected areas that are represented as single points instead of polygons, this will create a circle around the polygon that is equal to the reported area. This will not affect the connectivity metrics if using centroids but may cause inaccuracies if assessing connectivity using the nearest edge. If left unchecked, all protected areas represented as points will be removed. Note that this is only relevant if using WDPA data.\nInclude marine protected areas: the user can specify whether they want to include marine protected areas in the analysis or not. Note that the analysis is still limited to the bounds of the study area polygon. This input is only relevant if using WDPA data.\nInclude OECMs: the user can specify whether they want to include other effective area-based conservation measures (OECMs) in the analysis or not. These areas are not officially designated protected areas but are still achieving conservation outcomes. This input is only relevant if using WDPA data.\n\n\n\n\n\n\nThis step retrieves protected areas in the country/region of interest from the WDPA database using the WDPA API. (This step is skipped if you are only using custom PA data).\n\n\n\nThis step returns the polygon for the country/region of interest.\n\n\n\nThis step cleans the data retrieved from the WDPA by correcting any geometry issues and filtering by the desired inputs. This step also crops the protected areas by the study area (This step is skipped if you are only using custom PA data).\n\n\n\nThis step performs the ProtConn analysis on the protected areas of interest. ProtConn is calculated by creating a pairwise matrix of the distances between each protected area. Then, it calculates the probability of a species dispersing between these protected areas using a negative exponential dispersal kernel with the input distance assigned to a probability of 0.5. This means that if the protected areas are very near one another, there is a high probability that species will be able to disperse between them, but this probability decays exponentially with increasing distance. Different dispersal distances can be specified based on the species of interest, as very small species such as rodents can not disperse as far as large mammals such as deer, so the connectedness would not be the same for those groups. Then, the dispersal probabilities between each of the protected areas are summed together, multiplied by the area of the protected areas, and divided by the area of the study area. Thus, ProtConn is the percentage of the total study area (country or region) that is protected with well-connected protected areas.\n\n\n\n\n\nProtConn results: The pipeline gives a table with several measures\n\nUnprotected - percentage of study area that is protected\nProtConn - percentage of the study area that is protected and connected\nProtUnconn - percentage of the study area that is protected and unconnected\n\nResult plot: donut plot of percentage of the study area that is protected and unconnected, and protected and connected.\nProtConn time series: plot of ProtConn over time, based on the dates that protected areas were established and the specified dispersal distance.\n\n\n\n\n\nSample run: See an example ProtConn run here in the [run ui] (https://pipelines-results.geobon.org/pipeline-form/Protconn-pipeline%3EProtConn_pipeline/1809e8c81dd453dd652d7904224e6522) and viewer.\n\n\n\nCommon errors:\n\nError: Could not retrieve protected areas from WDPA: if you encounter this error, it means the WDPA API is not able to retrieve the data for the country/region of interest. This sometimes happens with very large datasets and is a problem with the API itself, not the pipeline.\nError: Script produced no results. Check log for errors and make sure that the script calls biab_output.: if you encounter this error and you are running ProtConn for a large area with many protected areas, it is likely that Docker has terminated the process because you have run out of computer RAM. You may need to run the analysis with smaller areas or on a computer with more RAM.\n\n\n\n\nSaura, Santiago, Lucy Bastin, Luca Battistella, Andrea Mandrici, and Grégoire Dubois. “Protected Areas in the World’s Ecoregions: How Well Connected Are They?” Ecological Indicators 76 (May 1, 2017): 144–58. https://doi.org/10.1016/j.ecolind.2016.12.047.\nSaura, Santiago, Bastian Bertzky, Lucy Bastin, Luca Battistella, Andrea Mandrici, and Grégoire Dubois. “Protected Area Connectivity: Shortfalls in Global Targets and Country-Level Priorities.” Biological Conservation 219 (March 1, 2018): 53–67. https://doi.org/10.1016/j.biocon.2017.12.020.\nGodínez-Gómez, O. and Correa Ayram C.A. 2020. Makurhini: Analyzing landscape connectivity. 10.5281/zenodo.3771605"
  },
  {
    "objectID": "protconn_pipeline.html#introduction",
    "href": "protconn_pipeline.html#introduction",
    "title": "Protected Connected Index",
    "section": "",
    "text": "The Protected Connected Index (ProtConn) is a component indicator in the Global Biodiversity Framework (GBF). ProtConn measures the percent of a given country or region that is conserved and managed through well-connected protected areas. This is an important indicator for assessing progress towards Goal A and Target 3 of the Kunming-Montreal Global Biodiversity Framework, which aim to have 30% of land area protected by a network of well-connected protected areas by 2030.\n\n\nProtConn can be used to assess current progress towards Goal A and Target 3 of the the GBF. The pipeline can also be used to compare the connectedness of different proposed protected areas, assisting with planning and design.\n\n\n\n\nOn larger datasets, the pipeline is slow and uses a lot of memory\nCurrently, the pipeline does not take into account landscape resistance (ie. whether areas between protected areas are easily traversed by species)"
  },
  {
    "objectID": "protconn_pipeline.html#use-casecontext",
    "href": "protconn_pipeline.html#use-casecontext",
    "title": "Protected Connected Index",
    "section": "",
    "text": "ProtConn can be used to assess current progress towards the goals outlined in the introduction and for protected area planning and design."
  },
  {
    "objectID": "protconn_pipeline.html#pipeline-limitations",
    "href": "protconn_pipeline.html#pipeline-limitations",
    "title": "Protected Connected Index",
    "section": "",
    "text": "On larger datasets, the pipeline is slow and uses a lot of memory\nThe pipeline does not take into account landscape resistance (ie. whether areas between protected areas are easily traversed by species)"
  },
  {
    "objectID": "protconn_pipeline.html#before-you-start",
    "href": "protconn_pipeline.html#before-you-start",
    "title": "Protected Connected Index",
    "section": "",
    "text": "To use this pipeline, you’ll need a Protected Planet API key to access data on the World Database of Protected Areas. If you would like to run the pipeline with custom protected area data, ensure your data is in GeoPackage format and use the ProtConn Analysis with custom PAs pipeline."
  },
  {
    "objectID": "protconn_pipeline.html#running-the-pipeline",
    "href": "protconn_pipeline.html#running-the-pipeline",
    "title": "Protected Connected Index",
    "section": "",
    "text": "BON in a Box contains a pipeline to calculate ProtConn for a given country or region of interest. The pipeline has the following user inputs:\n\nISO3 country code: the user can input the ISO3 country code of the country of interest and the pipeline will pull the polygon and protected areas for this country.\nState/Province: the user can specify a state/province within the country of interest and the pipeline will pull the polygon and protected areas for this region. This is input as the full name of the state.\n\nPolygon of study area: there is also an option to add a custom study area file, which will override the polygon from the specified country or region of interest.\n\nPolygon of protected areas: this input should only be used if the user wants to use custom protected area data, for example if they want to calculate ProtConn for proposed protected areas or protected areas that are not yet in WDPA. If you use the ProtConn Analysis with WDPA pipeline, this input is optional and any file added will be combined with WDPA data of the country of interest. If you use the ProtConn Analysis with custom PAs pipeline, this input is mandatory and the pipeline will analyze only user-input protected area polygons.\nCoordinate Reference System: the coordinate reference system of choice. Search for a CRS of interest here. This needs to be a projected coordinate reference system for ProtConn calculations.\nDate Column Name: the user must indicate the name of the column in the custom protected area data file that specifies when the protected area was created (leave blank if only using WDPA data).\nDistance analysis threshold: the user can specify one or more dispersal distances depending on which species they are interested in. Common dispersal distances are 1,000 meters (1km), 10,000 m (10km) and 100,000 m (100 km) The dispersal distance is the median of the negative exponential dispersal kernel, meaning that at that distance there is a dispersal probability of 0.5. Note that larger dispersal distances will be more computationally intensive. \nType of distance matrix: the user can specify whether the distances between protected areas should be measured using the centroid (geometric center) of the protected area or the closest edge.\n \nYear for cutoff: the user can specify a year for the analysis. The analysis will only calculate values for protected areas that were established before this cutoff year.\nStart year: the start year of the time series of ProtConn values. The time series will calculate ProtConn for the protected areas established on or before the chosen year, and will count up to the cutoff year by the specified interval.\nYear interval: the year interval for the time series of ProtConn values. (eg. an input of 10 will calculate ProtConn values every 10 years).\nPA legal status types to include: the user can choose legal status types of WDPA data to include in the analysis. The protected areas can have a legal status of Designated, Inscribed, or Established.\nDesignated means that it is officially established under national or international law/policy.\nInscribed means that it is inscribed in an international list (e.g. World Heritage). This can overlap with designated.\nEstablished means that it is protected and managed, but possibly lacks formal legal designation.\nInclude UNESCO Biosphere reserves: the user can specify whether they want to include UNESCO Man and the Biosphere reserves in the analysis or not. These serve as learning sites for sustainable development and combine biodiversity conservation with the sustainable use of natural resources and sustainable development. They may not be legally protected and may not be fully conserved, as they are often used for development or human settlement. Excluding these will limit the dataset to meeting stricter conservation standards. Note that this is only relevant if using WDPA data.\nBuffer protected area points: the user can specify whether they want to buffer protected area points. For any protected areas that are represented as single points instead of polygons, this will create a circle around the polygon that is equal to the reported area. This will not affect the connectivity metrics if using centroids but may cause inaccuracies if assessing connectivity using the nearest edge. If left unchecked, all protected areas represented as points will be removed. Note that this is only relevant if using WDPA data.\nInclude marine protected areas: the user can specify whether they want to include marine protected areas in the analysis or not. Note that the analysis is still limited to the bounds of the study area polygon. This input is only relevant if using WDPA data.\nInclude OECMs: the user can specify whether they want to include other effective area-based conservation measures (OECMs) in the analysis or not. These areas are not officially designated protected areas but are still achieving conservation outcomes. This input is only relevant if using WDPA data.\n\n\n\n\n\n\nThis step retrieves protected areas in the country/region of interest from the WDPA database using the WDPA API. (This step is skipped if you are only using custom PA data).\n\n\n\nThis step returns the polygon for the country/region of interest.\n\n\n\nThis step cleans the data retrieved from the WDPA by correcting any geometry issues and filtering by the desired inputs. This step also crops the protected areas by the study area (This step is skipped if you are only using custom PA data).\n\n\n\nThis step performs the ProtConn analysis on the protected areas of interest. ProtConn is calculated by creating a pairwise matrix of the distances between each protected area. Then, it calculates the probability of a species dispersing between these protected areas using a negative exponential dispersal kernel with the input distance assigned to a probability of 0.5. This means that if the protected areas are very near one another, there is a high probability that species will be able to disperse between them, but this probability decays exponentially with increasing distance. Different dispersal distances can be specified based on the species of interest, as very small species such as rodents can not disperse as far as large mammals such as deer, so the connectedness would not be the same for those groups. Then, the dispersal probabilities between each of the protected areas are summed together, multiplied by the area of the protected areas, and divided by the area of the study area. Thus, ProtConn is the percentage of the total study area (country or region) that is protected with well-connected protected areas.\n\n\n\n\n\nProtConn results: The pipeline gives a table with several measures\n\nUnprotected - percentage of study area that is protected\nProtConn - percentage of the study area that is protected and connected\nProtUnconn - percentage of the study area that is protected and unconnected\n\nResult plot: donut plot of percentage of the study area that is protected and unconnected, and protected and connected.\nProtConn time series: plot of ProtConn over time, based on the dates that protected areas were established and the specified dispersal distance."
  },
  {
    "objectID": "protconn_pipeline.html#example",
    "href": "protconn_pipeline.html#example",
    "title": "Protected Connected Index",
    "section": "",
    "text": "Sample run: See an example ProtConn run here in the [run ui] (https://pipelines-results.geobon.org/pipeline-form/Protconn-pipeline%3EProtConn_pipeline/1809e8c81dd453dd652d7904224e6522) and viewer."
  },
  {
    "objectID": "protconn_pipeline.html#troubleshooting",
    "href": "protconn_pipeline.html#troubleshooting",
    "title": "Protected Connected Index",
    "section": "",
    "text": "Common errors:\n\nError: Could not retrieve protected areas from WDPA: if you encounter this error, it means the WDPA API is not able to retrieve the data for the country/region of interest. This sometimes happens with very large datasets and is a problem with the API itself, not the pipeline.\nError: Script produced no results. Check log for errors and make sure that the script calls biab_output.: if you encounter this error and you are running ProtConn for a large area with many protected areas, it is likely that Docker has terminated the process because you have run out of computer RAM. You may need to run the analysis with smaller areas or on a computer with more RAM."
  },
  {
    "objectID": "protconn_pipeline.html#references",
    "href": "protconn_pipeline.html#references",
    "title": "Protected Connected Index",
    "section": "",
    "text": "Saura, Santiago, Lucy Bastin, Luca Battistella, Andrea Mandrici, and Grégoire Dubois. “Protected Areas in the World’s Ecoregions: How Well Connected Are They?” Ecological Indicators 76 (May 1, 2017): 144–58. https://doi.org/10.1016/j.ecolind.2016.12.047.\nSaura, Santiago, Bastian Bertzky, Lucy Bastin, Luca Battistella, Andrea Mandrici, and Grégoire Dubois. “Protected Area Connectivity: Shortfalls in Global Targets and Country-Level Priorities.” Biological Conservation 219 (March 1, 2018): 53–67. https://doi.org/10.1016/j.biocon.2017.12.020.\nGodínez-Gómez, O. and Correa Ayram C.A. 2020. Makurhini: Analyzing landscape connectivity. 10.5281/zenodo.3771605"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Does your analysis pipeline fit in BON in a Box?",
    "section": "",
    "text": "Does your analysis pipeline fit in BON in a Box?\nWe are looking for pipelines that calculate biodiversity metrics that are relevant to policy and biodiversity reporting, such as Essential Biodiversity Variables (EBVs) and Indicators. These pipelines should be\n\nopen source, written in the coding languages R, Julia, or Python\nrelevant to many contexts (different species, countries, and regions)\n\n\n\nSteps for contributing\nWe strongly encourage those who have created or are developing code to calculate Essential Biodiversity Variables or indicators to share it through BON in a Box. The purpose of the modeling tool is to have state of the art processing pipelines shared across the community, for the benefit of all. We recommend following these steps: \n\nIdentify a workflow with a good potential for reuse by other organisations, and that can be mostly automated.\nDraw a high-level view of the pipeline that identifies each step, the inputs and outputs, how the scripts are put together, and the format of the final results.\nMeet the BON in a Box team. We will help design the pipeline, ensuring that each step is modular, identifying scripts that are already in the modeling tool to avoid rewriting code, and finding places where the existing code would need to be modified. For example, using cloud-optimized approaches such as STAC and Parquet files instead of downloading large files during the pipeline’s execution.\nSecure the resources. This could be the time of a graduate student or work study student. If applying for funding, include BON in a Box in your proposal as a tool.\nCreate the pipeline following the steps in the contribution instructions. We recommend starting with the simplest version and then adding complexity or features when needed.\nWhen you feel that the pipeline is complete and you have verified that it meets the quality requirement standards, you can submit the pipeline for peer review. While you are waiting, why not contribute by reviewing someone else’s contribution?\nOnce the reviews are complete, implement any necessary changes.\nIf accepted, the pipeline will be published in BON in a Box and Zenodo and will receive a DOI for identification. Now, the pipeline can be used by others.\n\nIf you wish to contribute a pipeline, please email us at boninabox@geobon.org."
  },
  {
    "objectID": "how_to_use.html#start-bon-in-a-box",
    "href": "how_to_use.html#start-bon-in-a-box",
    "title": "User Guide",
    "section": "Start BON in a Box",
    "text": "Start BON in a Box\n\nStart the services by running ./server-up.sh in a Linux terminal (e.g. Git Bash or WSL on Windows or Terminal on Mac). Make sure you are in the bon-in-a-box-pipeline directory on your computer.\nOpen a web browser and navigate to http://localhost/ for a local setup, or the instance’s address on the web if hosted on a server."
  },
  {
    "objectID": "how_to_contribute.html#pipeline_editor",
    "href": "how_to_contribute.html#pipeline_editor",
    "title": "Instructions for adapting your code to an analysis pipeline",
    "section": "Step 6: Connect your scripts with the BON in a Box pipeline editor",
    "text": "Step 6: Connect your scripts with the BON in a Box pipeline editor\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from GitHub, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nA pipeline is a collection of steps that are connected to automate an analysis workflow. Each script becomes a pipeline step.\nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows the user to create pipelines by connecting individual scripts through inputs and outputs.\nThe left pane shows the available steps, which can be dragged onto the pipeline editor canvas.\n\n\nOn the right side, a collapsible pane allows the user to edit the labels and descriptions of the pipeline inputs and outputs.\n\nBelow that pane, there is a pane for writing the metadata of the pipeline.\n\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many steps, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as shown below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and the user will have to fill the value.\n\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons."
  }
]