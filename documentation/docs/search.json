[
  {
    "objectID": "how_to_contribute.html",
    "href": "how_to_contribute.html",
    "title": "How to Contribute",
    "section": "",
    "text": "If you wish to contribute a pipeline, please email us at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation."
  },
  {
    "objectID": "how_to_contribute.html#contributing",
    "href": "how_to_contribute.html#contributing",
    "title": "How to Contribute",
    "section": "",
    "text": "If you wish to contribute a pipeline, please email us at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation."
  },
  {
    "objectID": "how_to_contribute.html#is-your-analysis-pipeline-right-for-us",
    "href": "how_to_contribute.html#is-your-analysis-pipeline-right-for-us",
    "title": "How to Contribute",
    "section": "Is your analysis pipeline right for us?",
    "text": "Is your analysis pipeline right for us?\nWe are looking for pipelines that calculate biodiversity metrics that are relevant to policy and biodiversity reporting, such as Essential Biodiversity Variables (EBVs) and Indicators. These pipelines should be\n\nopen source\nbroadly applicable (not just a specific region)\nusing data that is publicly available"
  },
  {
    "objectID": "how_to_contribute.html#instructions-for-adapting-your-code-to-an-analysis-pipeline",
    "href": "how_to_contribute.html#instructions-for-adapting-your-code-to-an-analysis-pipeline",
    "title": "How to Contribute",
    "section": "Instructions for adapting your code to an analysis pipeline",
    "text": "Instructions for adapting your code to an analysis pipeline\nAnalysis workflows can be adapted to BON in a Box pipelines using a few simple steps. Once BON in a Box is installed on your computer, Pipelines can be edited and run locally by creating files in your local BON-in-a-box folder that was cloned from the GItHub repository. Workflows can be in a variety of languages, including R, Julia, and Python, and not all steps of the pipeline need to be in the same language.\nAnalysis workflows can be converted to BON in a Box pipelines with a few steps:\n\nStep 1: Creating a YAML file\nEach script should be accompanied by a YAML file (.yml extension) that specifies inputs and outputs of that step of the pipeline.\nYAML files should begin with a name, description, and authors of the script. The name should be the same as the script that it describes but with a .yml extension.\nExample:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\nNext, the YAML file should specify the inputs of the script. These inputs will either be outputs from the previous script, which can be connected by lines in the pipeline editor, or specified by the user in the input form.\nHere is an example where the input is a raster file of land cover pulled from a public database and a coordinate reference system that is specified by the user.\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\nSpecifying an example will autofill that input with the example unless changed by the user. Not specifying an example will leave that input blank.\nAfter specifying the inputs, you can specify what you want the script to output.\nHere is an example:\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nThere are several different types of user inputs that can be specified in the YAML file:\n\n\n\n\n\n\n\n\n“Type” attribute in the yaml\nIU rendering\nDescription\n\n\n\n\nboolean\nPlain text\ntrue/false\n\n\nfloat, float[]\nPlain text\npositive and negative numbers with a decimal point\n\n\nint, int[]\nPlain text\ninteger\n\n\noptions (INSERT FOOTNOTE)\nPlain text\ndropdown menu\n\n\ntext, text[]\nPlain text\ntext input (words)\n\n\n(any unknown type)\nPlain text\n\n\n\n\nThere are also several types of outputs that can be created\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON\napplication/geo+json\nMap\n\n\nGeoPackage\napplication/geopackage+sqlite3\nLink\n\n\nGeoTIFF\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nJPG\nimage/jpg\ntag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\nWhen used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\noptions type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\nHere is an example of a complete YAML file with inputs and outputs:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nYAML is a space sensitive format, so make sure all the tab sets are correct in the file.\nIf inputs are outputs from a previous step in the pipeline, make sure to give the inputs and outputs the same name and they will be automatically linked in the pipeline.\nHere is an empty commented sample that can be filled in\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example\n\n\nStep 2: Integrating YAML inputs and outputs into your script\nNow that you have created your YAML file, the inputs and outputs need to be integrated into your scripts.\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle:\n\nScript launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (number, string, etc.)\n\nSee empty R script for a minimal script lifecycle example.\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads the input.json file that is in the run folder. This JSON file is automatically created from the BON in a Box pipeline. Here is an example of how to read the input folder in the R script:\nFirst, set the environment for the script:\nSys.getenv(\"SCRIPT_LOCATION\")\nAnd then set the ‘input’ environment variables. The ‘input’ environment contains the specified inputs from the BON in a Box platform.\ninput &lt;- rjson::fromJSON(file=file.path(outputFolder, \"input.json\")) # Load input file\nNext, for any functions that are using these inputs, they need to be specified.\nInputs are specified by input$input_name.\nFor example, to create a function to take the square root of a number that was input by the user in the UI:\nresult &lt;- sqrt(input$number)\nThis means if the user inputs the number 144, this will be run in the function and output 12.\nHere is another example to read in an output from a previous step of the pipeline, which was output in csv format.\ndat &lt;- read.csv(input$csv_file)\nNext, you have to specify the outputs, which can be saved in a variety of formats. The format must be accurately specified in the YAML file.\nHere is an example of an output with a csv file and a plot in png format.\nresult_path &lt;- file.path(outputFolder, \"result.csv\")\nwrite.csv(result, result_path, row.names = F )\n\nresult_plot_path &lt;- file.path(outputFolder, \"result_plot.png\")\nggsave(result_path, result)\n\noutput &lt;- list(result = result_path,\nresult_plot = result_plot_path)\nLastly, write the output as a JSON file:\nsetwd(outputFolder)\njsonlite::write_json(output, \"output.json\", auto_unbox = TRUE, pretty = TRUE)\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\nScript dependencies\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies.\n\n\n\nStep 3: Connect your scripts with the BON in a Box pipeline editor.\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from github, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nADD PHOTOS.\n\n\nStep 4: Run pipeline and troubleshoot\nThe last step is to load your saved pipeline in BON and a Box and run it. The error log at the bottom of the page will show errors with the pipeline.\nADD PHOTO"
  },
  {
    "objectID": "how_to_install.html",
    "href": "how_to_install.html",
    "title": "How to Install",
    "section": "",
    "text": "BON in a Box can be installed and run locally on your local computer. This allows the user to add scripts and create pipelines on their local computer to be run through BON in a Box. Here are the instructions for installing BON in a Box to run on a local computer."
  },
  {
    "objectID": "how_to_install.html#running-the-servers-locally",
    "href": "how_to_install.html#running-the-servers-locally",
    "title": "How to Install",
    "section": "Running the servers locally",
    "text": "Running the servers locally\nPrerequisites :\n\nGit - A github account, with an SSH key registered. See Adding a new SSH key to your GitHub account.\nAt least 6 GB of free space (this includes the installation of Docker Desktop) - RAM requirements will depend on the scripts that you run.\nWindows:\n\nDocker Desktop - Note that it is not necessary to make an account\n\nA Linux shell (git bash, Bash through PowerShell, cygwin, etc.) necessary to run th .sh scripts in the instructions below.\nMac:\n\nDocker Desktop Note that it is not necessary to make an account\nMake sure docker is added to the path of your terminal. From a terminal, run command docker run hello-world. If there is an error message, see https://stackoverflow.com/a/71923962/3519951.\nIf you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951.\n\nLinux: Docker with Docker Compose installed. It is recommended to add your user to the docker group.\n\nTo run:\n\nClone repository (Windows users: do not clone the repo in a folder under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop.\nProvide the environment variables: - Open the newly cloned repository on your computer - Find the file called runner-sample.env - Duplicate the file and rename the copy to runner.env. - Fill the properties depending on what you intend to run. - Adjust any server option as you see fit.\nUsing a linux terminal (terminal on Mac or Git Bash), navigate to top-level folder.\nIn the linux terminal, type ./server-up.sh\n\nMake sure you have docker open and running on your computer.\nThe first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes.\nNetwork problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure.\nWindows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization. Access to the BIOS may be required to enable virtualization)\n\nType http://localhost/ to open BON in a Box\nRun ./server-down.sh in a terminal to stop the server when done\nOn Windows, to completely stop the processes, you might have to run wsl --shutdown\n\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted: - When modifying an existing script, simply re-run the script from the UI and the new version will be executed. - When adding or renaming scripts, refresh the browser page.\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted: - In the pipeline editor, click save, paste the file to your file in the pipeline folder and run it from the “pipeline run” page. - When adding or renaming pipelines, refresh the browser page."
  },
  {
    "objectID": "how_to_install.html#running-the-servers-remotely",
    "href": "how_to_install.html#running-the-servers-remotely",
    "title": "How to Install",
    "section": "Running the servers remotely",
    "text": "Running the servers remotely\nUse the ansible playbook instructions."
  },
  {
    "objectID": "how_to_install.html#running-a-script-or-pipeline",
    "href": "how_to_install.html#running-a-script-or-pipeline",
    "title": "How to Install",
    "section": "Running a script or pipeline",
    "text": "Running a script or pipeline\nYou have an instance of BON in a Box running, either locally or remotely, and you want to run your first script or pipeline.\nThere is one page to run scripts, and one to run pipelines. Select the script or pipelines from the dropdown and fill the form.\nThe form might ask you for a file. In order to provide a file that you own locally, upload or copy it to the userdata folder. You can then refer to it with a url as such: /userdata/myFile.shp, or /userdata/myFolder/myFile.shp if there are subfolders."
  },
  {
    "objectID": "how_to_install.html#scripts",
    "href": "how_to_install.html#scripts",
    "title": "How to Install",
    "section": "Scripts",
    "text": "Scripts\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle:\n\nScript launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (number, string, etc.)\n\nSee empty R script for a minimal script lifecycle example.\n\nDescribing a script\nThe script description is in a .yml file next to the script. It is necessary for the script to be found and connected to other scripts in a pipeline.\nHere is an empty commented sample:\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example\n\nInput and output types\nEach input and output must declare a type, in lowercase. It can be a primitive or a file.\nThe following primitive types are accepted:\n\n\n\n“Type” attribute in the yaml\nIU rendering\n\n\n\n\nboolean\nPlain text\n\n\nfloat, float[]\nPlain text\n\n\nint, int[]\nPlain text\n\n\noptions (INSERT FOOTNOTE)\nPlain text\n\n\ntext, text[]\nPlain text\n\n\n(any unknown type)\nPlain text\n\n\n\nAny MIME type is accepted. Here are a few common ones:\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON\napplication/geo+json\nMap\n\n\nGeoTIFF (INSERT FOOTNOTE)\napplication/geopackage+sqlite3\nLink\n\n\nJPG\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nJPG\nimage/jpg\n tag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\nSearch the web to find the appropriate MIME type for your content. Here are a few references: - http://www.iana.org/assignments/media-types/media-types.xhtml - http://svn.apache.org/viewvc/httpd/httpd/trunk/docs/conf/mime.types?view=markup\nWhen used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\noptions type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\n\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\nScript dependencies\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies.\n\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads and uses inputs from the input.json file. Example in R:\n## Receiving arguments from input.json.\n## outputFolder is already defined by server\nlibrary(\"rjson\")\ninput &lt;- fromJSON(file=file.path(outputFolder, \"input.json\"))\n\n## Can now be accessed from the map\nprint(input$predictors)\nThe script should perform appropriate parameter validation.\nNote that the inputs will be null if the user left the text box empty.\n\n\nGenerating outputs\nThe output files generated by the script must be saved in the run folder. The script must also generate an output.json file in the same folder, that contains a map associating the output ids to their values. Example:\n{\n  \"sdm_pred\": \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_pred.tif\",\n  \"sdm_runs\":[\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_1.tif\",\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_2.tif\"\n  ]\n}"
  },
  {
    "objectID": "how_to_install.html#pipelines",
    "href": "how_to_install.html#pipelines",
    "title": "How to Install",
    "section": "Pipelines",
    "text": "Pipelines\nA pipeline is a collection of steps to acheive the desired processing. Each script becomes a pipeline step. \nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output (rightmost red box in image above). Pipeline IO supports the same types and UI rendering as individual steps, since its inputs are directly fed to the steps, and outputs come from the step outputs.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows you to create pipelines by plugging steps together.\nThe left pane shows the available steps, the main section shows the canvas.\nOn the right side, a collapsible pane allows to edit the labels and descriptions of the pipeline inputs and outputs.\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\n\n\nimage\n\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many step, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and user will have to fill the value.\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\n\n\nimage\n\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons.\nIn the event that saving has been disabled on your server instance, the save button will display “Save to clipboard”. To save your modifications: 1. Click save: the content is copied to your clipboard. 2. Make sure you are up to date (e.g. git pull --rebase). 3. Remove all the content of the target file. 4. Paste content and save. 5. Commit and push on a branch using git. 6. To share your modifications, create a pull request for that branch through the github UI."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "BON in a Box serves as a hub for Biodiversity Observation Networks (BONs), consolidating diverse tools and resources to enhance data analysis and collaboration. It’s the ultimate ecosystem for harmonizing biodiversity observations, fostering efficiency, and amplifying the power of data-driven insights.\nBON in a Box now features a community-contributed and open-source data to essential biodiversity variable (EBV) and indicator workflow repository curated by GEO BON. Its engine allows to seamlessly connect steps in different languages (R, Julia, Python) through a modular pipeline approach that reads both from local and global data sources. These pipelines can calculate EBVs and indicators using publicaly available data or user-provided data."
  },
  {
    "objectID": "about.html#what-is-bon-in-a-box",
    "href": "about.html#what-is-bon-in-a-box",
    "title": "About",
    "section": "",
    "text": "BON in a Box serves as a hub for Biodiversity Observation Networks (BONs), consolidating diverse tools and resources to enhance data analysis and collaboration. It’s the ultimate ecosystem for harmonizing biodiversity observations, fostering efficiency, and amplifying the power of data-driven insights.\nBON in a Box now features a community-contributed and open-source data to essential biodiversity variable (EBV) and indicator workflow repository curated by GEO BON. Its engine allows to seamlessly connect steps in different languages (R, Julia, Python) through a modular pipeline approach that reads both from local and global data sources. These pipelines can calculate EBVs and indicators using publicaly available data or user-provided data."
  },
  {
    "objectID": "about.html#why-do-we-need-this-platform",
    "href": "about.html#why-do-we-need-this-platform",
    "title": "About",
    "section": "Why do we need this platform?",
    "text": "Why do we need this platform?\nScientists around the world are developing tools to create workflows to calculate essential biodiversity variables and indicators to report on biodiversity change. However, there are almost no platforms to share these workflows and tools, leading to duplication of effort. BON in a Box is a platform to consolidate these tools and facilitate coordination and networking between scientists, organizations, and governments to face the global biodiversity crisis."
  },
  {
    "objectID": "about.html#what-are-the-project-benefits",
    "href": "about.html#what-are-the-project-benefits",
    "title": "About",
    "section": "What are the project benefits?",
    "text": "What are the project benefits?\na. Academics/researchers/users (BONs) • Improve science further, through open and repeatable processes increasing research impact.\n• Complement your analysis processes with routines developed by other researchers and to be part of a global community that generates analysis of the state and trends of biodiversity.\nb. Decision Makers\nWith BON in a Box you will be able to:\n• Obtain insights and results from your own data as needed to report to multilateral agreements and other public policy commitments with validated tools and methodologies.\n• Optimize resources and efforts for quick and informed environmental decision-making"
  },
  {
    "objectID": "working_with_stac.html",
    "href": "working_with_stac.html",
    "title": "Working with STAC",
    "section": "",
    "text": "STAC stands for SpatioTemporal Asset Catalog. It is a common language to describe geospatial information, so it can be more easily worked with, indexed, and discovered. Spatial information is stored in a geoJSON format, which can be easily adapted to different domains. The goal of STAC is to enable a global index of all imagery derived data products with an easily implementable standard for organization to expose their data in a persistent and reliable way. STAC objects are hosted in geoJSON format as HTML pages."
  },
  {
    "objectID": "working_with_stac.html#what-is-a-stac-catalog",
    "href": "working_with_stac.html#what-is-a-stac-catalog",
    "title": "Working with STAC",
    "section": "",
    "text": "STAC stands for SpatioTemporal Asset Catalog. It is a common language to describe geospatial information, so it can be more easily worked with, indexed, and discovered. Spatial information is stored in a geoJSON format, which can be easily adapted to different domains. The goal of STAC is to enable a global index of all imagery derived data products with an easily implementable standard for organization to expose their data in a persistent and reliable way. STAC objects are hosted in geoJSON format as HTML pages."
  },
  {
    "objectID": "working_with_stac.html#geo-bon-stac-catalog",
    "href": "working_with_stac.html#geo-bon-stac-catalog",
    "title": "Working with STAC",
    "section": "GEO BON STAC Catalog",
    "text": "GEO BON STAC Catalog\nPipelines can be created using layers pulled from any STAC catalog, such as the Planetary Computer. GEO BON hosts a STAC catalog that contains many commonly used data layers, which can be viewed in JSON format in an HTML or in a viewer.\nThere is R script to pull and filter data from a STAC catalog in BON in a Box called ‘Load from STAC’, where users can specify the layers, bounding box, projection system, and spatial resolution they are interested in, and save this as a GeoJSON file to input into a subsequent pipeline step."
  },
  {
    "objectID": "working_with_stac.html#working-with-the-stac-catalog-in-r",
    "href": "working_with_stac.html#working-with-the-stac-catalog-in-r",
    "title": "Working with STAC",
    "section": "Working with the STAC Catalog in R",
    "text": "Working with the STAC Catalog in R\nData in STAC catalogs can be loaded directly into R and visualized using the rstac package.\nTo start, install and load the following packages.\n\n# install.packages(c(('gdalcubes', 'rstac', 'knitr')))\nlibrary(gdalcubes)\nlibrary(rstac)\nlibrary(knitr)\nlibrary(stars)\n\nLoading required package: abind\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nNext, connect to the STAC catalog using the stac function in the rstac package\n\nstac_obj &lt;- stac(\"https://stac.geobon.org/\")\n\nIn the STAC catalog, layers are organized into collections. To make a list of the collections\n\ncollections &lt;- stac_obj |&gt; collections() |&gt; get_request()\n\nAnd to view the collections and their descriptions in a table\n\ndf&lt;-data.frame(id=character(),title=character(),description=character())\nfor (c in collections[['collections']]){\n  df&lt;-rbind(df,data.frame(id=c$id,title=c$title,description=c$description))\n}\nkable(head(df))\n\n\n\n\n\n\n\n\n\nid\ntitle\ndescription\n\n\n\n\nchelsa-clim\nCHELSA Climatologies\nCHELSA Climatologies\n\n\ngfw-lossyear\nGlobal Forest Watch - Loss year\nGlobal Forest Watch - Loss year\n\n\nsoilgrids\nSoil Grids datasets\nSoilGrids aggregated datasets at 1km resolution\n\n\nghmts\nGlobal Human Modification of Terrestrial Systems\nThe Global Human Modification of Terrestrial Systems data set provides a cumulative measure of the human modification of terrestrial lands across the globe at a 1-km resolution. It is a continuous 0-1 metric that reflects the proportion of a landscape modified, based on modeling the physical extents of 13 anthropogenic stressors and their estimated impacts using spatially-explicit global data sets with a median year of 2016.\n\n\ngfw-treecover2000\nGlobal Forest Watch - Tree cover 2000\nGlobal Forest Watch - Tree cover 2000\n\n\ngfw-gain\nGlobal Forest Watch - Gain\nGlobal Forest Watch - Gain\n\n\n\n\n\nTo search for a specific collection. Here we will search for the land cover collection from EarthEnv\n\nit_obj &lt;- stac_obj |&gt;\n  stac_search(collections = \"earthenv_landcover\") |&gt;\n  post_request() |&gt; items_fetch()\nit_obj\n\n###Items\n- features (12 item(s)):\n  - class_9\n  - class_8\n  - class_7\n  - class_6\n  - class_5\n  - class_4\n  - class_3\n  - class_2\n  - class_12\n  - class_11\n  - class_10\n  - class_1\n- assets: data\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nView the available layers in this collection\n\nit_obj &lt;- stac_obj |&gt;\n  collections(\"earthenv_landcover\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nit_obj\n\n###Items\n- features (12 item(s)):\n  - class_9\n  - class_8\n  - class_7\n  - class_6\n  - class_5\n  - class_4\n  - class_3\n  - class_2\n  - class_12\n  - class_11\n  - class_10\n  - class_1\n- assets: data\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nView the properties of the first item (layer).\n\nit_obj[['features']][[1]]$properties\n\n$class\n[1] \"9\"\n\n$datetime\n[1] \"2000-01-01T00:00:00Z\"\n\n$`proj:epsg`\n[1] 4326\n\n$description\n[1] \"Urban/Built-up\"\n\n$full_filename\n[1] \"consensus_full_class_9.tif\"\n\n\nWe can summarize each layer in a table\n\ndf&lt;-data.frame(id=character(),datetime=character(), description=character())\nfor (f in it_obj[['features']]){\n  df&lt;-rbind(df,data.frame(id=f$id,datetime=f$properties$datetime,description=f$properties$description))\n}\nkable(head(df))\n\n\n\n\nid\ndatetime\ndescription\n\n\n\n\nclass_9\n2000-01-01T00:00:00Z\nUrban/Built-up\n\n\nclass_8\n2000-01-01T00:00:00Z\nRegularly Flooded Vegetation\n\n\nclass_7\n2000-01-01T00:00:00Z\nCultivated and Managed Vegetation\n\n\nclass_6\n2000-01-01T00:00:00Z\nHerbaceous Vegetation\n\n\nclass_5\n2000-01-01T00:00:00Z\nShrubs\n\n\nclass_4\n2000-01-01T00:00:00Z\nMixed/Other Trees\n\n\n\n\n\nNow, you can load one of the items with the stars package, which allows access to files remotely in a fast and efficient way.\nHere we will select the layer representing the percentage of “Evergreen/Deciduous Needleleaf Trees”, which is the 12th layer in the collection.\n\nlc1&lt;-read_stars(paste0('/vsicurl/',it_obj[['features']][[12]]$assets$data$href), proxy = TRUE)\nplot(lc1)\n\ndownsample set to 23\n\n\n\n\n\n\n\n\n\nYou can crop the raster by a bounding box and visualize it.\n\nbbox&lt;-st_bbox(c(xmin = -76, xmax = -70, ymax = 54, ymin = 50), crs = st_crs(4326))\nlc2 &lt;- lc1 |&gt; st_crop(bbox)\n\npal &lt;- colorRampPalette(c(\"black\",\"darkblue\",\"red\",\"yellow\",\"white\"))\nplot(lc2,breaks=seq(0,100,10),col=pal(10))\n\n\n\n\n\n\n\n\nYou can now save this object to your computer in Cloud Optimized GeoTiff (COG) format. COG works on the cloud and allows the user to access just the part of the file that is needed because all relevant information is stored together in each tile.\n# Replace \" ~ \" for the directory of your choice on your computer.\nwrite_stars(lc2,'~/lc2.tif',driver='COG',options=c('COMPRESS=DEFLATE'))\nNote that for a layer with categorical variables, saving is more complex.\nlc1 |&gt; st_crop(bbox) |&gt; write_stars('~/lc1.tif', driver='COG', RasterIO=c('resampling'='mode'), \noptions=c('COMPRESS=DEFLATE', 'OVERVIEW_RESAMPLING=MODE', 'LEVEL=6', \n'OVERVIEW_COUNT=8', 'RESAMPLING=MODE', 'WARP_RESAMPLING=MODE', 'OVERVIEWS=IGNORE_EXISTING'))"
  },
  {
    "objectID": "working_with_stac.html#working-with-gdal-cubes",
    "href": "working_with_stac.html#working-with-gdal-cubes",
    "title": "Working with STAC",
    "section": "Working with GDAL Cubes",
    "text": "Working with GDAL Cubes\nThe R package gdalcubes allows for the processing of four dimensional regular raster data cubes from irregular image collections, hiding complexities in he data such as different map projections and spatial overlaps of images or different spatial resolutions of spectral bands. This can be useful for working with many layers or time series of satellite imagery.\nThis is a necessary step for GDAL cubes to work with STAC catalog data\n\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\n\nYou can filter based on item properties and create a collection\n\nst &lt;- stac_image_collection(it_obj$features, asset_names=c('data'), \n             property_filter = function(f){f[['class']] %in% c('1','2','3','4')},srs='EPSG:4326')\n\nNext, you can build a GDAL cube to process or visualize the data. This cube can be in a different CRS and resolution from those of the original elements/files. However, the time dimension must capture the temporal framework of the item. dt is expressed as a time period, P1M is a period of one month, P1Y is a period of one year. Resampling methods should be tailored to the data type. For categorical data, use “mode” or “nearest”. For continuous data, use “bilinear”. Aggregation is relevant only when multiple rasters overlap.\nHere is an example that sums the four forest categories using aggregation=“sum”, with a change of the reference system to use Quebec Lambert (EPSG:32198) and a resolution of 1 km.\n\nbbox&lt;-st_bbox(c(xmin = -483695, xmax = -84643, ymin = 112704 , ymax = 684311), crs = st_crs(32198))\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2000-01-01\", t1 = \"2000-01-01\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin), \n                                                dx=1000, dy=1000, dt=\"P1D\", aggregation = \"sum\", resampling = \"mean\")\n\nCreate a raster cube\n\nlc_cube &lt;- raster_cube(st, v)\n\nSave the resulting file to your computer\n# Replace \" ~ \" for the directory of your choice on your computer.\nlc_cube |&gt; write_tif('~/',prefix='lc2',creation_options=list('COMPRESS'='DEFLATE'))\nPlot the raster cube\n\nlc_cube |&gt; plot(zlim=c(0,100),col=pal(10))\n\n\n\n\n\n\n\n\nUse the accessibility from cities dataset, keeping the same CRS and extent\n\nit_obj &lt;- stac_obj |&gt;\n  collections(\"accessibility_to_cities\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2015-01-01\", t1 = \"2015-01-01\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin), \n                                                dx=1000, dy=1000, dt=\"P1D\", aggregation\n\n = \"mean\", resampling = \"bilinear\")\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\nst &lt;- stac_image_collection(it_obj$features)\nlc_cube &lt;- raster_cube(st, v)\nlc_cube |&gt; plot(col=heat.colors)\n\n\n\n\n\n\n\n\nUse the CHELSA dataset on climatologies and create an average map for the months of June, July, and August 2010 to 2019\nit_obj &lt;- s_obj |&gt;\n  stac_search(collections = \"chelsa-monthly\", datetime=\"2010-06-01T00:00:00Z/2019-08-01T00:00:00Z\") |&gt; \n  get_request() |&gt; items_fetch()\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2010-06-01\", t1 = \"2019-08-31\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin),\n               dx=1000, dy=1000, dt=\"P10Y\",\n               aggregation = \"mean\",\n               resampling = \"bilinear\")\n\nfor (i in 1:length(it_obj$features)){\n  names(it_obj$features[[i]]$assets)='data'\n  it_obj$features[[i]]$assets$data$roles='data'\n}\nanames=unlist(lapply(it_obj$features,function(f){f['id']}))\nst &lt;- stac_image_collection(it_obj$features, asset_names = 'data',  \n                       property_filter = function(f){f[['variable']] == 'tas' & (f[['month']] %in% c(6,7,8)) })\nc_cube &lt;- raster_cube(st, v)\nc_cube |&gt; plot(col=heat.colors)\n\nc_cube |&gt; write_tif('~/',prefix='chelsa-monthly',creation_options=list('COMPRESS'='DEFLATE'))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BON in a Box: A tool for monitoring biodiversity",
    "section": "",
    "text": "The earth’s biodiversity is declining at an alarming rate due to human activities, necessitating urgent coordinated action to halt this loss and restore biodiversity. Recognizing this need, 196 countries have signed the Kunming-Montreal Global Biodiversity Framework, committing to taking steps to halt this loss. Assessing progress towards the goals and targets of the Kunming-Montreal Global Biodiversity Framework (GBF) requires collaboration across scales to monitor biodiversity change. Current monitoring efforts and tools are developed by individual actors without being shared across organizations and borders, which can lead to the duplication of effort in some places while others have less resources to allocate to biodiversity reporting.\nBON in a Box can provide biodiversity monitoring tools and facilitate coordination and collaboration among countries. BON in a Box now provides an open-source, fully transparent platform that allows users to contribute data and analysis pipelines to calculate indicators which can then be used for reporting to assess progress towards biodiversity targets. BON in Box can integrate diverse data and expert knowledge, from Earth observation to genetic diversity metrics to calculate monitoring framework indicators.\nBON in a Box Website"
  },
  {
    "objectID": "how_to_contribute.html#step-1-creating-a-yaml-file",
    "href": "how_to_contribute.html#step-1-creating-a-yaml-file",
    "title": "How to Contribute",
    "section": "Step 1: Creating a YAML file",
    "text": "Step 1: Creating a YAML file\nEach script should be accompanied by a YAML file (.yml extension) that specifies inputs and outputs of that step of the pipeline.\nYAML files should begin with a name, description, and authors of the script. The name should be the same as the script that it describes but with a .yml extension.\nExample:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\nNext, the YAML file should specify the inputs of the script. These inputs will either be outputs from the previous script, which can be connected by lines in the pipeline editor, or specified by the user in the input form.\nHere is an example where the input is a raster file of land cover pulled from a public database and a coordinate reference system that is specified by the user.\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\nSpecifying an example will autofill that input with the example unless changed by the user. Not specifying an example will leave that input blank.\nAfter specifying the inputs, you can specify what you want the script to output.\nHere is an example:\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nThere are several different types of user inputs that can be specified in the YAML file:\n\n\n\n\n\n\n\n\n“Type” attribute in the yaml\nIU rendering\nDescription\n\n\n\n\nboolean\nPlain text\ntrue/false\n\n\nfloat, float[]\nPlain text\npositive and negative numbers with a decimal point\n\n\nint, int[]\nPlain text\ninteger\n\n\noptions (INSERT FOOTNOTE)\nPlain text\ndropdown menu\n\n\ntext, text[]\nPlain text\ntext input (words)\n\n\n(any unknown type)\nPlain text\n\n\n\n\nThere are also several types of outputs that can be created\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON\napplication/geo+json\nMap\n\n\nGeoPackage\napplication/geopackage+sqlite3\nLink\n\n\nGeoTIFF\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nJPG\nimage/jpg\ntag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\nWhen used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\noptions type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\nHere is an example of a complete YAML file with inputs and outputs:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nYAML is a space sensitive format, so make sure all the tab sets are correct in the file.\nIf inputs are outputs from a previous step in the pipeline, make sure to give the inputs and outputs the same name and they will be automatically linked in the pipeline.\nHere is an empty commented sample that can be filled in\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example"
  },
  {
    "objectID": "how_to_contribute.html#step-2-integrating-yaml-inputs-and-outputs-into-your-script",
    "href": "how_to_contribute.html#step-2-integrating-yaml-inputs-and-outputs-into-your-script",
    "title": "How to Contribute",
    "section": "Step 2: Integrating YAML inputs and outputs into your script",
    "text": "Step 2: Integrating YAML inputs and outputs into your script\nNow that you have created your YAML file, the inputs and outputs need to be integrated into your scripts.\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle:\n\nScript launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (number, string, etc.)\n\nSee empty R script for a minimal script lifecycle example.\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads the input.json file that is in the run folder. This JSON file is automatically created from the BON in a Box pipeline. Here is an example of how to read the input folder in the R script:\nFirst, set the environment for the script:\nSys.getenv(\"SCRIPT_LOCATION\")\nAnd then set the ‘input’ environment variables. The ‘input’ environment contains the specified inputs from the BON in a Box platform.\ninput &lt;- rjson::fromJSON(file=file.path(outputFolder, \"input.json\")) # Load input file\nNext, for any functions that are using these inputs, they need to be specified.\nInputs are specified by input$input_name.\nFor example, to create a function to take the square root of a number that was input by the user in the UI:\nresult &lt;- sqrt(input$number)\nThis means if the user inputs the number 144, this will be run in the function and output 12.\nHere is another example to read in an output from a previous step of the pipeline, which was output in csv format.\ndat &lt;- read.csv(input$csv_file)\nNext, you have to specify the outputs, which can be saved in a variety of formats. The format must be accurately specified in the YAML file.\nHere is an example of an output with a csv file and a plot in png format.\nresult_path &lt;- file.path(outputFolder, \"result.csv\")\nwrite.csv(result, result_path, row.names = F )\n\nresult_plot_path &lt;- file.path(outputFolder, \"result_plot.png\")\nggsave(result_path, result)\n\noutput &lt;- list(result = result_path,\nresult_plot = result_plot_path)\nLastly, write the output as a JSON file:\nsetwd(outputFolder)\njsonlite::write_json(output, \"output.json\", auto_unbox = TRUE, pretty = TRUE)\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\nScript dependencies\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies."
  },
  {
    "objectID": "how_to_contribute.html#step-3-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor.",
    "href": "how_to_contribute.html#step-3-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor.",
    "title": "How to Contribute",
    "section": "Step 3: Connect your scripts with the BON in a Box pipeline editor.",
    "text": "Step 3: Connect your scripts with the BON in a Box pipeline editor.\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from github, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nA pipeline is a collection of steps to acheive the desired processing. Each script becomes a pipeline step.\nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output (rightmost red box in image above). Pipeline IO supports the same types and UI rendering as individual steps, since its inputs are directly fed to the steps, and outputs come from the step outputs.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows you to create pipelines by plugging steps together.\nThe left pane shows the available steps, the main section shows the canvas.\nOn the right side, a collapsible pane allows to edit the labels and descriptions of the pipeline inputs and outputs.\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\n\n\nimage\n\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many step, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and user will have to fill the value.\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\n\n\nimage\n\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons.\nIn the event that saving has been disabled on your server instance, the save button will display “Save to clipboard”. To save your modifications: 1. Click save: the content is copied to your clipboard. 2. Make sure you are up to date (e.g. git pull --rebase). 3. Remove all the content of the target file. 4. Paste content and save. 5. Commit and push on a branch using git. 6. To share your modifications, create a pull request for that branch through the github UI."
  },
  {
    "objectID": "how_to_contribute.html#step-4-run-pipeline-and-troubleshoot",
    "href": "how_to_contribute.html#step-4-run-pipeline-and-troubleshoot",
    "title": "How to Contribute",
    "section": "Step 4: Run pipeline and troubleshoot",
    "text": "Step 4: Run pipeline and troubleshoot\nThe last step is to load your saved pipeline in BON and a Box and run it. The error log at the bottom of the page will show errors with the pipeline.\nADD PHOTO"
  },
  {
    "objectID": "about.html#why-should-you-contribute",
    "href": "about.html#why-should-you-contribute",
    "title": "About",
    "section": "Why should you contribute?",
    "text": "Why should you contribute?\nContributing to BON in a Box will increase the impact of your research by allowing your analysis workflows to be used in broader areas and contexts for decision-making. Contributors will become part of a global community to generate analyses of the state and trends of biodiversity to inform management decisions."
  }
]