[
  {
    "objectID": "how_to_contribute.html",
    "href": "how_to_contribute.html",
    "title": "How to Contribute",
    "section": "",
    "text": "If you wish to contribute a pipeline, please email us at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation."
  },
  {
    "objectID": "how_to_contribute.html#contributing",
    "href": "how_to_contribute.html#contributing",
    "title": "How to Contribute",
    "section": "",
    "text": "If you wish to contribute a pipeline, please email us at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation."
  },
  {
    "objectID": "how_to_contribute.html#is-your-analysis-pipeline-right-for-us",
    "href": "how_to_contribute.html#is-your-analysis-pipeline-right-for-us",
    "title": "How to Contribute",
    "section": "Is your analysis pipeline right for us?",
    "text": "Is your analysis pipeline right for us?\nWe are looking for pipelines that calculate biodiversity metrics that are relevant to policy and biodiversity reporting, such as Essential Biodiversity Variables (EBVs) and Indicators. These pipelines should be\n\nopen source\nbroadly applicable (not just a specific region)\nusing data that is publicly available"
  },
  {
    "objectID": "how_to_contribute.html#instructions-for-adapting-your-code-to-an-analysis-pipeline",
    "href": "how_to_contribute.html#instructions-for-adapting-your-code-to-an-analysis-pipeline",
    "title": "How to Contribute",
    "section": "Instructions for adapting your code to an analysis pipeline",
    "text": "Instructions for adapting your code to an analysis pipeline\nAnalysis workflows can be adapted to BON in a Box pipelines using a few simple steps. Once BON in a Box is installed on your computer, Pipelines can be edited and run locally by creating files in your local BON-in-a-box folder that was cloned from the GItHub repository. Workflows can be in a variety of languages, including R, Julia, and Python, and not all steps of the pipeline need to be in the same language.\nAnalysis workflows can be converted to BON in a Box pipelines with a few steps:\n\nStep 1: Creating a YAML file\nEach script should be accompanied by a YAML file (.yml extension) that specifies inputs and outputs of that step of the pipeline.\nYAML files should begin with a name, description, and authors of the script. The name should be the same as the script that it describes but with a .yml extension.\nExample:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\nNext, the YAML file should specify the inputs of the script. These inputs will either be outputs from the previous script, which can be connected by lines in the pipeline editor, or specified by the user in the input form.\nHere is an example where the input is a raster file of land cover pulled from a public database and a coordinate reference system that is specified by the user.\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\nSpecifying an example will autofill that input with the example unless changed by the user. Not specifying an example will leave that input blank.\nAfter specifying the inputs, you can specify what you want the script to output.\nHere is an example:\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nThere are several different types of inputs/outputs that can be specified in the YAML file:\n\n“int” is an integer\n“string” is a string value (word or words)\n“text/csv” is a txt or csv file\n“application/geo+json” is a GeoJSON spatial file, if specified in outputs this will plot a map in leaflet\n“options” is a dropdown menu, and the options you want displayed can be written below.\n“image/png” is an image\n\nHere is an example of a complete YAML file with inputs and outputs:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nYAML is a space sensitive format, so make sure all the tab sets are correct in the file.\nIf inputs are outputs from a previous step in the pipeline, make sure to give the inputs and outputs the same name and they will be automatically linked in the pipeline.\n\n\nStep 2: Integrating YAML inputs and outputs into your script\nNow that you have created your YAML file, the inputs and outputs need to be integrated into your scripts.\nFirst, set the environment for the script:\nSys.getenv(\"SCRIPT_LOCATION\")\nAnd then set the ‘input’ environment variables. The ‘input’ environment contains the specified inputs from the BON in a Box platform.\nThe input file ‘input.json’ is automatically generated by executing the ‘Run Script’ command in the Bon in a Box platform.\ninput &lt;- rjson::fromJSON(file=file.path(outputFolder, \"input.json\")) # Load input file\nNext, for any functions that are using these inputs, they need to be specified.\nInputs are specified by input$input_name.\nFor example, to create a function to take the square root of a number that was input by the user in the UI:\nresult &lt;- sqrt(input$number)\nThis means if the user inputs the number 144, this will be run in the function and output 12.\nHere is another example to read in an output from a previous step of the pipeline, which was output in csv format.\ndat &lt;- read.csv(input$csv_file)\nNext, you have to specify the outputs, which can be saved in a variety of formats. The format must be accurately specified in the YAML file.\nHere is an example of an output with a csv file and a plot in png format.\nresult_path &lt;- file.path(outputFolder, \"result.csv\")\nwrite.csv(result, result_path, row.names = F )\n\nresult_plot_path &lt;- file.path(outputFolder, \"result_plot.png\")\nggsave(result_path, result)\n\noutput &lt;- list(result = result_path,\nresult_plot = result_plot_path)\nLastly, write the output as a JSON file:\nsetwd(outputFolder)\njsonlite::write_json(output, \"output.json\", auto_unbox = TRUE, pretty = TRUE)\n\n\nStep 3: Connect your scripts with the BON in a Box pipeline editor.\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from github, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nADD PHOTOS.\n\n\nStep 4: Run pipeline and troubleshoot\nThe last step is to load your saved pipeline in BON and a Box and run it. The error log at the bottom of the page will show errors with the pipeline.\nADD PHOTO"
  },
  {
    "objectID": "how_to_install.html",
    "href": "how_to_install.html",
    "title": "How to Install",
    "section": "",
    "text": "Prerequisites :\n\nGit - A github account, with an SSH key registered. See Adding a new SSH key to your GitHub account.\nAt least 6 GB of free space (this includes the installation of Docker Desktop) - RAM requirements will depend on the scripts that you run.\nWindows:\n\nDocker Desktop - Note that it is not necessary to make an account\n\nA Linux shell (git bash, Bash through PowerShell, cygwin, etc.) necessary to run th .sh scripts in the instructions below.\nMac:\n\nDocker Desktop Note that it is not necessary to make an account\nMake sure docker is added to the path of your terminal. From a terminal, run command docker run hello-world. If there is an error message, see https://stackoverflow.com/a/71923962/3519951.\nIf you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951.\n\nLinux: Docker with Docker Compose installed. It is recommended to add your user to the docker group.\n\nTo run:\n\nClone repository (Windows users: do not clone the repo in a folder under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop.\nProvide the environment variables: - Open the newly cloned repository on your computer - Find the file called runner-sample.env - Duplicate the file and rename the copy to runner.env. - Fill the properties depending on what you intend to run. - Adjust any server option as you see fit.\nUsing a linux terminal (terminal on Mac or Git Bash), navigate to top-level folder.\nIn the linux terminal, type ./server-up.sh\n\nMake sure you have docker open and running on your computer.\nThe first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes.\nNetwork problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure.\nWindows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization. Access to the BIOS may be required to enable virtualization)\n\nType http://localhost/ to open BON in a Box\nRun ./server-down.sh in a terminal to stop the server when done\nOn Windows, to completely stop the processes, you might have to run wsl --shutdown\n\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted: - When modifying an existing script, simply re-run the script from the UI and the new version will be executed. - When adding or renaming scripts, refresh the browser page.\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted: - In the pipeline editor, click save, paste the file to your file in the pipeline folder and run it from the “pipeline run” page. - When adding or renaming pipelines, refresh the browser page."
  },
  {
    "objectID": "how_to_install.html#running-the-servers-locally",
    "href": "how_to_install.html#running-the-servers-locally",
    "title": "How to Install",
    "section": "",
    "text": "Prerequisites :\n\nGit - A github account, with an SSH key registered. See Adding a new SSH key to your GitHub account.\nAt least 6 GB of free space (this includes the installation of Docker Desktop) - RAM requirements will depend on the scripts that you run.\nWindows:\n\nDocker Desktop - Note that it is not necessary to make an account\n\nA Linux shell (git bash, Bash through PowerShell, cygwin, etc.) necessary to run th .sh scripts in the instructions below.\nMac:\n\nDocker Desktop Note that it is not necessary to make an account\nMake sure docker is added to the path of your terminal. From a terminal, run command docker run hello-world. If there is an error message, see https://stackoverflow.com/a/71923962/3519951.\nIf you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951.\n\nLinux: Docker with Docker Compose installed. It is recommended to add your user to the docker group.\n\nTo run:\n\nClone repository (Windows users: do not clone the repo in a folder under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop.\nProvide the environment variables: - Open the newly cloned repository on your computer - Find the file called runner-sample.env - Duplicate the file and rename the copy to runner.env. - Fill the properties depending on what you intend to run. - Adjust any server option as you see fit.\nUsing a linux terminal (terminal on Mac or Git Bash), navigate to top-level folder.\nIn the linux terminal, type ./server-up.sh\n\nMake sure you have docker open and running on your computer.\nThe first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes.\nNetwork problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure.\nWindows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization. Access to the BIOS may be required to enable virtualization)\n\nType http://localhost/ to open BON in a Box\nRun ./server-down.sh in a terminal to stop the server when done\nOn Windows, to completely stop the processes, you might have to run wsl --shutdown\n\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted: - When modifying an existing script, simply re-run the script from the UI and the new version will be executed. - When adding or renaming scripts, refresh the browser page.\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted: - In the pipeline editor, click save, paste the file to your file in the pipeline folder and run it from the “pipeline run” page. - When adding or renaming pipelines, refresh the browser page."
  },
  {
    "objectID": "how_to_install.html#running-the-servers-remotely",
    "href": "how_to_install.html#running-the-servers-remotely",
    "title": "How to Install",
    "section": "Running the servers remotely",
    "text": "Running the servers remotely\nUse the ansible playbook instructions."
  },
  {
    "objectID": "how_to_install.html#running-a-script-or-pipeline",
    "href": "how_to_install.html#running-a-script-or-pipeline",
    "title": "How to Install",
    "section": "Running a script or pipeline",
    "text": "Running a script or pipeline\nYou have an instance of BON in a Box running, either locally or remotely, and you want to run your first script or pipeline.\nThere is one page to run scripts, and one to run pipelines. Select the script or pipelines from the dropdown and fill the form.\nThe form might ask you for a file. In order to provide a file that you own locally, upload or copy it to the userdata folder. You can then refer to it with a url as such: /userdata/myFile.shp, or /userdata/myFolder/myFile.shp if there are subfolders."
  },
  {
    "objectID": "how_to_install.html#scripts",
    "href": "how_to_install.html#scripts",
    "title": "How to Install",
    "section": "Scripts",
    "text": "Scripts\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle:\n\nScript launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (number, string, etc.)\n\nSee empty R script for a minimal script lifecycle example.\n\nDescribing a script\nThe script description is in a .yml file next to the script. It is necessary for the script to be found and connected to other scripts in a pipeline.\nHere is an empty commented sample:\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example\n\nInput and output types\nEach input and output must declare a type, in lowercase. It can be a primitive or a file.\nThe following primitive types are accepted:\n\n\n\n“Type” attribute in the yaml\nIU rendering\n\n\n\n\nboolean\nPlain text\n\n\nfloat, float[]\nPlain text\n\n\nint, int[]\nPlain text\n\n\noptions (INSERT FOOTNOTE)\nPlain text\n\n\ntext, text[]\nPlain text\n\n\n(any unknown type)\nPlain text\n\n\n\nAny MIME type is accepted. Here are a few common ones:\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON\napplication/geo+json\nMap\n\n\nGeoTIFF (INSERT FOOTNOTE)\napplication/geopackage+sqlite3\nLink\n\n\nJPG\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nJPG\nimage/jpg\n tag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\nSearch the web to find the appropriate MIME type for your content. Here are a few references: - http://www.iana.org/assignments/media-types/media-types.xhtml - http://svn.apache.org/viewvc/httpd/httpd/trunk/docs/conf/mime.types?view=markup\nWhen used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\noptions type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\n\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\nScript dependencies\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies.\n\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads and uses inputs from the input.json file. Example in R:\n## Receiving arguments from input.json.\n## outputFolder is already defined by server\nlibrary(\"rjson\")\ninput &lt;- fromJSON(file=file.path(outputFolder, \"input.json\"))\n\n## Can now be accessed from the map\nprint(input$predictors)\nThe script should perform appropriate parameter validation.\nNote that the inputs will be null if the user left the text box empty.\n\n\nGenerating outputs\nThe output files generated by the script must be saved in the run folder. The script must also generate an output.json file in the same folder, that contains a map associating the output ids to their values. Example:\n{\n  \"sdm_pred\": \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_pred.tif\",\n  \"sdm_runs\":[\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_1.tif\",\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_2.tif\"\n  ]\n}"
  },
  {
    "objectID": "how_to_install.html#pipelines",
    "href": "how_to_install.html#pipelines",
    "title": "How to Install",
    "section": "Pipelines",
    "text": "Pipelines\nA pipeline is a collection of steps to acheive the desired processing. Each script becomes a pipeline step. \nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output (rightmost red box in image above). Pipeline IO supports the same types and UI rendering as individual steps, since its inputs are directly fed to the steps, and outputs come from the step outputs.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows you to create pipelines by plugging steps together.\nThe left pane shows the available steps, the main section shows the canvas.\nOn the right side, a collapsible pane allows to edit the labels and descriptions of the pipeline inputs and outputs.\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\n\n\nimage\n\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many step, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and user will have to fill the value.\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\n\n\nimage\n\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons.\nIn the event that saving has been disabled on your server instance, the save button will display “Save to clipboard”. To save your modifications: 1. Click save: the content is copied to your clipboard. 2. Make sure you are up to date (e.g. git pull --rebase). 3. Remove all the content of the target file. 4. Paste content and save. 5. Commit and push on a branch using git. 6. To share your modifications, create a pull request for that branch through the github UI."
  }
]