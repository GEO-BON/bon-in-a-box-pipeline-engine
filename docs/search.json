[
  {
    "objectID": "how_to_contribute.html",
    "href": "how_to_contribute.html",
    "title": "How to Contribute",
    "section": "",
    "text": "If you wish to contribute a pipeline, please email us at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before being accepted."
  },
  {
    "objectID": "how_to_contribute.html#contributing",
    "href": "how_to_contribute.html#contributing",
    "title": "How to Contribute",
    "section": "",
    "text": "If you wish to contribute a pipeline, please email us at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation."
  },
  {
    "objectID": "how_to_contribute.html#is-your-analysis-pipeline-right-for-us",
    "href": "how_to_contribute.html#is-your-analysis-pipeline-right-for-us",
    "title": "How to Contribute",
    "section": "Is your analysis pipeline right for us?",
    "text": "Is your analysis pipeline right for us?\nWe are looking for pipelines that calculate biodiversity metrics that are relevant to policy and biodiversity reporting, such as Essential Biodiversity Variables (EBVs) and Indicators. These pipelines should be\n\nopen source\nbroadly applicable (not just a specific region)\nusing data that is publicly available"
  },
  {
    "objectID": "how_to_contribute.html#instructions-for-adapting-your-code-to-an-analysis-pipeline",
    "href": "how_to_contribute.html#instructions-for-adapting-your-code-to-an-analysis-pipeline",
    "title": "How to Contribute",
    "section": "Instructions for adapting your code to an analysis pipeline",
    "text": "Instructions for adapting your code to an analysis pipeline\nAnalysis workflows can be adapted to BON in a Box pipelines using a few simple steps. Once BON in a Box is installed on your computer, Pipelines can be edited and run locally by creating files in your local BON-in-a-box folder that was cloned from the GItHub repository. Workflows can be in a variety of languages, including R, Julia, and Python, and not all steps of the pipeline need to be in the same language.\nAnalysis workflows can be converted to BON in a Box pipelines with a few steps:\n\nStep 1: Creating a YAML file\nEach script should be accompanied by a YAML file (.yml extension) that specifies inputs and outputs of that step of the pipeline.\nYAML files should begin with a name, description, and authors of the script. The name should be the same as the script that it describes but with a .yml extension.\nExample:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\nNext, the YAML file should specify the inputs of the script. These inputs will either be outputs from the previous script, which can be connected by lines in the pipeline editor, or specified by the user in the input form.\nHere is an example where the input is a raster file of land cover pulled from a public database and a coordinate reference system that is specified by the user.\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\nSpecifying an example will autofill that input with the example unless changed by the user. Not specifying an example will leave that input blank.\nAfter specifying the inputs, you can specify what you want the script to output.\nHere is an example:\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nThere are several different types of user inputs that can be specified in the YAML file:\n\n\n\n\n\n\n\n\n“Type” attribute in the yaml\nIU rendering\nDescription\n\n\n\n\nboolean\nPlain text\ntrue/false\n\n\nfloat, float[]\nPlain text\npositive and negative numbers with a decimal point\n\n\nint, int[]\nPlain text\ninteger\n\n\noptions (INSERT FOOTNOTE)\nPlain text\ndropdown menu\n\n\ntext, text[]\nPlain text\ntext input (words)\n\n\n(any unknown type)\nPlain text\n\n\n\n\nThere are also several types of outputs that can be created\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON\napplication/geo+json\nMap\n\n\nGeoPackage\napplication/geopackage+sqlite3\nLink\n\n\nGeoTIFF\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nJPG\nimage/jpg\ntag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\nWhen used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\noptions type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\nHere is an example of a complete YAML file with inputs and outputs:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of \n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\noutputs: \n  result: \n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv \n  result_plot: \n    label: Result plot \n    description: Plot of results\n    type: image/png\nYAML is a space sensitive format, so make sure all the tab sets are correct in the file.\nIf inputs are outputs from a previous step in the pipeline, make sure to give the inputs and outputs the same name and they will be automatically linked in the pipeline.\nHere is an empty commented sample that can be filled in\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example\n\n\nStep 2: Integrating YAML inputs and outputs into your script\nNow that you have created your YAML file, the inputs and outputs need to be integrated into your scripts.\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle:\n\nScript launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (number, string, etc.)\n\nSee empty R script for a minimal script lifecycle example.\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads the input.json file that is in the run folder. This JSON file is automatically created from the BON in a Box pipeline. Here is an example of how to read the input folder in the R script:\nFirst, set the environment for the script:\nSys.getenv(\"SCRIPT_LOCATION\")\nAnd then set the ‘input’ environment variables. The ‘input’ environment contains the specified inputs from the BON in a Box platform.\ninput &lt;- rjson::fromJSON(file=file.path(outputFolder, \"input.json\")) # Load input file\nNext, for any functions that are using these inputs, they need to be specified.\nInputs are specified by input$input_name.\nFor example, to create a function to take the square root of a number that was input by the user in the UI:\nresult &lt;- sqrt(input$number)\nThis means if the user inputs the number 144, this will be run in the function and output 12.\nHere is another example to read in an output from a previous step of the pipeline, which was output in csv format.\ndat &lt;- read.csv(input$csv_file)\nNext, you have to specify the outputs, which can be saved in a variety of formats. The format must be accurately specified in the YAML file.\nHere is an example of an output with a csv file and a plot in png format.\nresult_path &lt;- file.path(outputFolder, \"result.csv\")\nwrite.csv(result, result_path, row.names = F )\n\nresult_plot_path &lt;- file.path(outputFolder, \"result_plot.png\")\nggsave(result_path, result)\n\noutput &lt;- list(result = result_path,\nresult_plot = result_plot_path)\nLastly, write the output as a JSON file:\nsetwd(outputFolder)\njsonlite::write_json(output, \"output.json\", auto_unbox = TRUE, pretty = TRUE)\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\nScript dependencies\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies.\n\n\n\nStep 3: Connect your scripts with the BON in a Box pipeline editor.\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from github, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nADD PHOTOS.\n\n\nStep 4: Run pipeline and troubleshoot\nThe last step is to load your saved pipeline in BON and a Box and run it. The error log at the bottom of the page will show errors with the pipeline.\nADD PHOTO"
  },
  {
    "objectID": "how_to_install.html",
    "href": "how_to_install.html",
    "title": "How to Install",
    "section": "",
    "text": "The user interface allows to run and edit pipelines. It is used through the browser, but doesn’t need to be installed on a web server. This can all happen locally. The code for the scripts, however, is not edited through the UI but rather with a standard text editor such as VS Code or RStudio."
  },
  {
    "objectID": "how_to_install.html#running-the-servers-locally",
    "href": "how_to_install.html#running-the-servers-locally",
    "title": "How to Install",
    "section": "Running BON in a Box locally",
    "text": "Running BON in a Box locally\nBON in a Box can be installed and ran on a local computer. While personnal computers have less computing power than servers, it is more convenient for pipeline development, or when working with small examples. Users adding scripts and creating pipelines on their local computer can directly run them through BON in a Box.\nPrerequisites :\n\nGit - A github account, with an SSH key registered. See Adding a new SSH key to your GitHub account.\nAt least 6 GB of free space (this includes the installation of Docker Desktop) - RAM requirements will depend on the scripts that you run.\nWindows:\n\nDocker Desktop - Note that it is not necessary to make an account\nA Linux shell (git bash, Bash through PowerShell, cygwin, etc.) necessary to run th .sh scripts in the instructions below.\n\nMac:\n\nDocker Desktop Note that it is not necessary to make an account\nMake sure docker is added to the path of your terminal. From a terminal, run command docker run hello-world. If there is an error message, see https://stackoverflow.com/a/71923962/3519951.\nIf you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951.\n\nLinux: Docker with Docker Compose installed. It is recommended to add your user to the docker group.\n\nTo run:\n\nClone repository (Windows users: do not clone the repo in a folder under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop.\nProvide the environment variables: - Open the newly cloned repository on your computer\n\nFind the file called runner-sample.env\nDuplicate the file and rename the copy to runner.env.\nFill the properties depending on what you intend to run. Include any API keys that you need to access data (e.g. GBIF or IUCN)\nAdjust any server option as you see fit.\n\nUsing a linux terminal (terminal on Mac or Git Bash), navigate to the folder of the cloned repository.\nIn the linux terminal, type ./server-up.sh\n\nMake sure the server is started from a linux terminal that is in the same folder as the cloned repository so it can access the files.\nMake sure you have docker open and running on your computer.\nThe first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes.\nNetwork problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure.\nWindows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization. Access to the BIOS may be required to enable virtualization)\n\nType http://localhost/ to open BON in a Box\nRun ./server-down.sh in a terminal to stop the server when done\nOn Windows, to completely stop the processes, you might have to run wsl --shutdown\n\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted: - When modifying an existing script, simply re-run the script from the UI and the new version will be executed. - When adding or renaming scripts, refresh the browser page.\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted: - In the pipeline editor, click save, paste the file to your file in the pipeline folder and run it from the “pipeline run” page. - When adding or renaming pipelines, refresh the browser page."
  },
  {
    "objectID": "how_to_install.html#running-the-servers-remotely",
    "href": "how_to_install.html#running-the-servers-remotely",
    "title": "How to Install",
    "section": "Running BON in a Box on a server",
    "text": "Running BON in a Box on a server\nTo share an instance or to have larger data processing capabilities, a server installation is recommended.\nRefer to the ansible playbook for instructions."
  },
  {
    "objectID": "how_to_install.html#running-a-script-or-pipeline",
    "href": "how_to_install.html#running-a-script-or-pipeline",
    "title": "How to Install",
    "section": "Running a script or pipeline",
    "text": "Running a script or pipeline\nYou now have an instance of BON in a Box running, either locally or remotely, and you want to run your first script or pipeline.\nThere is one page to run scripts, and one to run pipelines. Select the script or pipelines from the dropdown and fill the form.\nThe form might ask you for a file. In order to provide a file that you own locally, upload or copy it to the userdata folder. You can then refer to it with a url as such: /userdata/myFile.shp, or /userdata/myFolder/myFile.shp if there are subfolders."
  },
  {
    "objectID": "how_to_install.html#scripts",
    "href": "how_to_install.html#scripts",
    "title": "How to Install",
    "section": "Scripts",
    "text": "Scripts\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle:\n\nScript launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (number, string, etc.)\n\nSee empty R script for a minimal script lifecycle example.\n\nDescribing a script\nThe script description is in a .yml file next to the script. It is necessary for the script to be found and connected to other scripts in a pipeline.\nHere is an empty commented sample:\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example\n\nInput and output types\nEach input and output must declare a type, in lowercase. It can be a primitive or a file.\nThe following primitive types are accepted:\n\n\n\n“Type” attribute in the yaml\nIU rendering\n\n\n\n\nboolean\nPlain text\n\n\nfloat, float[]\nPlain text\n\n\nint, int[]\nPlain text\n\n\noptions (INSERT FOOTNOTE)\nPlain text\n\n\ntext, text[]\nPlain text\n\n\n(any unknown type)\nPlain text\n\n\n\nAny MIME type is accepted. Here are a few common ones:\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON\napplication/geo+json\nMap\n\n\nGeoTIFF (INSERT FOOTNOTE)\napplication/geopackage+sqlite3\nLink\n\n\nJPG\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nJPG\nimage/jpg\n tag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\nSearch the web to find the appropriate MIME type for your content. Here are a few references: - http://www.iana.org/assignments/media-types/media-types.xhtml - http://svn.apache.org/viewvc/httpd/httpd/trunk/docs/conf/mime.types?view=markup\nWhen used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\noptions type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\n\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\nScript dependencies\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies.\n\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads and uses inputs from the input.json file. Example in R:\n## Receiving arguments from input.json.\n## outputFolder is already defined by server\nlibrary(\"rjson\")\ninput &lt;- fromJSON(file=file.path(outputFolder, \"input.json\"))\n\n## Can now be accessed from the map\nprint(input$predictors)\nThe script should perform appropriate parameter validation.\nNote that the inputs will be null if the user left the text box empty.\n\n\nGenerating outputs\nThe output files generated by the script must be saved in the run folder. The script must also generate an output.json file in the same folder, that contains a map associating the output ids to their values. Example:\n{\n  \"sdm_pred\": \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_pred.tif\",\n  \"sdm_runs\":[\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_1.tif\",\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_2.tif\"\n  ]\n}"
  },
  {
    "objectID": "how_to_install.html#pipelines",
    "href": "how_to_install.html#pipelines",
    "title": "How to Install",
    "section": "Pipelines",
    "text": "Pipelines\nA pipeline is a collection of steps to acheive the desired processing. Each script becomes a pipeline step. \nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output (rightmost red box in image above). Pipeline IO supports the same types and UI rendering as individual steps, since its inputs are directly fed to the steps, and outputs come from the step outputs.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows you to create pipelines by plugging steps together.\nThe left pane shows the available steps, the main section shows the canvas.\nOn the right side, a collapsible pane allows to edit the labels and descriptions of the pipeline inputs and outputs.\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\n\n\nimage\n\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many step, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and user will have to fill the value.\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\n\n\nimage\n\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons.\nIn the event that saving has been disabled on your server instance, the save button will display “Save to clipboard”. To save your modifications: 1. Click save: the content is copied to your clipboard. 2. Make sure you are up to date (e.g. git pull --rebase). 3. Remove all the content of the target file. 4. Paste content and save. 5. Commit and push on a branch using git. 6. To share your modifications, create a pull request for that branch through the github UI."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What is BON in a Box’s pipeline engine?\nBON in a Box serves as a hub for Biodiversity Observation Networks (BONs), consolidating diverse tools and resources to enhance data analysis and collaboration, as well as a project catalogue. For more information about BON in a Box in general, please refer to BON in a Box’s website\nWe present here the BON in a Box pipeline engine as the ultimate ecosystem for harmonizing biodiversity observations, fostering efficiency, and amplifying the power of data-driven insights.\nIt is a community-contributed and open-source data to biodiversity indicator or data to Essential Biodiversity Variable (EBV) workflow repository curated by GEO BON. Its engine allows to seamlessly connect steps in different languages (R, Julia, Python) through a modular pipeline approach that reads both from user-provided and publicly available data sources. These pipelines combine local and global data sources in the EBVs and biodiversity indicators calculations.\n\n\nWhy do we need this platform?\nScientists around the world are developing tools to create workflows to calculate essential biodiversity variables and indicators to report on biodiversity change. However, there are almost no platforms to share these workflows and tools, leading to duplication of effort. BON in a Box allows networks to share and enhance each other’s work.\nAdditionally, organisations often lack the technical capabilities to calculate these indicators to monitor biodiversity, which impairs decision-making. BON in a Box can bridge the gap between those knowing how to calculate, and those needing the indicators for monitoring and conservation.\n\n\nWhat is a pipeline?\nA pipeline is a collection of steps needed to transform input data into valuable output for the stakeholder in an automated way. It can be an EBV, an indicator, or anything useful to analysis and decision-making in the context of biodiversity conservation and natural resource management.\nThe smaller steps composing a pipeline are scripts. These scripts also convert input data into valuable outputs, just like pipelines, but at a finer scale: cleaning, augmenting, standardizing, or performing statistical analyses. Importantly, these scripts can be reused or repurposed across varied contexts. For instance, data cleaning scripts could be applied to multiple pipelines that utilize similar types of data, enhancing efficiency and consistency in data processing workflows. Since the outputs are in-between every step, the pipeline can make abstraction of the programming language, allowing a pipeline to mix R, python and Julia scripts without any interoperability issues.\nIf a pipeline builds upon the results of another pipeline, this “inner pipeline” it can be included as a step in the “outer pipeline”; we call this a subpipeline. An SDM pipeline could serve to generate range maps inside a Species Habitat Index pipeline, or embedded in a Species Richness pipeline. We can thus define a step as “a script or a subpipeline performing an operation in the context of a pipeline”.\n\n\n\nSimplified pipeline with steps (blue), intermediate results (grey), and final results (green)\n\n\n\n\nWhy should you contribute?\nContributing to BON in a Box will increase the impact of your research by allowing your analysis workflows to be used in broader areas and contexts for decision-making. Contributors will become part of a global community to generate analyses of the state and trends of biodiversity to inform management decisions."
  },
  {
    "objectID": "about.html#what-is-bon-in-a-box",
    "href": "about.html#what-is-bon-in-a-box",
    "title": "About",
    "section": "",
    "text": "BON in a Box serves as a hub for Biodiversity Observation Networks (BONs), consolidating diverse tools and resources to enhance data analysis and collaboration. It’s the ultimate ecosystem for harmonizing biodiversity observations, fostering efficiency, and amplifying the power of data-driven insights.\nBON in a Box now features a community-contributed and open-source data to essential biodiversity variable (EBV) and indicator workflow repository curated by GEO BON. Its engine allows to seamlessly connect steps in different languages (R, Julia, Python) through a modular pipeline approach that reads both from local and global data sources. These pipelines can calculate EBVs and indicators using publicaly available data or user-provided data."
  },
  {
    "objectID": "about.html#why-do-we-need-this-platform",
    "href": "about.html#why-do-we-need-this-platform",
    "title": "About",
    "section": "Why do we need this platform?",
    "text": "Why do we need this platform?\nScientists around the world are developing tools to create workflows to calculate essential biodiversity variables and indicators to report on biodiversity change. However, there are almost no platforms to share these workflows and tools, leading to duplication of effort. BON in a Box is a platform to consolidate these tools and facilitate coordination and networking between scientists, organizations, and governments to face the global biodiversity crisis."
  },
  {
    "objectID": "about.html#what-are-the-project-benefits",
    "href": "about.html#what-are-the-project-benefits",
    "title": "About",
    "section": "What are the project benefits?",
    "text": "What are the project benefits?\na. Academics/researchers/users (BONs) • Improve science further, through open and repeatable processes increasing research impact.\n• Complement your analysis processes with routines developed by other researchers and to be part of a global community that generates analysis of the state and trends of biodiversity.\nb. Decision Makers\nWith BON in a Box you will be able to:\n• Obtain insights and results from your own data as needed to report to multilateral agreements and other public policy commitments with validated tools and methodologies.\n• Optimize resources and efforts for quick and informed environmental decision-making"
  },
  {
    "objectID": "working_with_stac.html",
    "href": "working_with_stac.html",
    "title": "Working with STAC",
    "section": "",
    "text": "STAC stands for SpatioTemporal Asset Catalog. It is a common language to describe geospatial information, so it can be more easily worked with, indexed, and discovered. Spatial information is stored in a geoJSON format, which can be easily adapted to different domains. The goal of STAC is to enable a global index of all imagery derived data products with an easily implementable standard for organization to expose their data in a persistent and reliable way. STAC objects are hosted in geoJSON format as HTML pages."
  },
  {
    "objectID": "working_with_stac.html#what-is-a-stac-catalog",
    "href": "working_with_stac.html#what-is-a-stac-catalog",
    "title": "Working with STAC",
    "section": "",
    "text": "STAC stands for SpatioTemporal Asset Catalog. It is a common language to describe geospatial information, so it can be more easily worked with, indexed, and discovered. Spatial information is stored in a geoJSON format, which can be easily adapted to different domains. The goal of STAC is to enable a global index of all imagery derived data products with an easily implementable standard for organization to expose their data in a persistent and reliable way. STAC objects are hosted in geoJSON format as HTML pages."
  },
  {
    "objectID": "working_with_stac.html#geo-bon-stac-catalog",
    "href": "working_with_stac.html#geo-bon-stac-catalog",
    "title": "Working with STAC",
    "section": "GEO BON STAC Catalog",
    "text": "GEO BON STAC Catalog\nPipelines can be created using layers pulled from any STAC catalog, such as the Planetary Computer. GEO BON hosts a STAC catalog that contains many commonly used data layers, which can be explored in JSON format or in a more user-friendly viewer.\nThere is a step to pull and filter data from a STAC catalog in BON in a Box called ‘Load from STAC’, where users can specify the layers, bounding box, projection system, and spatial resolution they are interested in. It outputs a geoJSON file that becomes an input into a subsequent pipeline step."
  },
  {
    "objectID": "working_with_stac.html#working-with-the-stac-catalog-in-r",
    "href": "working_with_stac.html#working-with-the-stac-catalog-in-r",
    "title": "Working with STAC",
    "section": "Working with the STAC Catalog in R",
    "text": "Working with the STAC Catalog in R\nData in STAC catalogs can be loaded directly into R and visualized using the rstac package.\nTo start, install and load the following packages.\n\nlibrary(gdalcubes)\nlibrary(rstac)\nlibrary(knitr)\nlibrary(stars)\n\nLoading required package: abind\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nNext, connect to the STAC catalog using the stac function in the rstac package.\n\nstac_obj &lt;- stac(\"https://stac.geobon.org/\")\n\nIn the STAC catalog, layers are organized into collections. To make a list of the collections:\n\ncollections &lt;- stac_obj |&gt; collections() |&gt; get_request()\n\nAnd to view the collections and their descriptions in a table.\n\ndf&lt;-data.frame(id=character(),title=character(),description=character())\nfor (c in collections[['collections']]){\n  df&lt;-rbind(df,data.frame(id=c$id,title=c$title,description=c$description))\n}\nkable(head(df))\n\n\n\n\n\n\n\n\n\nid\ntitle\ndescription\n\n\n\n\nchelsa-clim\nCHELSA Climatologies\nCHELSA Climatologies\n\n\ngfw-lossyear\nGlobal Forest Watch - Loss year\nGlobal Forest Watch - Loss year\n\n\nsoilgrids\nSoil Grids datasets\nSoilGrids aggregated datasets at 1km resolution\n\n\nghmts\nGlobal Human Modification of Terrestrial Systems\nThe Global Human Modification of Terrestrial Systems data set provides a cumulative measure of the human modification of terrestrial lands across the globe at a 1-km resolution. It is a continuous 0-1 metric that reflects the proportion of a landscape modified, based on modeling the physical extents of 13 anthropogenic stressors and their estimated impacts using spatially-explicit global data sets with a median year of 2016.\n\n\ngfw-treecover2000\nGlobal Forest Watch - Tree cover 2000\nGlobal Forest Watch - Tree cover 2000\n\n\ngfw-gain\nGlobal Forest Watch - Gain\nGlobal Forest Watch - Gain\n\n\n\n\n\nTo search for a specific collection. Here we will search for the land cover collection from EarthEnv.\n\nit_obj &lt;- stac_obj |&gt;\n  stac_search(collections = \"earthenv_landcover\") |&gt;\n  post_request() |&gt; items_fetch()\nit_obj\n\n###Items\n- features (12 item(s)):\n  - class_9\n  - class_8\n  - class_7\n  - class_6\n  - class_5\n  - class_4\n  - class_3\n  - class_2\n  - class_12\n  - class_11\n  - class_10\n  - class_1\n- assets: data\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nView the available layers in this collection.\n\nit_obj &lt;- stac_obj |&gt;\n  collections(\"earthenv_landcover\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nit_obj\n\n###Items\n- features (12 item(s)):\n  - class_9\n  - class_8\n  - class_7\n  - class_6\n  - class_5\n  - class_4\n  - class_3\n  - class_2\n  - class_12\n  - class_11\n  - class_10\n  - class_1\n- assets: data\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\n\nView the properties of the first item (layer).\n\nit_obj[['features']][[1]]$properties\n\n$class\n[1] \"9\"\n\n$datetime\n[1] \"2000-01-01T00:00:00Z\"\n\n$`proj:epsg`\n[1] 4326\n\n$description\n[1] \"Urban/Built-up\"\n\n$full_filename\n[1] \"consensus_full_class_9.tif\"\n\n\nWe can summarize each layer in a table\n\ndf&lt;-data.frame(id=character(),datetime=character(), description=character())\nfor (f in it_obj[['features']]){\n  df&lt;-rbind(df,data.frame(id=f$id,datetime=f$properties$datetime,description=f$properties$description))\n}\nkable(head(df))\n\n\n\n\nid\ndatetime\ndescription\n\n\n\n\nclass_9\n2000-01-01T00:00:00Z\nUrban/Built-up\n\n\nclass_8\n2000-01-01T00:00:00Z\nRegularly Flooded Vegetation\n\n\nclass_7\n2000-01-01T00:00:00Z\nCultivated and Managed Vegetation\n\n\nclass_6\n2000-01-01T00:00:00Z\nHerbaceous Vegetation\n\n\nclass_5\n2000-01-01T00:00:00Z\nShrubs\n\n\nclass_4\n2000-01-01T00:00:00Z\nMixed/Other Trees\n\n\n\n\n\nNow, you can load one of the items with the stars package, which allows access to files remotely in a fast and efficient way by only loading the areas that you need.\nHere we will select the layer representing the percentage of “Evergreen/Deciduous Needleleaf Trees”, which is the 12th layer in the collection.\n\nlc1&lt;-read_stars(paste0('/vsicurl/',it_obj[['features']][[12]]$assets$data$href), proxy = TRUE)\nplot(lc1)\n\ndownsample set to 23\n\n\n\n\n\n\n\n\n\nYou can crop the raster by a bounding box and visualize it.\n\nbbox&lt;-st_bbox(c(xmin = -76, xmax = -70, ymax = 54, ymin = 50), crs = st_crs(4326))\nlc2 &lt;- lc1 |&gt; st_crop(bbox)\n\npal &lt;- colorRampPalette(c(\"black\",\"darkblue\",\"red\",\"yellow\",\"white\"))\nplot(lc2,breaks=seq(0,100,10),col=pal(10))\n\n\n\n\n\n\n\n\nYou can now save this object to your computer in Cloud Optimized GeoTiff (COG) format. COG works on the cloud and allows the user to access just the part of the file that is needed because all relevant information is stored together in each tile.\n# Replace \" ~ \" for the directory of your choice on your computer.\nwrite_stars(lc2,'~/lc2.tif',driver='COG',options=c('COMPRESS=DEFLATE'))\nNote that for a layer with categorical variables, saving is more complex.\nlc1 |&gt; st_crop(bbox) |&gt; write_stars('~/lc1.tif', driver='COG', RasterIO=c('resampling'='mode'),\noptions=c('COMPRESS=DEFLATE', 'OVERVIEW_RESAMPLING=MODE', 'LEVEL=6',\n'OVERVIEW_COUNT=8', 'RESAMPLING=MODE', 'WARP_RESAMPLING=MODE', 'OVERVIEWS=IGNORE_EXISTING'))"
  },
  {
    "objectID": "working_with_stac.html#working-with-gdal-cubes",
    "href": "working_with_stac.html#working-with-gdal-cubes",
    "title": "Working with STAC",
    "section": "Working with GDAL Cubes",
    "text": "Working with GDAL Cubes\nThe R package gdalcubes allows for the processing of four dimensional regular raster data cubes from irregular image collections, hiding complexities in the data such as different map projections and spatial overlaps of images or different spatial resolutions of spectral bands. This can be useful for working with many layers or time series of satellite imagery.\nThe following step is necessary for GDAL Cubes to work with STAC catalog data.\n\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\n\nYou can filter based on item properties and create a collection.\n\nst &lt;- stac_image_collection(it_obj$features, asset_names=c('data'),\n             property_filter = function(f){f[['class']] %in% c('1','2','3','4')},srs='EPSG:4326')\n\nNext, you can build a GDAL cube to process or visualize the data. This cube can be in a different CRS and resolution from those of the original elements/files. However, the time dimension must capture the temporal framework of the item. dt is expressed as a time period, P1M is a period of one month, P1Y is a period of one year. Resampling methods should be tailored to the data type. For categorical data, use “mode” or “nearest”. For continuous data, use “bilinear”. Aggregation is relevant only when multiple rasters overlap.\nHere is an example that sums the four forest categories using aggregation=“sum”, with a change of the reference system to use Quebec Lambert (EPSG:32198) and a resolution of 1 km.\n\nbbox&lt;-st_bbox(c(xmin = -483695, xmax = -84643, ymin = 112704 , ymax = 684311), crs = st_crs(32198))\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2000-01-01\", t1 = \"2000-01-01\", left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin),\ndx=1000, dy=1000, dt=\"P1D\", aggregation = \"sum\", resampling = \"mean\")\n\nCreate a raster cube\n\nlc_cube &lt;- raster_cube(st, v)\n\nSave the resulting file to your computer\n# Replace \" ~ \" for the directory of your choice on your computer.\nlc_cube |&gt; write_tif('~/',prefix='lc2',creation_options=list('COMPRESS'='DEFLATE'))\nPlot the raster cube\n\nlc_cube |&gt; plot(zlim=c(0,100),col=pal(10))\n\n\n\n\n\n\n\n\nUse the accessibility from cities dataset, keeping the same CRS and extent\n\nit_obj &lt;- stac_obj |&gt;\n  collections(\"accessibility_to_cities\") |&gt; items() |&gt;\n  get_request() |&gt; items_fetch()\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2015-01-01\", t1 = \"2015-01-01\",\n                                                left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin),\n                                                dx=1000, dy=1000, dt=\"P1D\", aggregation\n\n = \"mean\", resampling = \"bilinear\")\nfor (i in 1:length(it_obj$features)){\n  it_obj$features[[i]]$assets$data$roles='data'\n}\nst &lt;- stac_image_collection(it_obj$features)\nlc_cube &lt;- raster_cube(st, v)\nlc_cube |&gt; plot(col=heat.colors)\n\n\n\n\n\n\n\n\nUse the CHELSA dataset on climatologies and create an average map for the months of June, July, and August 2010 to 2019\nit_obj &lt;- s_obj |&gt;\n  stac_search(collections = \"chelsa-monthly\", datetime=\"2010-06-01T00:00:00Z/2019-08-01T00:00:00Z\") |&gt;\n  get_request() |&gt; items_fetch()\n\nv &lt;- cube_view(srs = \"EPSG:32198\", extent = list(t0 = \"2010-06-01\", t1 = \"2019-08-31\", left = bbox$xmin, right =bbox$xmax, top = bbox$ymax, bottom = bbox$ymin), dx=1000, dy=1000, dt=\"P10Y\", aggregation = \"mean\",resampling = \"bilinear\")\n\nfor (i in 1:length(it_obj$features)){\n  names(it_obj$features[[i]]$assets)='data'\n  it_obj$features[[i]]$assets$data$roles='data'\n}\n\nanames=unlist(lapply(it_obj$features,function(f){f['id']}))\n\nst &lt;- stac_image_collection(it_obj$features, asset_names = 'data', property_filter = function(f){f[['variable']] == 'tas' & (f[['month']] %in% c(6,7,8)) })\n\nc_cube &lt;- raster_cube(st, v)\n\nc_cube |&gt; plot(col=heat.colors)\n\nc_cube |&gt; write_tif('~/',prefix='chelsa-monthly',creation_options=list('COMPRESS'='DEFLATE'))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "BON in a Box serves as a hub for Biodiversity Observation Networks (BONs), consolidating diverse tools and resources to enhance data analysis and collaboration, as well as a project catalogue. For more information about BON in a Box in general, please refer to BON in a Box’s website\n\nWhat is BON in a Box’s pipeline engine?\nBON in a Box now provides an open-source, fully transparent platform that allows users to contribute data and analysis pipelines to calculate indicators which can then be used for reporting to assess progress towards biodiversity targets. BON in Box can integrate diverse data and expert knowledge, from Earth observation to genetic diversity metrics to calculate monitoring framework indicators.\nWe present here the BON in a Box pipeline engine as the ultimate ecosystem for harmonizing biodiversity observations, fostering efficiency, and amplifying the power of data-driven insights.\nIt is a community-contributed and open-source data to biodiversity indicator or data to Essential Biodiversity Variable (EBV) workflow repository curated by GEO BON. Its engine allows to seamlessly connect steps in different languages (R, Julia, Python) through a modular pipeline approach that reads both from user-provided and publicly available data sources. These pipelines combine local and global data sources in the EBVs and biodiversity indicators calculations.\n\n\nWhy do we need this platform?\nScientists around the world are developing tools to create workflows to calculate essential biodiversity variables and indicators to report on biodiversity change. However, there are almost no platforms to share these workflows and tools, leading to duplication of effort. BON in a Box allows networks to share and enhance each other’s work.\nAdditionally, organisations often lack the technical capabilities to calculate these indicators to monitor biodiversity, which impairs decision-making. BON in a Box can bridge the gap between those knowing how to calculate, and those needing the indicators for monitoring and conservation.\n\n\nWhat is a pipeline?\nA pipeline is a collection of steps needed to transform input data into valuable output for the stakeholder in an automated way. It can be an EBV, an indicator, or anything useful to analysis and decision-making in the context of biodiversity conservation and natural resource management.\nThe smaller steps composing a pipeline are scripts. These scripts also convert input data into valuable outputs, just like pipelines, but at a finer scale: cleaning, augmenting, standardizing, or performing statistical analyses. Importantly, these scripts can be reused or repurposed across varied contexts. For instance, data cleaning scripts could be applied to multiple pipelines that utilize similar types of data, enhancing efficiency and consistency in data processing workflows. Since the outputs are in-between every step, the pipeline can make abstraction of the programming language, allowing a pipeline to mix R, python and Julia scripts without any interoperability issues.\nIf a pipeline builds upon the results of another pipeline, this “inner pipeline” it can be included as a step in the “outer pipeline”; we call this a subpipeline. An SDM pipeline could serve to generate range maps inside a Species Habitat Index pipeline, or embedded in a Species Richness pipeline. We can thus define a step as “a script or a subpipeline performing an operation in the context of a pipeline”.\n\n\n\nSimplified pipeline with steps (blue), intermediate results (grey), and final results (green)\n\n\n\n\nWhy should you contribute?\nContributing to BON in a Box will increase the impact of your research by allowing your analysis workflows to be used in broader areas and contexts for decision-making. Contributors will become part of a global community to generate analyses of the state and trends of biodiversity to inform management decisions."
  },
  {
    "objectID": "how_to_contribute.html#step-1-creating-a-yaml-file",
    "href": "how_to_contribute.html#step-1-creating-a-yaml-file",
    "title": "How to Contribute",
    "section": "Step 1: Creating a YAML file",
    "text": "Step 1: Creating a YAML file\nEach script should be accompanied by a YAML file (.yml extension) that specifies inputs and outputs of that step of the pipeline.\n\nName and description\nYAML files should begin with a name, description, and authors of the script. The name should be the same as the script that it describes but with a .yml extension.\nExample:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\n\n\nInputs\nNext, the YAML file should specify the inputs of the script. These inputs will either be outputs from the previous script, which can be connected by lines in the pipeline editor, or specified by the user in the input form.\nHere is an example where the input is a raster file of land cover pulled from a public database and a coordinate reference system that is specified by the user:\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of\n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\nSpecifying an example will autofill that input with the example unless changed by the user. Not specifying an example will leave that input blank (null).\nThere are several different types of user inputs that can be specified in the YAML file:\n\n\n\n\n\n\n\n\n“Type” attribute in the yaml\nIU rendering\nDescription\n\n\n\n\nboolean\nPlain text\ntrue/false\n\n\nfloat, float[]\nPlain text\npositive and negative numbers with a decimal point\n\n\nint, int[]\nPlain text\ninteger\n\n\noptions1\nPlain text\ndropdown menu\n\n\ntext, text[]\nPlain text\ntext input (words)\n\n\n(any unknown type)\nPlain text\n\n\n\n\n1options type requires an additioal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n  - first option\n  - second option\n  - third option\n  example: third option\n\n\nOutputs\nAfter specifying the inputs, you can specify what you want the script to output.\nHere is an example:\noutputs:\n  result:\n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv\n  result_plot:\n    label: Result plot\n    description: Plot of results\n    type: image/png\nThere are also several types of outputs that can be created\n\n\n\n\n\n\n\n\nFile Type\nMIME type to use in the yaml\nUI rendering\n\n\n\n\nCSV\ntext/csv\nHTML table (partial content)\n\n\nGeoJSON\napplication/geo+json\nMap\n\n\nGeoPackage\napplication/geopackage+sqlite3\nLink\n\n\nGeoTIFF2\nimage/tiff;application=geotiff\nMap widget (leaflet)\n\n\nJPG\nimage/jpg\ntag\n\n\nShapefile\napplication/dbf\nLink\n\n\nText\ntext/plain\nPlain text\n\n\nTSV\ntext/tab-separated-values\nHTML table (partial content)\n\n\n\n(any unknown type)\nPlain text or link\n\n\n\n2When used as an output, image/tiff;application=geotiff type allows an additional range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows something\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\n\n\nSpecifying R package dependencies with conda\n“Conda provides package, dependency, and environment management for any language.” If a script is dependent on a specific package version, conda will load that package version to prevent breaking changes when packages are updated. BON in a Box uses a central conda environment that includes commonly used libraries, and supports dedicated environments for the scripts that need it.\nThis means that instead of loading packages in R scripts, they are loaded in the R environment in conda.\n\nCheck if the libraries you need are in conda\n\nTo check if the packages you need are already in the R environment in conda, go to runners/r-environment.yml in the BON in a Box project folder. The list of packages already installed will be under “dependencies”.\n\n\n\nInstall any libraries that are not already in the environment\n\nIf your package is not already in the R environment, you will need to add it.\nFirst, search for the package name on anaconda. Packages that are in anaconda will be recognized and easily loaded by conda. If it is there:\n\nAdd it to the runners/r-environment.yml file\nAdd it to your local environment, replacing &lt;package name&gt; in: docker exec -it biab-runner-conda mamba install -n rbase &lt;package name&gt; in the terminal. Make sure the program is in front of the package name (e.g. r-tidyverse)\n\nIf it is not found in anaconda\n\nGo to the runners/conda-dockerfile and add it to the install command. Replace &lt;package&gt; with your package name.\nRUN bash --login -c \"mamba activate rbase; R -e 'install.packages(c(\\&lt;package 1&gt;\\\", \\\n        \\\"&lt;package 2&gt;\\\", \\\n        \\\"&lt;package 3&gt;\\\"\\\n    ), repos=\\\"https://cloud.r-project.org/\\\")'\"\nOr for packages in development loaded from github.\nRUN bash --login -c \"mamba activate rbase; R -e 'devtools::install_github(\\\"&lt;user&gt;/&lt;package&gt;\\\")'\"\nInstall in in your local environment by typing this in the terminal, replacing &lt;packages&gt; (three commands should be pasted separately):\ndocker exec -it biab-runner-conda bash\n\nmamba activate rbase\n\nR -e 'install.packages(c(&lt;packages&gt;), repose=\"https://cloud.r-project.org/\")\nYou can use devtools::install_github() in a similar fashion:\ndocker exec -it biab-runner-conda bash\n\nmamba activate rbase\n\nR -e 'devtools::install_github(\"&lt;user&gt;/&lt;package&gt;\")'\n\n\n\n\nAdd any version dependencies to your yml file\n\nIf your script does not require a specific version, you do not need to include a conda section to the yml file. The environment will be the default R environment.\nIf you need a specific version of an already installed library, such as when a breaking change occurs and the script cannot be adapted to use the new version, you can use conda to specify a specific package version in your yml file.\n\nIf you include a conda section, make sure you list all your dependencies. This is creating a new separate environment, not a supplement to the base environment. This means that all of the packages that you need need to be specified in here, even if they are in the r-environment.yml file.\n\n\nconda: # Optional: if you script requires specific versions of packages.\n  channels:\n    - r\n  dependencies:\n    - r-rjson # read inputs and save outputs for R scripts\n    - package=version\nFor more information, refer to the conda documentation.\n\n\n\nExamples\nHere is an example of a complete YAML file with inputs and outputs:\nscript: script_name.R\nname: Script Name\ndescription: Describe what the script does\nauthor:\n  - name: John Doe (john.doe@gmail.com)\n  - name: Jane Doe (jane.doe@gmail.com)\ninputs:\n  landcover_raster:\n    label: Raster of landcover\n    description: Landcover data pulled from EarthEnv database with 1x1km resolution\n    type: application/geo+json\n  crs:\n    label: coordinate reference system for output\n    description: ESPG coordinate reference system for output\n    type: int\n    example: 4326\n  unit_distance:\n    label: unit for distance measurement\n    description: String value of\n    type: options\n    options:\n      - \"m2\"\n      - \"km2\"\noutputs:\n  result:\n    label: Analysis result\n    description: The result of the analysis, in CSV form\n    type: text/csv\n  result_plot:\n    label: Result plot\n    description: Plot of results\n    type: image/png\nconda: # Optional, only needed if your script has specific package dependencies\n  channels:\n    - r\n  dependencies:\n    - r-rjson=0.2.22 #package=version\nYAML is a space sensitive format, so make sure all the tab sets are correct in the file.\nIf inputs are outputs from a previous step in the pipeline, make sure to give the inputs and outputs the same name and they will be automatically linked in the pipeline.\nHere is an empty commented sample that can be filled in\nscript: # script file with extension, such as \"myScript.R\".\nname: # short name, such as My Script\ndescription: # Targeted to those who will interpret pipeline results and edit pipelines.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description:\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\n\nconda: # Optional, only needed if your script has specific package dependencies\n  channels: # programs that you need\n    - # list here\n  dependencies: # any packages that are dependent on a specfic version\n    - # list here\nSee example"
  },
  {
    "objectID": "how_to_contribute.html#step-2-integrating-yaml-inputs-and-outputs-into-your-script",
    "href": "how_to_contribute.html#step-2-integrating-yaml-inputs-and-outputs-into-your-script",
    "title": "How to Contribute",
    "section": "Step 2: Integrating YAML inputs and outputs into your script",
    "text": "Step 2: Integrating YAML inputs and outputs into your script\nNow that you have created your YAML file, the inputs and outputs need to be integrated into your scripts.\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle:\n\nScript launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.)\nScript reads input.json to get execution parameters (ex. species, area, data source, etc.)\nScript performs its task\nScript generates output.json containing links to result files, or native values (number, string, etc.)\n\nSee empty R script for a minimal script lifecycle example.\n\nReceiving inputs\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads the input.json file that is in the run folder. This JSON file is automatically created from the BON in a Box pipeline. Here is an example of how to read the input folder in the R script:\nFirst, set the environment for the script:\nSys.getenv(\"SCRIPT_LOCATION\")\nAnd then set the ‘input’ environment variables. The ‘input’ environment contains the specified inputs from the BON in a Box platform.\ninput &lt;- rjson::fromJSON(file=file.path(outputFolder, \"input.json\")) # Load input file\nNext, for any functions that are using these inputs, they need to be specified.\nInputs are specified by input$input_name.\nFor example, to create a function to take the square root of a number that was input by the user in the UI:\nresult &lt;- sqrt(input$number)\nThis means if the user inputs the number 144, this will be run in the function and output 12.\nHere is another example to read in an output from a previous step of the pipeline, which was output in csv format.\ndat &lt;- read.csv(input$csv_file)\nNext, you have to specify the outputs, which can be saved in a variety of formats. The format must be accurately specified in the YAML file.\nHere is an example of an output with a csv file and a plot in png format.\nresult_path &lt;- file.path(outputFolder, \"result.csv\")\nwrite.csv(result, result_path, row.names = F )\n\nresult_plot_path &lt;- file.path(outputFolder, \"result_plot.png\")\nggsave(result_path, result)\n\noutput &lt;- list(result = result_path,\nresult_plot = result_plot_path)\nLastly, write the output as a JSON file:\nsetwd(outputFolder)\njsonlite::write_json(output, \"output.json\", auto_unbox = TRUE, pretty = TRUE)\n\n\nScript validation\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\nReporting problems\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\nTo code an error so it prints in the pipeline step, you will need to wrap your script in a try catch function and then code it to print errors.\n{r}\noutput&lt;- tryCatch({\n\nSCRIPT BODY HERE\n\n}, error = function(e) { list(error= conditionMessage(e)) })\nThis will print any errors in the output of the script in the UI.\nYou can also code specific error messages. For example, if a user wanted the analysis to stop if the result was less than 0:\noutput&lt;- tryCatch({\n\nif (result &lt; 0){\nstop(\"Analysis was halted because result is less than 0.\")\n}\n\n}, error = function(e) { list(error= conditionMessage(e)) })\nAnd then the customized error message would appear in the UI."
  },
  {
    "objectID": "how_to_contribute.html#step-3-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor.",
    "href": "how_to_contribute.html#step-3-connect-your-scripts-with-the-bon-in-a-box-pipeline-editor.",
    "title": "How to Contribute",
    "section": "Step 3: Connect your scripts with the BON in a Box pipeline editor.",
    "text": "Step 3: Connect your scripts with the BON in a Box pipeline editor.\nNext, create your pipeline in the BON in a Box pipeline editor by connecting your inputs and outputs.\nOnce you have saved your scripts and YAML files to your local computer in the Bon-in-a-box pipelines folder that was cloned from github, they will show up as scripts in the pipeline editor. You can drag the scripts into the editor and connect the inputs and outputs to create your pipeline.\nA pipeline is a collection of steps to achieve the desired processing. Each script becomes a pipeline step.\nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output (rightmost red box in image above). Pipeline IO supports the same types and UI rendering as individual steps, since its inputs are directly fed to the steps, and outputs come from the step outputs.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\nPipeline editor\nThe pipeline editor allows you to create pipelines by plugging steps together.\nThe left pane shows the available steps, the main section shows the canvas.\nOn the right side, a collapsible pane allows to edit the labels and descriptions of the pipeline inputs and outputs.\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\n\n\nimage\n\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many step, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\nPipeline inputs and outputs\nAny input with no constant value assigned will be considered a pipeline input and user will have to fill the value.\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\n\n\nimage\n\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\nSaving and loading\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons.\nIn the event that saving has been disabled on your server instance, the save button will display “Save to clipboard”. To save your modifications: 1. Click save: the content is copied to your clipboard. 2. Make sure you are up to date (e.g. git pull --rebase). 3. Remove all the content of the target file. 4. Paste content and save. 5. Commit and push on a branch using git. 6. To share your modifications, create a pull request for that branch through the github UI."
  },
  {
    "objectID": "how_to_contribute.html#step-4-run-pipeline-and-troubleshoot",
    "href": "how_to_contribute.html#step-4-run-pipeline-and-troubleshoot",
    "title": "How to Contribute",
    "section": "Step 4: Run pipeline and troubleshoot",
    "text": "Step 4: Run pipeline and troubleshoot\nThe last step is to load your saved pipeline in BON and a Box and run it. The error log at the bottom of the page will show errors with the pipeline.\nOnce you change a script and save to your local computer, the changes will immediately be implemented when you run the script through BON in a Box. However, if you change the YAML file, you will need to reload the pipeline in the pipeline editor to see those changes. If this does not work, you may need to delete the script from the pipeline and re-drag it to the pipeline editor. Inputs and outputs can also be directly edited in the pipeline editor."
  },
  {
    "objectID": "about.html#why-should-you-contribute",
    "href": "about.html#why-should-you-contribute",
    "title": "About",
    "section": "Why should you contribute?",
    "text": "Why should you contribute?\nContributing to BON in a Box will increase the impact of your research by allowing your analysis workflows to be used in broader areas and contexts for decision-making. Contributors will become part of a global community to generate analyses of the state and trends of biodiversity to inform management decisions."
  },
  {
    "objectID": "README-user.html",
    "href": "README-user.html",
    "title": "BON in a Box Pipelines - User Documentation",
    "section": "",
    "text": "If you wish to contribute your indicator or EBV code, please let us know at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation.\n\n\n\nPrerequisites : - Git - A github account, with an SSH key registered. See Adding a new SSH key to your GitHub account. - At least 6 GB of free space (this includes the installation of Docker Desktop) - RAM requirements will depend on the scripts that you run. - Windows: - Docker Desktop - Note that it is not necessary to make an account - A Linux shell (git bash, Bash through PowerShell, cygwin, etc.) necessary to run th .sh scripts in the instructions below. - Mac: Docker Desktop - Note that it is not necessary to make an account - Make sure docker is added to the path of your terminal. From a terminal, run command docker run hello-world. If there is an error message, see https://stackoverflow.com/a/71923962/3519951. - If you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951. - Linux: Docker with Docker Compose installed. It is recommended to add your user to the docker group.\nTo run: 1. Clone repository (Windows users: do not clone the repo in a folder under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop. 2. Provide the environment variables: - Open the newly cloned repository on your computer - Find the file called runner-sample.env - Duplicate the file and rename the copy to runner.env. - Fill the properties depending on what you intend to run. - Adjust any server option as you see fit. 3. Using a linux terminal (terminal on Mac or Git Bash), navigate to top-level folder. 4. In the linux terminal, type ./server-up.sh - Make sure you have docker open and running on your computer. - The first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes. - Network problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure. - Windows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization. Access to the BIOS may be required to enable virtualization) 5. In browser: - http://localhost/ shows the UI 6. ./server-down.sh (to stop the server when done) 7. On Windows, to completely stop the processes, you might have to run wsl --shutdown\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted: - When modifying an existing script, simply re-run the script from the UI and the new version will be executed. - When adding or renaming scripts, refresh the browser page.\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted: - In the pipeline editor, click save, paste the file to your file in the pipeline folder and run it from the “pipeline run” page. - When adding or renaming pipelines, refresh the browser page.\n\n\n\nUse the ansible playbook instructions.\n\n\n\nYou have an instance of BON in a Box running, either locally or remotely, and you want to run your first script or pipeline.\nThere is one page to run scripts, and one to run pipelines. Select the script or pipelines from the dropdown and fill the form.\nThe form might ask you for a file. In order to provide a file that you own locally, upload or copy it to the userdata folder. You can then refer to it with a url as such: /userdata/myFile.shp, or /userdata/myFolder/myFile.shp if there are subfolders.\n\n\n\nThe scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle: 1. Script launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.) 3. Script reads input.json to get execution parameters (ex. species, area, data source, etc.) 4. Script performs its task 5. Script generates output.json containing links to result files, or native values (number, string, etc.)\nSee empty R script for a minimal script lifecycle example.\n\n\nThe script description is in a .yml file next to the script. It is necessary for the script to be found and connected to other scripts in a pipeline.\nMarkdown is supported for script, input and output descriptions. See this CommonMark quick reference for allowed markups.\nHere is an empty commented sample:\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines. Markdown is supported.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines. Markdown is supported.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description: # Markdown is supported.\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example\n\n\nEach input and output must declare a type, in lowercase. It can be a primitive or a file.\nThe following primitive types are accepted: | “type” attribute in the yaml | UI rendering | |——————————–|——————————| | boolean | Plain text | | float, float[] | Plain text | | int, int[] | Plain text | | options 2 | Plain text | | text, text[] | Plain text | | (any unknown type) | Plain text |\nAny MIME type is accepted. Here are a few common ones: | File type | MIME type to use in the yaml | UI rendering | | —————————- |——————————- |——————————| | CSV | text/csv | HTML table (partial content) | | GeoJSON | application/geo+json | Map | | GeoPackage | application/geopackage+sqlite3 | Link | | GeoTIFF 1 | image/tiff;application=geotiff | Map widget (leaflet) | | JPG | image/jpg | &lt;img&gt; tag | | Shapefile | application/dbf | Link | | Text | text/plain | Plain text | | TSV | text/tab-separated-values | HTML table (partial content) | | | (any unknown type) | Plain text or link |\nSearch the web to find the appropriate MIME type for your content. Here are a few references: - http://www.iana.org/assignments/media-types/media-types.xhtml - http://svn.apache.org/viewvc/httpd/httpd/trunk/docs/conf/mime.types?view=markup\n1 When used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\n2 options type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\n\n\n\n\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\n\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\n\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies.\n\n\n\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads and uses inputs from the input.json file. Example in R:\n## Receiving arguments from input.json.\n## outputFolder is already defined by server\nlibrary(\"rjson\")\ninput &lt;- fromJSON(file=file.path(outputFolder, \"input.json\"))\n\n## Can now be accessed from the map\nprint(input$predictors)\nThe script should perform appropriate parameter validation.\nNote that the inputs will be null if the user left the text box empty.\n\n\n\nThe output files generated by the script must be saved in the run folder. The script must also generate an output.json file in the same folder, that contains a map associating the output ids to their values. Example:\n{\n  \"sdm_pred\": \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_pred.tif\",\n  \"sdm_runs\":[\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_1.tif\",\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_2.tif\"\n  ]\n}\n\n\n\n\nA pipeline is a collection of steps to acheive the desired processing. Each script becomes a pipeline step. \nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output (rightmost red box in image above). Pipeline IO supports the same types and UI rendering as individual steps, since its inputs are directly fed to the steps, and outputs come from the step outputs.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\n\nThe pipeline editor allows you to create pipelines by plugging steps together.\nThe left pane shows the available steps, the main section shows the canvas.\nOn the right side, a collapsible pane allows to edit the labels and descriptions of the pipeline inputs and outputs.\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\n\n\nimage\n\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many step, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\n\nAny input with no constant value assigned will be considered a pipeline input and user will have to fill the value.\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\n\n\nimage\n\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\n\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons.\nIn the event that saving has been disabled on your server instance, the save button will display “Save to clipboard”. To save your modifications: 1. Click save: the content is copied to your clipboard. 2. Make sure you are up to date (e.g. git pull --rebase). 3. Remove all the content of the target file. 4. Paste content and save. 5. Commit and push on a branch using git. 6. To share your modifications, create a pull request for that branch through the github UI."
  },
  {
    "objectID": "README-user.html#contributing",
    "href": "README-user.html#contributing",
    "title": "BON in a Box Pipelines - User Documentation",
    "section": "",
    "text": "If you wish to contribute your indicator or EBV code, please let us know at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup below. You can create a branch or fork to save your work. Make sure that the code is general, and will work when used with various parameters, such as in different regions around the globe. Once the integration of the new scripts or pipelines are complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation."
  },
  {
    "objectID": "README-user.html#running-the-servers-locally",
    "href": "README-user.html#running-the-servers-locally",
    "title": "BON in a Box Pipelines - User Documentation",
    "section": "",
    "text": "Prerequisites : - Git - A github account, with an SSH key registered. See Adding a new SSH key to your GitHub account. - At least 6 GB of free space (this includes the installation of Docker Desktop) - RAM requirements will depend on the scripts that you run. - Windows: - Docker Desktop - Note that it is not necessary to make an account - A Linux shell (git bash, Bash through PowerShell, cygwin, etc.) necessary to run th .sh scripts in the instructions below. - Mac: Docker Desktop - Note that it is not necessary to make an account - Make sure docker is added to the path of your terminal. From a terminal, run command docker run hello-world. If there is an error message, see https://stackoverflow.com/a/71923962/3519951. - If you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951. - Linux: Docker with Docker Compose installed. It is recommended to add your user to the docker group.\nTo run: 1. Clone repository (Windows users: do not clone the repo in a folder under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop. 2. Provide the environment variables: - Open the newly cloned repository on your computer - Find the file called runner-sample.env - Duplicate the file and rename the copy to runner.env. - Fill the properties depending on what you intend to run. - Adjust any server option as you see fit. 3. Using a linux terminal (terminal on Mac or Git Bash), navigate to top-level folder. 4. In the linux terminal, type ./server-up.sh - Make sure you have docker open and running on your computer. - The first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes. - Network problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure. - Windows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization. Access to the BIOS may be required to enable virtualization) 5. In browser: - http://localhost/ shows the UI 6. ./server-down.sh (to stop the server when done) 7. On Windows, to completely stop the processes, you might have to run wsl --shutdown\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted: - When modifying an existing script, simply re-run the script from the UI and the new version will be executed. - When adding or renaming scripts, refresh the browser page.\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted: - In the pipeline editor, click save, paste the file to your file in the pipeline folder and run it from the “pipeline run” page. - When adding or renaming pipelines, refresh the browser page."
  },
  {
    "objectID": "README-user.html#running-the-servers-remotely",
    "href": "README-user.html#running-the-servers-remotely",
    "title": "BON in a Box Pipelines - User Documentation",
    "section": "",
    "text": "Use the ansible playbook instructions."
  },
  {
    "objectID": "README-user.html#running-a-script-or-pipeline",
    "href": "README-user.html#running-a-script-or-pipeline",
    "title": "BON in a Box Pipelines - User Documentation",
    "section": "",
    "text": "You have an instance of BON in a Box running, either locally or remotely, and you want to run your first script or pipeline.\nThere is one page to run scripts, and one to run pipelines. Select the script or pipelines from the dropdown and fill the form.\nThe form might ask you for a file. In order to provide a file that you own locally, upload or copy it to the userdata folder. You can then refer to it with a url as such: /userdata/myFile.shp, or /userdata/myFolder/myFile.shp if there are subfolders."
  },
  {
    "objectID": "README-user.html#scripts",
    "href": "README-user.html#scripts",
    "title": "BON in a Box Pipelines - User Documentation",
    "section": "",
    "text": "The scripts perform the actual work behind the scenes. They are located in /scripts folder\nCurrently supported : - R v4.3.1 - Julia v1.9.3 - Python3 v3.9.2 - sh\nScript lifecycle: 1. Script launched with output folder as a parameter. (In R, an outputFolder variable in the R session. In Julia, Shell and Python, the output folder is received as an argument.) 3. Script reads input.json to get execution parameters (ex. species, area, data source, etc.) 4. Script performs its task 5. Script generates output.json containing links to result files, or native values (number, string, etc.)\nSee empty R script for a minimal script lifecycle example.\n\n\nThe script description is in a .yml file next to the script. It is necessary for the script to be found and connected to other scripts in a pipeline.\nMarkdown is supported for script, input and output descriptions. See this CommonMark quick reference for allowed markups.\nHere is an empty commented sample:\nscript: # script file with extension, such as \"myScript.py\".\nname: # short name, such as My Script\ndescription: # Targetted to those who will interpret pipeline results and edit pipelines. Markdown is supported.\nauthor: # 1 to many\n  - name: # Full name\n    email: # Optional, email address of the author. This will be publicly available.\n    identifier: # Optional, full URL of a unique digital identifier, such as an ORCID.\nlicense: # Optional. If unspecified, the project's MIT license will apply.\nexternal_link: # Optional, link to a separate project, github repo, etc.\ntimeout: # Optional, in minutes. By defaults steps time out after 1h to avoid hung process to consume resources. It can be made longer for heavy processes.\n\ninputs: # 0 to many\n  key: # replace the word \"key\" by a snake case identifier for this input\n    label: # Human-readable version of the name\n    description: # Targetted to those who will interpret pipeline results and edit pipelines. Markdown is supported.\n    type: # see below\n    example: # will also be used as default value, can be null\n\noutputs: # 1 to many\n  key:\n    label:\n    description: # Markdown is supported.\n    type:\n    example: # optional, for documentation purpose only\n\nreferences: # 0 to many\n  - text: # plain text reference\n    doi: # link\nSee example\n\n\nEach input and output must declare a type, in lowercase. It can be a primitive or a file.\nThe following primitive types are accepted: | “type” attribute in the yaml | UI rendering | |——————————–|——————————| | boolean | Plain text | | float, float[] | Plain text | | int, int[] | Plain text | | options 2 | Plain text | | text, text[] | Plain text | | (any unknown type) | Plain text |\nAny MIME type is accepted. Here are a few common ones: | File type | MIME type to use in the yaml | UI rendering | | —————————- |——————————- |——————————| | CSV | text/csv | HTML table (partial content) | | GeoJSON | application/geo+json | Map | | GeoPackage | application/geopackage+sqlite3 | Link | | GeoTIFF 1 | image/tiff;application=geotiff | Map widget (leaflet) | | JPG | image/jpg | &lt;img&gt; tag | | Shapefile | application/dbf | Link | | Text | text/plain | Plain text | | TSV | text/tab-separated-values | HTML table (partial content) | | | (any unknown type) | Plain text or link |\nSearch the web to find the appropriate MIME type for your content. Here are a few references: - http://www.iana.org/assignments/media-types/media-types.xhtml - http://svn.apache.org/viewvc/httpd/httpd/trunk/docs/conf/mime.types?view=markup\n1 When used as an output, image/tiff;application=geotiff type allows an additionnal range attribute to be added with the min and max values that the tiff should hold. This will be used for display purposes.\nmap:\n  label: My map\n  description: Some map that shows bla bla...\n  type: image/tiff;application=geotiff\n  range: [0.1, 254]\n  example: https://example.com/mytiff.tif\n2 options type requires an additionnal options attribute to be added with the available options.\noptions_example:\n  label: Options example\n  description: The user has to select between a fixed number of text options. Also called select or enum. The script receives the selected option as text.\n  type: options\n  options:\n    - first option\n    - second option\n    - third option\n  example: third option\n\n\n\n\nThe syntax and structure of the script description file will be validated on push. To run the validation locally, run validate.sh\nThis validates that the syntax and structure are correct, but not that it’s content is correct. Hence, peer review of the scripts and the description files is mandatory before accepting a pull requests.\n\n\n\nThe output keys info, warning and error can be used to report problems in script execution. They do not need to be described in the outputs section of the description. They will be displayed specially in the UI.\nAny error message will halt the rest of the pipeline.\n\n\n\nScripts can install their own dependencies directly (install.packages in R, Pkg.add in Julia, etc). However, it will need to be reinstalled if the server is deployed on another computer or server.\nTo pre-compile the dependency in the image, add it to runners/r-dockerfile or runners/julia-dockerfile. When the pull request is merged to main, a new image will be available to docker compose pull with the added dependencies.\n\n\n\nWhen running a script, a folder is created for each given set of parameters. The same parameters result in the same folder, different parameters result in a different folder. The inputs for a given script are saved in an input.json file in this unique run folder.\nThe file contains the id of the parameters that were specified in the yaml script description, associated to the values for this run. Example:\n{\n    \"fc\": [\"L\", \"LQ\", \"LQHP\"],\n    \"method_select_params\": \"AUC\",\n    \"n_folds\": 2,\n    \"orientation_block\": \"lat_lon\",\n    \"partition_type\": \"block\",\n    \"predictors\": [\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio1_75548ca61981-01-01.tif\",\n        \"/output/data/loadFromStac/6af2ccfcd4b0ffe243ff01e3b7eccdc3/bio2_7333b3d111981-01-01.tif\"\n    ],\n    \"presence_background\": \"/output/SDM/setupDataSdm/edb9492031df9e063a5ec5c325bacdb1/presence_background.tsv\",\n    \"proj\": \"EPSG:6623\",\n    \"rm\": [0.5, 1.0, 2.0]\n}\nThe script reads and uses inputs from the input.json file. Example in R:\n## Receiving arguments from input.json.\n## outputFolder is already defined by server\nlibrary(\"rjson\")\ninput &lt;- fromJSON(file=file.path(outputFolder, \"input.json\"))\n\n## Can now be accessed from the map\nprint(input$predictors)\nThe script should perform appropriate parameter validation.\nNote that the inputs will be null if the user left the text box empty.\n\n\n\nThe output files generated by the script must be saved in the run folder. The script must also generate an output.json file in the same folder, that contains a map associating the output ids to their values. Example:\n{\n  \"sdm_pred\": \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_pred.tif\",\n  \"sdm_runs\":[\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_1.tif\",\n    \"/output/SDM/runMaxent/b5937ba69418b65cae7c6cfcfa78b5e8/sdm_runs_2.tif\"\n  ]\n}"
  },
  {
    "objectID": "README-user.html#pipelines",
    "href": "README-user.html#pipelines",
    "title": "BON in a Box Pipelines - User Documentation",
    "section": "",
    "text": "A pipeline is a collection of steps to acheive the desired processing. Each script becomes a pipeline step. \nPipelines also have inputs and outputs. In order to run, a pipeline needs to specify at least one output (rightmost red box in image above). Pipeline IO supports the same types and UI rendering as individual steps, since its inputs are directly fed to the steps, and outputs come from the step outputs.\nFor a general description of pipelines in software engineering, see Wikipedia.\n\n\nThe pipeline editor allows you to create pipelines by plugging steps together.\nThe left pane shows the available steps, the main section shows the canvas.\nOn the right side, a collapsible pane allows to edit the labels and descriptions of the pipeline inputs and outputs.\nTo add a step: drag and drop from the left pane to the canvas. Steps that are single scripts will display with a single border, while steps that are pipelines will display with a double border.\n\n\n\nimage\n\n\nTo connect steps: drag to connect an output and an input handle. Input handles are on the left, output handles are on the right.\nTo add a constant value: double-click on any input to add a constant value linked to this input. It is pre-filled with the example value.\nTo add an output: double-click on any step output to add a pipeline output linked to it, or drag and drop the red box from the left pane and link it manually.\nTo delete a step or a pipe: select it and press the Delete key on your keyboard.\nTo make an array out of single value outputs: if many outputs of the same type are connected to the same input, it will be received as an array by the script.\n\nA single value can also be combined with an array of the same type, to produce a single array.\n\nUser inputs: To provide inputs at runtime, simply leave them unconnected in the pipeline editor. They will be added to the sample input file when running the pipeline.\nIf an input is common to many step, a special user input node can be added to avoid duplication. First, link your nodes to a constant.\n\nThen, use the down arrow to pop the node’s menu. Choose “Convert to user input”.\n\nThe node will change as below, and the details will appear on the right pane, along with the other user inputs.\n\n\n\n\nAny input with no constant value assigned will be considered a pipeline input and user will have to fill the value.\nAdd an output node linked to a step output to specify that this output is an output of the pipeline. All other unmarked step outputs will still be available as intermediate results in the UI.\n\n\n\nimage\n\n\nPipeline inputs and outputs then appear in a collapsible pane on the right of the canvas, where their descriptions and labels can be edited.\n\nOnce edited, make sure to save your work before leaving the page.\n\n\n\nThe editor supports saving and loading on the server, unless explicitly disabled by host. This is done intuitively via the “Load from server” and “Save” buttons.\nIn the event that saving has been disabled on your server instance, the save button will display “Save to clipboard”. To save your modifications: 1. Click save: the content is copied to your clipboard. 2. Make sure you are up to date (e.g. git pull --rebase). 3. Remove all the content of the target file. 4. Paste content and save. 5. Commit and push on a branch using git. 6. To share your modifications, create a pull request for that branch through the github UI."
  },
  {
    "objectID": "how_to_install.html#deploying-the-servers-locally",
    "href": "how_to_install.html#deploying-the-servers-locally",
    "title": "How to Install",
    "section": "Deploying BON in a Box locally",
    "text": "Deploying BON in a Box locally\nBON in a Box can be installed and ran on a local computer. While personnal computers have less computing power than servers, it is more convenient for pipeline development, or when working with small examples. Users adding scripts and creating pipelines on their local computer can directly run them through BON in a Box.\nPrerequisites :\n\nGit - A github account, with an SSH key registered. See Adding a new SSH key to your GitHub account.\nAt least 6 GB of free space (this includes the installation of Docker Desktop) - RAM requirements will depend on the scripts that you run.\nWindows:\n\nDocker Desktop - Note that it is not necessary to make an account\nA Linux shell (git bash, Bash through PowerShell, cygwin, etc.) necessary to run th .sh scripts in the instructions below.\n\nMac:\n\nDocker Desktop Note that it is not necessary to make an account\nMake sure docker is added to the path of your terminal. From a terminal, run command docker run hello-world. If there is an error message, see https://stackoverflow.com/a/71923962/3519951.\nIf you encounter error no matching manifest for linux/arm64/v8 in the manifest list entries, export DOCKER_DEFAULT_PLATFORM. See https://stackoverflow.com/a/76404045/3519951.\n\nLinux: Docker with Docker Compose installed. It is recommended to add your user to the docker group.\n\nTo run:\n\nClone repository (Windows users: do not clone the repo in a folder under OneDrive.) This can be done in terminal using the following code: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git or in GitHub desktop.\nProvide the environment variables: - Open the newly cloned repository on your computer\n\nFind the file called runner-sample.env\nDuplicate the file and rename the copy to runner.env.\nFill the properties depending on what you intend to run. Include any API keys that you need to access data (e.g. GBIF or IUCN)\nAdjust any server option as you see fit.\n\nUsing a linux terminal (terminal on Mac or Git Bash), navigate to the folder of the cloned repository.\nIn the linux terminal, type ./server-up.sh\n\nMake sure the server is started from a linux terminal that is in the same folder as the cloned repository so it can access the files.\nMake sure you have docker open and running on your computer.\nThe first execution will be long, in order to download the micro-services. The next ones will be shorter or immediate, depending on the changes.\nNetwork problems may cause the process to fail. First try running the command again. Intermediate states are saved so not everything will be redone even when there is a failure.\nWindows users may need to turn on virtualization and other tools for Docker Desktop to work and update wsl (“wsl –update”, see https://docs.docker.com/desktop/troubleshoot/topics/#virtualization. Access to the BIOS may be required to enable virtualization)\n\nType http://localhost/ to open BON in a Box\nRun ./server-down.sh in a terminal to stop the server when done\nOn Windows, to completely stop the processes, you might have to run wsl --shutdown\n\nWhen modifying scripts in the /scripts folder, servers do not need to be restarted: - When modifying an existing script, simply re-run the script from the UI and the new version will be executed. - When adding or renaming scripts, refresh the browser page.\nWhen modifying pipelines in the /pipelines folder, servers do not need to be restarted: - In the pipeline editor, click save, paste the file to your file in the pipeline folder and run it from the “pipeline run” page. - When adding or renaming pipelines, refresh the browser page."
  },
  {
    "objectID": "how_to_install.html#deploying-the-servers-remotely",
    "href": "how_to_install.html#deploying-the-servers-remotely",
    "title": "How to Install",
    "section": "Deploying BON in a Box on a server",
    "text": "Deploying BON in a Box on a server\nInstallation steps on a server are the same as installing on a Linux computer, but some additional configurations need to be changed:\n\nIn runner.env, choose the appropriate HTTP_ADDRESS and HTTP_PORT for your server configuration. In a typical installation, use 0.0.0.0 as an HTTP address and default port (80).\nIn runner.env, select the “partial” cache cleaning mode. This will allow for calculated results to be long-lived on the server, as long as they are not run again with a new version of the script. The results in the output folder are always accessible by URL.\nThe outputs are precious, while the server machine can be re-generated anytime. Consider mounting an external backed-up drive to the output folder of your BON in a Box installation.\nBy default, BON in a Box listens for http. To enable https, we hide it behind a reverse proxy and activate https with certbot. How to do this is out of scope for this readme."
  },
  {
    "objectID": "README-dev.html",
    "href": "README-dev.html",
    "title": "Developer documentation",
    "section": "",
    "text": "If you wish to contribute code to this pipeline engine, please let us know at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup in the user documentation. You can create a branch or fork to save your work. Once complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation.\n\n\n\nThe code in this repository runs an engine, but the engine needs content! Here are the steps to start the server in development mode with the BON in a Box scripts and pipelines:\n\ndocker and docker compose must be installed.\nClone this repo: git clone git@github.com:GEO-BON/bon-in-a-box-pipeline-engine.git pipeline-engine\ncd pipeline-engine\nClone the BON in a Box repo (or any compatible repo of your choice) into the pipeline-repo folder: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git pipeline-repo\nCreate a runner.env file as per user instructions.\ncd ..\nPull the pre-compiled images: ./dev-server.sh pull\n\n\n\n\nFor the global project, Visual Studio Code. Recommended extensions:\n\nGit graph\nMarkdown Preview Mermaid\nMermaid Markdown Syntax Highlighting\n\nFor the script-server (Kotlin code), IntelliJ Idea. Note that on Linux there will be an ownership conflict between gradle files generated by the development docker and those from the IDE. To solve this, make sure to stop the dockers and run sudo chown -R &lt;yourinfo&gt;:&lt;yourinfo&gt; . before running the tests in IntelliJ.\n\n\n\n\nBuild the remaining images: ./dev-server.sh build\nStart the development server: ./dev-server.sh up\n\nIf there is a container name conflict, run ./dev-server.sh clean\n\n\nThis command enables:\n\nOpenAPI editor at http://localhost/swagger/\nUI server: React automatic hot-swapping\nScript-server: Kotlin hot-swapping by launching ./script-server/hotswap.sh\nNGINX: http-proxy/conf.d/ngnix.conf will be loaded\n\nOnce in a while you should use docker compose -f compose.yml -f compose.dev.yml pull to have the latest base images.\n\n\n\nThe servers are versionned by date of build of the docker image. One can check the version in the version tab of the UI.\n\n\n\nCreate a branch that ends with “staging” from the head of the main branch.\nMerge your changes to that branch. The docker hub GH action will trigger for branch main and any branch with name ending by “staging”. The branch name is appended to the tag of the docker image. See\n\n.github/workflows/docker_script-server.yml\n.github/workflows/docker_ui.yml\n\nCaveat: this only compiles the image where the paths were modified. For example, if viewer folder is modified, only the gateway will be rebuilt. However, the server will look for both images with the same prefix. In this case script-server-staging might not exist, or might be outdated. It is possible to launch the build of the script-server manually to make sure it exists and is up to date.\n\nOn github website, navigate to the Actions tab\nopen the desired action\nClick on the arrow next to “run workflow”\nSelect the desired staging branch\nRun workflow\nWait for completion\n\nIt is now possible to test the staging prod servers by running ./server-up.sh &lt;branchname&gt;. The launch script will look for this special tag in the docker hub.For example, ./server-up.sh staging will download and use both “gateway-staging” and “script-server-staging” images.\nSend the above command to a few beta users.\n\n\n\n\nThe changes are live as soon as they are merged to main branch: the dockers are built, pushed to geobon’s docker hub, and next time someone starts the server, the new dockers will be pulled.\n\n\n\nYes, we all know problems occur in production that do not happen in dev mode. So in order to build and test production dockers locally, do the following:\n\nIn pipeline-repo folder, delete the .server folder.\nCreate a symbolic link from .server to the parent: ln -s ../ .server\nBuild the server with .server/prod-server.sh command build\nThen run it with .server/prod-server.sh command up\n(.server/prod-server.sh clean might be needed if you get the usual name conflict error)\nStop the process with ctrl+c unless you used -d option in the previous command.\n\nWarning: Undo this by removing the symlink if you are to use ./server-up.sh for a regular launch of the production servers, otherwise it will checkout files in your parent pipeline engine repo though the symlink.\n\n\n\n\nstateDiagram-v2\n    state \"script-server\" as script\n    state \"scripts (static)\" as scripts\n    state \"output (static)\" as output\n    state \"R runner\" as r\n    state \"Julia runner\" as julia\n\n    [*] --&gt; ngnix\n    ngnix --&gt; ui\n    ngnix --&gt; viewer\n    ngnix --&gt; script\n    ngnix --&gt; output\n    script --&gt; scripts\n    script --&gt; r\n    script --&gt; julia\n\nui and viewer: React front-end. In production, those are served statically in the NGINX gateway.\nscript-server: Running scripts and pipeline orchestration\nR runner: Docker dedicated to runs R code, with most relevant packages pre-installed\nJulia runner: Docker dedicated to runs Julia code\n\nIn addition to these services,\n\nscripts folder contains all the scripts that can be run.\noutput folder contains all scripts result.\n\n\n\n\nflowchart TD\n never[Never ran] --&gt; running[Running]\n running --&gt; input[(- run folder\\n- input.json)]\n running --&gt; log[(log file)]\n running --&gt; success{Success?}\n success --&gt; |Yes| Done\n Done --&gt; output[(output.json)]\n success --&gt; |No| Failed\n Failed --&gt; |Add error flag|output\n\n\n\nThe OpenApi specification file is used by the UI to launch runs and track them until completion.\n\n\nsequenceDiagram\n    ui-&gt;&gt;script_server: script/list\n    script_server--&gt;&gt;ui: json map of scripts -&gt; names\n\n    ui-&gt;&gt;script_server: script/{path}/info\n    script_server--&gt;&gt;ui: script info json\n\n    ui-&gt;&gt;script_server: script/run\n    script_server-&gt;&gt;script: launch\n    script_server--&gt;&gt;ui: runId\n\n    loop Until output.json file generated\n        ui-&gt;&gt;script_server: output/{runId}/logs.txt\n        script_server--&gt;&gt;ui: logs text\n\n        ui-&gt;&gt;script_server: output/{id}/output.json\n    end\n\n\n    script--&gt;&gt;script_server: output.json\n\n    ui-&gt;&gt;script_server: output/{runId}/output.json\n    script_server--&gt;&gt;ui: script output json\n\n\n\n\nsequenceDiagram\n    ui-&gt;&gt;script_server: pipeline/list\n    script_server--&gt;&gt;ui: json map of pipeline -&gt; names\n\n    ui-&gt;&gt;script_server: pipeline/{path}/info\n    script_server--&gt;&gt;ui: pipeline info json\n\n    ui-&gt;&gt;script_server: pipeline/{path}/run\n    script_server--&gt;&gt;ui: runId\n    loop For each step\n        script_server-&gt;&gt;script: run\n        Note right of script: see previous diagram\n        script--&gt;&gt;script_server: output.json (script)\n        ui-&gt;&gt;script_server: pipeline/{runId}/outputs\n        script_server--&gt;&gt;ui: pipelineOutput.json (pipeline)\n    end\n\nEvery second, the UI polls for:\n\npipelineOutput.json from the pipeline, to get the output folders of individual scripts. Stops polling when pipeline stops.\nlogs.txt of individual scripts, for realtime logging, only if log section is opened. Stops when individual script completes, or when log section closed.\noutput.json of individual scripts, to know when script completes and display its outputs. Stops when script stops.\n\n\n\n\n\nUsing http://localhost/swagger, edit the specification.\nCopy the result to script-server/api/openapi.yaml\nUse ui/BonInABoxScriptService/generate-client.sh and script-server/generate-server-openapitools.sh to regenerate the client and the server.\nMerge carefully, not all generated code is to be kept.\nImplement the gaps.\n\n\n\n\n\nSince runner-r and runner-julia run in a separate docker, when the user stops the pipeline, the signal must go from the script-server, to the runner, to the running script. Docker does not allow this by default, this is why we save the PID in a file and use a separate exec command to kill the process.\nThe PID file is called .pid and is located in the output folder of the run. It is deleted when the script completes. For details, see ScriptRun.kt."
  },
  {
    "objectID": "README-dev.html#contributing",
    "href": "README-dev.html#contributing",
    "title": "Developer documentation",
    "section": "",
    "text": "If you wish to contribute code to this pipeline engine, please let us know at web@geobon.org.\nThe recommended method is to setup an instance of BON in a Box somewhere you can easily play with the script files, using the local or remote setup in the user documentation. You can create a branch or fork to save your work. Once complete, open a pull request to this repository. The pull request will be peer-reviewed before acceptation."
  },
  {
    "objectID": "README-dev.html#getting-the-code",
    "href": "README-dev.html#getting-the-code",
    "title": "Developer documentation",
    "section": "",
    "text": "The code in this repository runs an engine, but the engine needs content! Here are the steps to start the server in development mode with the BON in a Box scripts and pipelines:\n\ndocker and docker compose must be installed.\nClone this repo: git clone git@github.com:GEO-BON/bon-in-a-box-pipeline-engine.git pipeline-engine\ncd pipeline-engine\nClone the BON in a Box repo (or any compatible repo of your choice) into the pipeline-repo folder: git clone git@github.com:GEO-BON/bon-in-a-box-pipelines.git pipeline-repo\nCreate a runner.env file as per user instructions.\ncd ..\nPull the pre-compiled images: ./dev-server.sh pull"
  },
  {
    "objectID": "README-dev.html#ide-setup",
    "href": "README-dev.html#ide-setup",
    "title": "Developer documentation",
    "section": "",
    "text": "For the global project, Visual Studio Code. Recommended extensions:\n\nGit graph\nMarkdown Preview Mermaid\nMermaid Markdown Syntax Highlighting\n\nFor the script-server (Kotlin code), IntelliJ Idea. Note that on Linux there will be an ownership conflict between gradle files generated by the development docker and those from the IDE. To solve this, make sure to stop the dockers and run sudo chown -R &lt;yourinfo&gt;:&lt;yourinfo&gt; . before running the tests in IntelliJ."
  },
  {
    "objectID": "README-dev.html#launching-the-dockers-in-development-mode",
    "href": "README-dev.html#launching-the-dockers-in-development-mode",
    "title": "Developer documentation",
    "section": "",
    "text": "Build the remaining images: ./dev-server.sh build\nStart the development server: ./dev-server.sh up\n\nIf there is a container name conflict, run ./dev-server.sh clean\n\n\nThis command enables:\n\nOpenAPI editor at http://localhost/swagger/\nUI server: React automatic hot-swapping\nScript-server: Kotlin hot-swapping by launching ./script-server/hotswap.sh\nNGINX: http-proxy/conf.d/ngnix.conf will be loaded\n\nOnce in a while you should use docker compose -f compose.yml -f compose.dev.yml pull to have the latest base images."
  },
  {
    "objectID": "README-dev.html#releasing-a-server-version",
    "href": "README-dev.html#releasing-a-server-version",
    "title": "Developer documentation",
    "section": "",
    "text": "The servers are versionned by date of build of the docker image. One can check the version in the version tab of the UI.\n\n\n\nCreate a branch that ends with “staging” from the head of the main branch.\nMerge your changes to that branch. The docker hub GH action will trigger for branch main and any branch with name ending by “staging”. The branch name is appended to the tag of the docker image. See\n\n.github/workflows/docker_script-server.yml\n.github/workflows/docker_ui.yml\n\nCaveat: this only compiles the image where the paths were modified. For example, if viewer folder is modified, only the gateway will be rebuilt. However, the server will look for both images with the same prefix. In this case script-server-staging might not exist, or might be outdated. It is possible to launch the build of the script-server manually to make sure it exists and is up to date.\n\nOn github website, navigate to the Actions tab\nopen the desired action\nClick on the arrow next to “run workflow”\nSelect the desired staging branch\nRun workflow\nWait for completion\n\nIt is now possible to test the staging prod servers by running ./server-up.sh &lt;branchname&gt;. The launch script will look for this special tag in the docker hub.For example, ./server-up.sh staging will download and use both “gateway-staging” and “script-server-staging” images.\nSend the above command to a few beta users.\n\n\n\n\nThe changes are live as soon as they are merged to main branch: the dockers are built, pushed to geobon’s docker hub, and next time someone starts the server, the new dockers will be pulled.\n\n\n\nYes, we all know problems occur in production that do not happen in dev mode. So in order to build and test production dockers locally, do the following:\n\nIn pipeline-repo folder, delete the .server folder.\nCreate a symbolic link from .server to the parent: ln -s ../ .server\nBuild the server with .server/prod-server.sh command build\nThen run it with .server/prod-server.sh command up\n(.server/prod-server.sh clean might be needed if you get the usual name conflict error)\nStop the process with ctrl+c unless you used -d option in the previous command.\n\nWarning: Undo this by removing the symlink if you are to use ./server-up.sh for a regular launch of the production servers, otherwise it will checkout files in your parent pipeline engine repo though the symlink."
  },
  {
    "objectID": "README-dev.html#microservice-infrastructure",
    "href": "README-dev.html#microservice-infrastructure",
    "title": "Developer documentation",
    "section": "",
    "text": "stateDiagram-v2\n    state \"script-server\" as script\n    state \"scripts (static)\" as scripts\n    state \"output (static)\" as output\n    state \"R runner\" as r\n    state \"Julia runner\" as julia\n\n    [*] --&gt; ngnix\n    ngnix --&gt; ui\n    ngnix --&gt; viewer\n    ngnix --&gt; script\n    ngnix --&gt; output\n    script --&gt; scripts\n    script --&gt; r\n    script --&gt; julia\n\nui and viewer: React front-end. In production, those are served statically in the NGINX gateway.\nscript-server: Running scripts and pipeline orchestration\nR runner: Docker dedicated to runs R code, with most relevant packages pre-installed\nJulia runner: Docker dedicated to runs Julia code\n\nIn addition to these services,\n\nscripts folder contains all the scripts that can be run.\noutput folder contains all scripts result."
  },
  {
    "objectID": "README-dev.html#script-lifecycle-artifacts",
    "href": "README-dev.html#script-lifecycle-artifacts",
    "title": "Developer documentation",
    "section": "",
    "text": "flowchart TD\n never[Never ran] --&gt; running[Running]\n running --&gt; input[(- run folder\\n- input.json)]\n running --&gt; log[(log file)]\n running --&gt; success{Success?}\n success --&gt; |Yes| Done\n Done --&gt; output[(output.json)]\n success --&gt; |No| Failed\n Failed --&gt; |Add error flag|output"
  },
  {
    "objectID": "README-dev.html#openapi-specification",
    "href": "README-dev.html#openapi-specification",
    "title": "Developer documentation",
    "section": "",
    "text": "The OpenApi specification file is used by the UI to launch runs and track them until completion.\n\n\nsequenceDiagram\n    ui-&gt;&gt;script_server: script/list\n    script_server--&gt;&gt;ui: json map of scripts -&gt; names\n\n    ui-&gt;&gt;script_server: script/{path}/info\n    script_server--&gt;&gt;ui: script info json\n\n    ui-&gt;&gt;script_server: script/run\n    script_server-&gt;&gt;script: launch\n    script_server--&gt;&gt;ui: runId\n\n    loop Until output.json file generated\n        ui-&gt;&gt;script_server: output/{runId}/logs.txt\n        script_server--&gt;&gt;ui: logs text\n\n        ui-&gt;&gt;script_server: output/{id}/output.json\n    end\n\n\n    script--&gt;&gt;script_server: output.json\n\n    ui-&gt;&gt;script_server: output/{runId}/output.json\n    script_server--&gt;&gt;ui: script output json\n\n\n\n\nsequenceDiagram\n    ui-&gt;&gt;script_server: pipeline/list\n    script_server--&gt;&gt;ui: json map of pipeline -&gt; names\n\n    ui-&gt;&gt;script_server: pipeline/{path}/info\n    script_server--&gt;&gt;ui: pipeline info json\n\n    ui-&gt;&gt;script_server: pipeline/{path}/run\n    script_server--&gt;&gt;ui: runId\n    loop For each step\n        script_server-&gt;&gt;script: run\n        Note right of script: see previous diagram\n        script--&gt;&gt;script_server: output.json (script)\n        ui-&gt;&gt;script_server: pipeline/{runId}/outputs\n        script_server--&gt;&gt;ui: pipelineOutput.json (pipeline)\n    end\n\nEvery second, the UI polls for:\n\npipelineOutput.json from the pipeline, to get the output folders of individual scripts. Stops polling when pipeline stops.\nlogs.txt of individual scripts, for realtime logging, only if log section is opened. Stops when individual script completes, or when log section closed.\noutput.json of individual scripts, to know when script completes and display its outputs. Stops when script stops.\n\n\n\n\n\nUsing http://localhost/swagger, edit the specification.\nCopy the result to script-server/api/openapi.yaml\nUse ui/BonInABoxScriptService/generate-client.sh and script-server/generate-server-openapitools.sh to regenerate the client and the server.\nMerge carefully, not all generated code is to be kept.\nImplement the gaps."
  },
  {
    "objectID": "README-dev.html#debugging-signal-forwarding",
    "href": "README-dev.html#debugging-signal-forwarding",
    "title": "Developer documentation",
    "section": "",
    "text": "Since runner-r and runner-julia run in a separate docker, when the user stops the pipeline, the signal must go from the script-server, to the runner, to the running script. Docker does not allow this by default, this is why we save the PID in a file and use a separate exec command to kill the process.\nThe PID file is called .pid and is located in the output folder of the run. It is deleted when the script completes. For details, see ScriptRun.kt."
  }
]